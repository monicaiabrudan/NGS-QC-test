[
  {
    "objectID": "meet the team.html",
    "href": "meet the team.html",
    "title": "Acknowledgements",
    "section": "",
    "text": "As included in ANSI/NISO Z39.104-2022, CRediT, Contributor Roles Taxonomy https://www.niso.org/publications/z39104-2022-credit\n\nContributor Roles\n\nConceptualization\n(Ideas; formulation or evolution of overarching research goals and aims.)\nData curation\n(Management activities to annotate (produce metadata), scrub data and maintain research data (including software code, where it is necessary for interpreting the data itself) for initial use and later re-use.)\nFormal analysis\n(Application of statistical, mathematical, computational, or other formal techniques to analyze or synthesize study data.)\nMethodology\n(Development or design of methodology; creation of models)\nResources\n(Provision of study materials, reagents, materials, patients, laboratory samples, animals, instrumentation, computing resources, or other analysis tools.)\nSupervision\n(Oversight and leadership responsibility for the research activity planning and execution, including mentorship external to the core team)\nValidation\n(Verification, whether as a part of the activity or separate, of the overall replication/reproducibility of results/experiments and other research outputs. )\nVisualization\n(Preparation, creation and/or presentation of the published work, specifically visualization/data presentation)\n\n© Wellcome Connecting Science",
    "crumbs": [
      "Home",
      "Contributors",
      "Acknowledgements"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_instructor_notes.html",
    "href": "course_modules/Module1/module1_instructor_notes.html",
    "title": "Instructor notes",
    "section": "",
    "text": "Learners often confuse FASTA vs FASTQ. Emphasize that FASTA has no quality scores, whereas FASTQ includes them\nTrainer Tips:\nUse Visual Aids: When teaching, show real examples. For instance, open a FASTQ file in a text editor or slide to illustrate the four-line structure. If possible, bring up an ASCII table chart during the quality score discussion so learners can see character codes.\nEmphasize Connections: Tie this module to downstream analysis – e.g., explain that understanding FASTQ is foundational since those files feed into aligners (module 2) and BAM files produced will be used in variant calling (module 3). This helps learners see the big picture pipeline.\nInteractive Demonstration: If time and resources allow, demonstrate using head/tail or samtools view commands on a sample file in class. For instance, show the first reads of a FASTQ and how they appear in a SAM after alignment, to reinforce how the formats are connected. This live demo can engage learners and clarify abstract concepts.\nLink to QC: Even though explicit quality control tools (e.g., FastQC) were not covered in text, you can mention them verbally. E.g., “After obtaining FASTQ files, typically one would run quality control (FastQC) to get an overview of base quality distribution.” This sets the stage that file formats are not just static info, but used in workflows.",
    "crumbs": [
      "Home",
      "File formats",
      "Instructor notes"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_assessment.html",
    "href": "course_modules/Module1/module1_assessment.html",
    "title": "Assessment Quiz",
    "section": "",
    "text": "Quiz\n\nLook at the following FASTQ line:\n\n@ERR007731.740 IL16_2979:6:1:9:1419/1\nTAAAAAAAAGATGTCATCAGCACATCAGAAAAGAAGGCAACTTTAAAACTTTTC\n+\nDBABBBABABAABABABBABBBAAA&gt;@B@BBAA@4AAA&gt;.&gt;BAA@779:AAA@A\nWhat is the quality of the first base (T)?\n\nLook at the following FASTQ line:\n\n@ERR007731.740 IL16_2979:6:1:9:1419/1  TAAAAAAAAGATGTCATCAGCACATCAGAAAAGAAGGCAACTTTAAAACTTTTC\n+\nDBABBBABABAABABABBABBBAAA&gt;@B@BBAA@4AAA&gt;.&gt;BAA@779:AAA@A\nWhat is the probability that there was a base call error for the first base (T)?\n\nLook at the following line from the header of a SAM file.\n\n@RGID:ERR003612 PL:ILLUMINA LB:g1k-sc-NA20538-TOS-1 PI:2000 DS:SRP000540 SM:NA20538 CN:SC\nWhat does RG stand for?"
  },
  {
    "objectID": "course_modules/Module1/module1_exercises.html",
    "href": "course_modules/Module1/module1_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "There are several file formats for storing Next Generation Sequencing (NGS) data. In this tutorial we will look at some of the most common formats for storing NGS reads and variant data. We will cover the following formats:\nFASTQ - This format stores unaligned read sequences with base qualities\nSAM/BAM - This format stores unaligned or aligned reads (text and binary formats)\nCRAM - This format is similar to BAM but has better compression than BAM\nVCF/BCF - Flexible variant call format for storing SNPs, indels, structural variations (text and binary formats)\nFollowing this, we will work through some examples of converting between the different formats.\nFurther to understanding the different file formats, it is important to remember that all sequencing platforms have technical limitations that can introduce biases in your sequencing data. Because of this it is very important to check the quality of the data before starting any analysis, whether you are planning to use something you have sequenced yourself or publicly available data. In the latter part of this tutorial we will describe how to perform a QC assessment for your NGS data.\n\n\n\nThis tutorial comprises the following sections:\n1. Data formats\n2. QC assessment\nIf you have time you can also complete:\n3. File conversion\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal on your computer and type the command below:\ncd ~/course-data/data_formats/\nNow you can follow the instructions in the tutorial from here.\n\n\n\nThis tutorial assumes that you have samtools, bcftools and Picard tools installed on your computer.\nThese are already installed on the VM you are using. To check that these are installed, you can run the following commands:\nsamtools --help\nbcftools --help\npicard -h\nThis should return the help message for samtools, bcftools and picard tools respectively.\nTo get started with the tutorial, go to the first section: Data formats",
    "crumbs": [
      "Home",
      "File formats",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_exercises.html#ngs-data-formats-and-qc",
    "href": "course_modules/Module1/module1_exercises.html#ngs-data-formats-and-qc",
    "title": "Exercises",
    "section": "",
    "text": "There are several file formats for storing Next Generation Sequencing (NGS) data. In this tutorial we will look at some of the most common formats for storing NGS reads and variant data. We will cover the following formats:\nFASTQ - This format stores unaligned read sequences with base qualities\nSAM/BAM - This format stores unaligned or aligned reads (text and binary formats)\nCRAM - This format is similar to BAM but has better compression than BAM\nVCF/BCF - Flexible variant call format for storing SNPs, indels, structural variations (text and binary formats)\nFollowing this, we will work through some examples of converting between the different formats.\nFurther to understanding the different file formats, it is important to remember that all sequencing platforms have technical limitations that can introduce biases in your sequencing data. Because of this it is very important to check the quality of the data before starting any analysis, whether you are planning to use something you have sequenced yourself or publicly available data. In the latter part of this tutorial we will describe how to perform a QC assessment for your NGS data.\n\n\n\nThis tutorial comprises the following sections:\n1. Data formats\n2. QC assessment\nIf you have time you can also complete:\n3. File conversion\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal on your computer and type the command below:\ncd ~/course-data/data_formats/\nNow you can follow the instructions in the tutorial from here.\n\n\n\nThis tutorial assumes that you have samtools, bcftools and Picard tools installed on your computer.\nThese are already installed on the VM you are using. To check that these are installed, you can run the following commands:\nsamtools --help\nbcftools --help\npicard -h\nThis should return the help message for samtools, bcftools and picard tools respectively.\nTo get started with the tutorial, go to the first section: Data formats",
    "crumbs": [
      "Home",
      "File formats",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_exercises.html#data-formats-for-ngs-data",
    "href": "course_modules/Module1/module1_exercises.html#data-formats-for-ngs-data",
    "title": "Exercises",
    "section": "Data formats for NGS data",
    "text": "Data formats for NGS data\nHere we will take a closer look at some of the most common NGS data formats. First, check you are in the correct directory:\npwd\nIt should display something like:\n/home/manager/course_data/data_formats/\n\nFASTA\nQ1: How many sequences are there in the fasta file data/example.fasta? (Hint: is there a grep option you can use?)\n\n\nFASTQ\nQ2: How many reads are there in the file example.fastq? (Hint: remember that @ is a possible quality score. Is there something else in the header that is unique?)\n\n\nSAM\nLook at the following line from the header of a SAM file and answering the questions that follow:\n@RG ID:ERR003612 PL:ILLUMINA LB:g1k-sc-NA20538-TOS-1 PI:2000 DS:SRP000540 SM:NA20538 CN:SC\nQ3: What does RG stand for?\nQ4: What is the sequencing platform?\nQ5: What is the sequencing centre?\nQ6: What is the lane identifier?\nQ7: What is the expected fragment insert size?\nLet’s have a look at example.sam. Notice that we can use the standard UNIX operations like cat on this file.\ncat data/example.sam\nQ8: What is the mapping quality of ERR003762.5016205? (Hint: can you use grep and awk to find this?)\nQ9: What is the CIGAR string for ERR003814.6979522? (Hint: we will go through the meaning of CIGAR strings in the next section)\nQ10: What is the inferred insert size of ERR003814.1408899?\n\n\nCIGAR string\nQ11: What does the CIGAR from Q9 mean?\nQ12: How would you represent the following alignment with a CIGAR string?\nRef: ACGT- - - - ACGTACGT\nRead: ACGTACGTACGTACGT\n\n\nBAM\nBAM (Binary Alignment/Map) format, is a compressed binary version of SAM. This means that, while SAM is human readable, BAM is only readable for computers. BAM files can be viewed using samtools, and will then have the same format as a SAM file. The key features of BAM are:\n• Can store alignments from most mappers\n• Supports multiple sequencing technologies\n• Supports indexing for quick retrieval/viewing\n• Compact size (e.g. 112Gbp Illumina = 116GB disk space)\n• Reads can be grouped into logical groups e.g. lanes, libraries, samples\n• Widely supported by variant calling packages and viewers\nSince BAM is a binary format, we can’t use the standard UNIX operations directly on this file format.\nSamtools is a set of programs for interacting with SAM and BAM files. Using the samtools view command, print the header of the BAM file:\nsamtools view -H data/NA20538.bam\nQ13: What version of the human assembly was used to perform the alignments? (Hint: Can you spot this somewhere in the @SQ records?)\nQ14: How many lanes are in this BAM file? (Hint: Do you recall what RG represents?)\nQ15: What programs were used to create this BAM file? (Hint: have a look for the program record, @PG)\nQ16: What version of bwa was used to align the reads? (Hint: is there anything in the @PG record that looks like it could be a version tag?)\nThe output from running samtools view on a BAM file without any options is a headerless SAM file.\nThis gets printed to STDOUT in the terminal, so we will want to pipe it to something. Let’s have a look at the first read of the BAM file:\nsamtools view data/NA20538.bam | head -n 1\nQ17: What is the name of the first read? (Hint: have a look at the alignment section if you can’t recall the different fields)\nQ18: What position does the alignment of the read start at?\n\n\nCRAM\nEven though BAM files are compressed, they are still very large. Typically they use 1.5-2 bytes for each base pair of sequencing data that they contain, and while disk capacity is ever improving, increases in disk capacity are being far outstripped by sequencing technologies.\nBAM stores all of the data, this includes every read base, every base quality, and it uses a single conventional compression technique for all types of data. CRAM was designed for better compression of genomic data than SAM/BAM. CRAM uses three important concepts:\n• Reference based compression\n• Controlled loss of quality information\n• Different compression methods to suit the type of data, e.g. base qualities vs. metadata vs. extra tags\nThe figure below displays how reference-based compression works. Instead of saving all the bases of all the reads, only the nucleotides that differ from the reference, and their positions, are kept.\n\n\n\nReference based compression\n\n\n\n\n\nReference based compression\n\n\nIn lossless (no information is lost) mode a CRAM file is 60% of the size of a BAM file, so archives and sequencing centres have moved from BAM to CRAM.\nSince samtools 1.3, CRAM files can be read in the same way that BAM files can. We will look closer at how you can convert between SAM, BAM and CRAM formats in the next section.\n\n\nIndexing\nTo allow for fast random access of regions in BAM and CRAM files, they can be indexed. The files must first be coordinate-sorted rather that sorted by read name. This can be done using samtools sort. If no options are supplied, it will by default sort by the left-most position of the reference.\nsamtools sort -o data/NA20538_sorted.bam data/NA20538.bam\nNow we can use samtools index to create an index file (.bai) for our sorted BAM file:\nsamtools index data/NA20538_sorted.bam\nTo look for reads mapped to a specific region, we can use samtools view and specify the region we are interested in as: RNAME[:STARTPOS[-ENDPOS]]. For example, to look at all the reads mapped to a region called chr4, we could use:\nsamtools view alignment.bam chr4\nTo look at the region on chr4 beginning at position 1,000,000 and ending at the end of the chromosome, we can do:\nsamtools view alignment.bam chr4:1000000\nAnd to explore the 1001bp long region on chr4 beginning at position 1,000 and ending at position\n2,000, we can use:\nsamtools view alignment.bam chr4:1000-2000\nQ19: How many reads are mapped to region 20025000-20030000 on chromosome 1?\n\n\nVCF\n\n\n\nThe VCF format\n\n\nThe VCF file format was introduced to store variation data. VCF consists of tab-delimited text and is parsable by standard UNIX commands which makes it flexible and user-extensible. The figure below provides an overview of the different components of a VCF file:\n\n\nVCF header\nThe VCF header consists of meta-information lines (starting with ##) and a header line (starting with #). All meta-information lines are optional and can be put in any order, except for fileformat. This holds the information about which version of VCF is used and must come first.\nThe meta-information lines consist of key=value pairs. Examples of meta-information lines that can be included are ##INFO, ##FORMAT and ##reference. The values can consist of multiple fields enclosed by &lt;&gt;. More information about these fields is available in the VCF specification http://samtools.github.io/hts-specs/VCFv4.3.pdf. This can be accessed using a web browser and there is a copy in the QC directory.\nHeader line The header line starts with # and consists of 8 required fields:\n1. CHROM: an identifier from the reference genome\n2. POS: the reference position\n3. ID: a list of unique identifiers (where available)\n4. REF: the reference base(s)\n5. ALT: the alternate base(s)\n6. QUAL: a phred-scaled quality score\n7. FILTER: filter status\n8. INFO: additional information\nIf the file contains genotype data, the required fields are also followed by a FORMAT column header, and then a number of sample IDs. The FORMAT field specifies the data types and order. Some examples of these data types are:\n• GT: Genotype, encoded as allele values separated by either / or |\n• DP: Read depth at this position for this sample\n• GQ: Conditional genotype quality, encoded as a phred quality\n\n\nBody\nIn the body of the VCF, each row contains information about a position in the genome along with genotype information on samples for each position, all according to the fields in the header line.\n\n\nBCF\nBCF is a compressed binary representation of VCF. VCF can be compressed with BGZF (bgzip) and indexed with TBI or CSI (tabix), but even compressed\nit can still be very big. For example, a compressed VCF with 3781 samples of human data will be 54 GB for chromosome 1, and 680 GB for the whole genome. VCFs can also be slow to parse, as text conversion is slow. The main bottleneck is the “FORMAT” fields. For this reason the BCF format was developed.\nIn BCF files the fields are rearranged for fast access. The following images show the process of converting a VCF file into a BCF file.\n\nBcftools comprises a set of programs for interacting with VCF and BCF files. It can be used to convert between VCF and BCF and to view or extract records from a region.\n\n\nbcftools view\nLet’s have a look at the header of the file 1kg.bcf in the data directory. Note that bcftools uses -h to print only the header, while samtools uses -H for this.\nbcftools view -h data/1kg.bcf\nSimilarly to BAM, BCF supports random access, that is, fast retrieval from a given region. For this, the file must be indexed:\nbcftools index data/1kg.bcf\nNow we can extract all records from the region 20:24042765-24043073, using the -r option. The -H option will make sure we don’t include the header in the output:\nbcftools view -H -r 20:24042765-24043073 data/1kg.bcf\n\n\nbcftools query\nThe versatile bcftools query command can be used to extract any VCF field. Combined with standard UNIX commands, this gives a powerful tool for quick querying of VCFs. Have a look at the usage options:\nbcftools query -h\nLet’s try out some useful options. As you can see from the usage, -l will print a list of all the samples in the file. Give this a go:\nbcftools query -l data/1kg.bcf\nAnother very useful option is -s which allows you to extract all the data relating to a particular sample. This is a common option meaning it can be used for many bcftools commands, like bcftools view. Try this for sample HG00131:\nbcftools view -s HG00131 data/1kg.bcf | head -n 50\nThe format option, -f can be used to select what gets printed from your query command. For example, the following will print the position, reference base and alternate base for sample HG00131, separated by tabs:\nbcftools query -f'%POS\\t%REF\\t%ALT\\n' -s HG00131 data/1kg.bcf | head\nFinally, let’s look at the -i option. With this option we can select only sites for which a particular expression is true. For instance, if we only want to look at sites that have at least 2 alternate alleles, we can use the following expression (piped to head to only show a subset of the output):\nbcftools query -f'%CHROM\\t%POS\\n' -i 'AC[0]&gt;2' data/1kg.bcf | head\nWe use -i with the expression AC[0]&gt;2. AC is an info field that holds the allele _count.\nSome fields can hold multiple values, so we use AC[0]&gt;2 to indicate that we are looking for the first value (this is zero indexed, and hence starts at 0 instead of 1), and that this value should be &gt; 2. To format our output, we use -f to specify that we want to print the chromosome name and position.\nThere is more information about expressions on the bcftools manual page http://samtools.github.io/bcftools/bcftools.html#expressions\nNow, try and answer the following questions about the file 1kg.bcf in the data directory. For more information about the different usage options you can open the bcftools query manual page http://samtools.github.io/bcftools/bcftools.html#query in a web browser.\nQ20: What version of the human assembly do the coordinates refer to?\nQ21: How many samples are there in the BCF?\nQ22: What is the genotype of the sample HG00107 at the position 20:24019472? (Hint: use the combination of -r, -s, and -f options)\nQ23: How many positions are there with more than 10 alternate alleles? (Hint: use the -i filtering option)\nQ24: In how many positions does HG00107 have a non-reference genotype and a read depth bigger than 10? (Hint: you can use pipes to combine bcftools queries)\n\n\ngVCF\nOften it is not enough to know variant sites only. For instance, we don’t know if a site was dropped because it matches the reference or because the data is missing. We sometimes need evidence for both variant and non-variant positions in the genome. In gVCF format, blocks of reference-only sites can be represented in a single record using the “INFO/END” tag. Symbolic alleles (&lt;*&gt;) are used for incremental calling:\n\n\n\ngVCF\n\n\nQ25: In the above example, what is the size of the reference-only block starting at position 9923?\nQ26: For the same block, what is the first base?\nQ27: How many reference reads does the block have?\nNow continue to the next section of the tutorial: QC assessment of NGS data.\n\n\nQC assessment of NGS data\nQC is an important part of any analysis. In this section we are going to look at some of the metrics and graphs that can be used to assess the QC of NGS data.\n\n\nBase quality\nIllumina sequencing technology relies on sequencing by synthesis. One of the most common problems with this is dephasing. For each sequencing cycle, there is a possibility that the replication machinery slips and either incorporates more than one nucleotide or perhaps misses to incorporate one at all. The more cycles that are run (i.e. the longer the read length gets), the greater the accumulation of these types of errors gets. This leads to a heterogeneous population in the cluster, and a decreased signal purity, which in turn reduces the precision of the base calling. The figure below shows an example of this.\n\n\n\nMean base quality\n\n\nBecause of dephasing, it is possible to have high-quality data at the beginning of the read but really low-quality data towards the end of the read. In those cases you can decide to trim off the low-quality reads, for example using a tool called Trimmomatic.\nThe figures below shows an example of a good sequencing run (left) and a poor sequencing run (right).\n\n\n\nBase quality\n\n\n\n\nOther base calling errors\nThere are several different reasons for a base to be called incorrectly, as shown in the figure below.\nPhasing noise and signal decay is a result of the dephasing issue described above. During library preparation, mixed clusters can occur if multiple templates get co-located. These clusters should be removed from the downstream analysis. Boundary effects occur due to optical effects when the intensity is uneven across each tile, resulting in higher intensity found toward the center. Cross-talk occurs because the emission frequency spectra for each of the four base dyes partly overlap, creating uncertainty. Finally, for previous sequencing cycle methods T fluorophore accumulation was an issue, where incomplete removal of the dye coupled to thymine lead to an ambient accumulation the nucleotides, causing a false high Thymine trend.\n\n\n\nBase calling errors\n\n\n\n\nMismatches per cycle\nAligning reads to a high-quality reference genome can provide insight to the quality of a sequencing run by showing you the mismatches to the reference sequence. This can help you detect cycle-specific errors. Mismatches can occur due to two main causes, sequencing errors and differences between your sample and the reference genome, which is important to bear in mind when interpreting mismatch graphs. The figure below shows an example of a good run (left) and a bad one (right). In the graph on the left, the distribution of the number of mismatches is even between the cycles, which is what we would expect from a good run. However, in the graph on the right, two cycles stand out with a lot of mismatches compared to the other cycles.\n\n\n\nMissmatches per cycle\n\n\n\nGC content\nIt is a good idea to compare the GC content of the reads against the expected distribution in a reference sequence. The GC content varies between species, so a shift in GC content like the one seen below could be an indication of sample contamination. In the left graph below, we can see that the GC content of the sample is about the same as for the reference, at ~38%. However, in the right graph, the GC content of the sample is closer to 55%, indicating that there is an issue with this sample.\n\n\n\nGC content\n\n\n\n\n\nGC content by cycle\nLooking at the GC content per cycle can help detect if the adapter sequence was trimmed. For a random library, it is expected to be little to no difference between the different bases of a sequence run, so the lines in this plot should be parallel with each other like in the graph on the left below.\nIn the graph on the right, the initial spikes are likely due to adapter sequences that have not been removed.\n\n\n\nGC content by cycle\n\n\n\n\nFragment size\nFor paired-end sequencing the size of DNA fragments also matters. In the first of the examples below, the fragment size peaks around 440 bp. In the second however, there is also a peak at around 200 bp. This indicates that there was an issue with the fragment size selection during library prep.\n\n\n\nFragment size distribution\n\n\nQ1: The figure below is from a 100bp paired-end sequencing. Can you spot any problems?\n\n\n\nInsert size\n\n\n\n\nInsertions/Deletions per cycle\nSometimes, air bubbles occur in the flow cell, which can manifest as false indels. The spike in the right image provides an example of how this can look.\n\n\n\nIndels per cycle\n\n\n\n\nGenerating QC stats\nNow let’s try this out! We will generate QC stats for two lanes of Illumina paired-end sequencing data from yeast. The reads have already been aligned to the Saccromyces cerevisiae reference genome to produce the BAM file lane1.sorted.bam.\nNow we will use samtools stats to generate the stats for the primary alignments. The option -f can be used to filter reads with specific tags, while -F can be used to filter out reads with specific tags. The following command will include only primary alignments:\nsamtools stats -F SECONDARY data/lane1.sorted.bam &gt; data/lane1.sorted.bam.bchk\nHave a look at the first 47 lines of the statistics file that was generated:\nhead -n 47 data/lane1.sorted.bam.bchk\nThis file contains a number of useful stats that we can use to get a better picture of our data, and it can even be plotted with plot-bamstats, as you will see soon. First let’s have a closer look at some of the different stats. Each part of the file starts with a # followed by a description of the section and how to extract it from the file. Let’s have a look at all the sections in the file:\ngrep ^'#' data/lane1.sorted.bam.bchk | grep 'Use'\n\n\nSummary Numbers (SN)\nThis initial section contains a summary of the alignment and includes some general statistics. In particular, you can see how many bases mapped, and how much of the genome that was covered.\nNow look at the output and try to answer the questions below.\nQ2: What is the total number of reads?\nQ3: What proportion of the reads were mapped?\nQ4: How many pairs were mapped to a different chromosome?\nQ5: What is the insert size mean and standard deviation?\nQ6: How many reads were paired properly?\n\n\nGenerating QC plots\nFinally, we will create some QC plots from the output of the stats command using the command plot-bamstats which is included in the samtools package:\nplot-bamstats -p data/lane1-plots/ data/lane1.sorted.bam.bchk\nNow in your web browser open the file lane1-plots/index.html to view the QC information.\nQ7: How many reads have zero mapping quality?\nQ8: Which read (forward/reverse) of the first fragments and second fragments are higher base quality on average?\nNow continue to the next section of the tutorial: File conversion.",
    "crumbs": [
      "Home",
      "File formats",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_exercises.html#file-conversion",
    "href": "course_modules/Module1/module1_exercises.html#file-conversion",
    "title": "Exercises",
    "section": "File conversion",
    "text": "File conversion\nIn this section we are going to look at how to convert from one file format to another. There are many tools available for converting between file formats, and we will use some of the most common ones: samtools, bcftools and Picard.\n\nSAM to BAM\nTo convert from SAM to BAM format we are going to use the samtools view command. In this instance, we would like to include the SAM header, so we use the -h option:\nsamtools view -h data/NA20538.bam &gt; data/NA20538.sam\nNow, have a look at the first ten lines of the SAM file. They should look like they did in the previous section when you viewed the BAM file header.\nhead data/NA20538.sam\nWell that was easy! And converting SAM to BAM is just as straightforward. This time there is no need for the -h option, however we have to tell samtools that we want the output in BAM format.\nWe do so by adding the -b option:\nsamtools view -b data/NA20538.sam &gt; data/NA20538_2.bam\nSamtools is very well documented, so for more usage options and functions, have a look at the samtools manual http://www.htslib.org/doc/samtools-1.0.html.\n\n\nBAM to CRAM\nThe samtools view command can be used to convert a BAM file to CRAM format. In the data directory there is a BAM file called yeast.bam that was created from S. cerevisiae Illumina sequencing data. There is also a reference genome in the directory, called Saccharomyces_cerevisiae.EF4.68.dna.toplevel.fa. For the conversion, an index file (.fai) for the reference must be created. This can be done using samtools faidx. However, as we will see, samtools will generate this file on the fly when we specify a reference file using the -F option.\nTo convert to CRAM, we use the -C option to tell samtools we want the output as CRAM, and the -T option to specify what reference file to use for the conversion. We also use the -o option to specify the name of the output file. Give this a try:\nsamtools view -C -T data/Saccharomyces_cerevisiae.EF4.68.dna.toplevel.fa -o data/yeast.cram data/yeast.bam\nHave a look at what files were created:\nls -l data\nAs you can see, this has created an index file for the reference genome called Saccharomyces_cerevisiae.EF4.68.dna.toplevel.fa.fai and the CRAM file yeast.cram.\nQ1: Since CRAM files use reference-based compression, we expect the CRAM file to be smaller than the BAM file. What is the size of the CRAM file?\nQ2: Is your CRAM file smaller than the original BAM file?\nTo convert CRAM back to BAM, simply change -C to -b and change places for the input and output CRAM/BAM:\nsamtools view -b -T data/Saccharomyces_cerevisiae.EF4.68.dna.toplevel.fa -o data/yeast.bam data/yeast.cram\n\n\nFASTQ to SAM\nSAM format is mainly used to store alignment data, however in some cases we may want to store\nthe unaligned data in SAM format and for this we can use the picard tools FastqToSam application.\nPicard tools is a Java application that comes with a number of useful options for manipulating high-throughput sequencing data. .\nTo convert the FASTQ files of lane 13681_1#18 to unaligned SAM format, run:\npicard FastqToSam F1=data/13681_1#18_1.fastq.gz F2=data/13681_1#18_2.fastq.gz O=data/13681_1#18.sam SM=13681_1#18\nFrom here you can go on and convert the SAM file to BAM and CRAM, as described previously.\nThere are also multiple options for specifying what metadata to include in the SAM header. To see all available options, run:\npicard FastqToSam -h\n\n\nCRAM to FASTQ\nIt is possible to convert CRAM to FASTQ directly using the samtools fastq command. However, for many applications we need the fastq files to be ordered so that the order of the reads in the first file match the order of the reads in the mate file. For this reason, we first use samtools collate to produce a collated BAM file.\nsamtools collate data/yeast.cram data/yeast.collated\nThe newly produced BAM file will be called yeast.collated.bam. Let’s use this to create two FASTQ files, one for the forward reads and one for the reverse reads:\nsamtools fastq -1 data/yeast.collated_1.fastq -2 data/yeast.collated_2.fastq data/yeast.collated.bam\nFor further information and usage options, have a look at the samtools manual page (http://www.htslib.org/doc/samtools.html).\n\n\nVCF to BCF\nIn a similar way that samtools view can be used to convert between SAM, BAM and CRAM, bcftools view can be used to convert between VCF and BCF. To convert the file called 1kg.bcf to a compressed VCF file called 1kg.vcf.gz, run:\nbcftools view -O z -o data/1kg.vcf.gz data/1kg.bcf\nThe -O option allows us to specify in what format we want the output, compressed BCF (b), uncompressed BCF (u), compressed VCF (z) or uncompressed VCF (v). With the -o option we can select the name of the output file.\nHave a look at what files were generated (the options -lrt will list the files in reverse chronological order):\nls -lrt data\nThis also generated an index file, 1kg.bcf.csi.\nTo convert a VCF file to BCF, we can run a similar command. If we want to keep the original BCF, we need to give the new one a different name so that the old one is not overwritten:\nbcftools view -O b -o data/1kg_2.bcf data/1kg.vcf.gz\nCongratulations you have reached the end of the Data formats and QC tutorial!",
    "crumbs": [
      "Home",
      "File formats",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6.html",
    "href": "course_modules/Module6/module6.html",
    "title": "Overview",
    "section": "",
    "text": "Module Title\nRNA-Seq\n\n\nAuthors\nThis tutorial was written by Victoria Offord based on materials from Adam Reid.\n\n\nDuration\n3 hours\n\n\nKey topics\n\n\nLearning outcomes\nBy the end of this tutorial you can expect to be able to:\n• Align RNA-Seq reads to a reference genome and a transcriptome\n• Visualise transcription data using standard tools\n• Perform QC of NGS transcriptomic data\n• Quantify the expression values of your transcripts using standard tools\n\n\nActivities\n\nLecture: 1 hour\nPractical exercises: 1.5 hours\nAssessment quiz: 0.5 hours\n\n\n\nManual\nModule manual\nExercises\nSolutions\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual. Instructions on how to that will be added here.)\n\n\nLecture recording, notes and scripts\n\n\nCheck you knowledge quiz\nQuestions\n\n\nSummary of this module\n\n\nTeaching guidance\nInstructor notes\n\n\nDatasets\nPractice files for this module can be found on Github.\n\n\nPlatform Guidance\nGuide",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Overview"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html",
    "href": "course_modules/Module6/module6_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "RNA sequencing (RNA-Seq) is a high-throughput method used to profile the transcriptome, quantify gene expression and discover novel RNA molecules. This tutorial uses RNA sequencing of malaria parasites to walk you through transcriptome visualisation, performing simple quality control checks and will show you how to profile transcriptomic differences by identifying differentially expressed genes.\nFor an introduction to RNA-Seq principles and best practices see:\nA survey of best practices for RNA-Seq data analysis, Ana Conesa, Pedro Madrigal, Sonia Tarazona, David Gomez-Cabrero, Alejandra Cervera, Andrew McPherson, Michał Wojciech Szcześniak, Daniel J. Gaffney, Laura L. Elo, Xuegong Zhang and Ali Mortazavi, Genome Biol. 2016 Jan 26;17:13 doi:10.1186/s13059-016-0881-8\n\n\n\nThis tutorial comprises the following sections:\n1. Introducing the tutorial dataset\n2. Mapping RNA-Seq reads to the genome with HISAT2\n3. Visualising transcriptomes with IGV\n4. Transcript quantification with Kallisto\n5. Identifying differentially expressed genes with Sleuth\n6. Interpreting the results\n7. Key aspects of differential expression analysis\nIn this tutorial, we are going to use the following software versions:\n\nHISAT2 v2.1.0\nsamtools v1.10\nIGV v2.7.2\nkallisto v0.46.2\nR v4.0.2\nsleuth v0.30.0\nbedtools v2.29.2\n\n\n\n\nThis tutorial assumes that you have the following software or packages and their dependencies installed on your computer. The software or packages used in this tutorial may be updated from time to time so, we have also given you the version which was used when writing the tutorial.\n\n\n\nYou can find the data for this tutorial by typing the following command in a new terminal window.\ncd /home/manager/course_data/rna_seq\nNow, let’s head to the first section of this tutorial which will be introducing the tutorial dataset.",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#rna-seq-expression-analysis",
    "href": "course_modules/Module6/module6_exercises.html#rna-seq-expression-analysis",
    "title": "Exercises",
    "section": "",
    "text": "RNA sequencing (RNA-Seq) is a high-throughput method used to profile the transcriptome, quantify gene expression and discover novel RNA molecules. This tutorial uses RNA sequencing of malaria parasites to walk you through transcriptome visualisation, performing simple quality control checks and will show you how to profile transcriptomic differences by identifying differentially expressed genes.\nFor an introduction to RNA-Seq principles and best practices see:\nA survey of best practices for RNA-Seq data analysis, Ana Conesa, Pedro Madrigal, Sonia Tarazona, David Gomez-Cabrero, Alejandra Cervera, Andrew McPherson, Michał Wojciech Szcześniak, Daniel J. Gaffney, Laura L. Elo, Xuegong Zhang and Ali Mortazavi, Genome Biol. 2016 Jan 26;17:13 doi:10.1186/s13059-016-0881-8\n\n\n\nThis tutorial comprises the following sections:\n1. Introducing the tutorial dataset\n2. Mapping RNA-Seq reads to the genome with HISAT2\n3. Visualising transcriptomes with IGV\n4. Transcript quantification with Kallisto\n5. Identifying differentially expressed genes with Sleuth\n6. Interpreting the results\n7. Key aspects of differential expression analysis\nIn this tutorial, we are going to use the following software versions:\n\nHISAT2 v2.1.0\nsamtools v1.10\nIGV v2.7.2\nkallisto v0.46.2\nR v4.0.2\nsleuth v0.30.0\nbedtools v2.29.2\n\n\n\n\nThis tutorial assumes that you have the following software or packages and their dependencies installed on your computer. The software or packages used in this tutorial may be updated from time to time so, we have also given you the version which was used when writing the tutorial.\n\n\n\nYou can find the data for this tutorial by typing the following command in a new terminal window.\ncd /home/manager/course_data/rna_seq\nNow, let’s head to the first section of this tutorial which will be introducing the tutorial dataset.",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#introducing-the-tutorial-dataset",
    "href": "course_modules/Module6/module6_exercises.html#introducing-the-tutorial-dataset",
    "title": "Exercises",
    "section": "Introducing the tutorial dataset",
    "text": "Introducing the tutorial dataset\nWorking through this tutorial, you will investigate the effect of vector transmission on gene expression of the malaria parasite. The dataset you will be using for this tutorial and Figure 1 have been taken from the following publication:\nSpence, P., Jarra, W., Lévy, P. et al. Vector transmission regulates immune control of Plasmodium virulence. Nature 498, 228–231 (2013). https://doi.org/10.1038/nature12231\n\n\n\nFigure 1. Serial blood passage increases virulence of malaria parasites.\n\n\n\nIs the transcriptome of a mosquito-transmitted parasite different from one which has not passed through a mosquito?\nThe key reason for asking this question is that parasites which are transmitted by mosquito (MT) are less virulent (severe/harmful) than those which are serially blood passaged (SBP) in the laboratory.\nFigure 1A shows the malaria life cycle, the red part highlighting the mosquito stage.\nFigure 1B shows the difference in virulence, measured by blood parasitemia (presence of parasites in the blood), between mosquito-transmitted and serially blood passaged parasites.\nFigure 1C shows that increasing numbers of blood passage post mosquito transmission results in increasing virulence, back to around 20% parasitemia. Subsequent mosquito transmission of high virulence parasites render them low virulence again.\nWe hypothesise that parasites which have been through the mosquito are somehow better able to control the mosquito immune system than those which have not. This control of the immune system would result in lower parasitemia because this is advantageous for the parasite. Too high a parasitemia is bad for the mouse and therefore bad for the parasite.\n\n\nExercise 1\nIn this tutorial, you will be analysing five RNA samples, each of which has been sequenced on an Illumina HiSeq sequencing machine. There are two conditions: serially blood-passaged parasites (SBP) and mosquito transmitted parasites (MT). One with three biological replicates (SBP), one with two biological replicates (MT).\n\n\n\nSample name\nExperimental condition\nReplicate number\n\n\n\n\nMT1\nmosquito transmitted parasites\n1\n\n\nMT2\nmosquito transmitted parasites\n2\n\n\nSBP1\nserially blood-passaged parasites\n1\n\n\nSBP2\nserially blood-passaged parasites\n2\n\n\nSBP3\nserially blood-passaged parasites\n3\n\n\n\nCheck that you can see the tutorial FASTQ files in the data directory.\nls data/*.fastq.gz\nThe FASTQ files contain the raw sequence reads for each sample. There are four lines per read:\n1. Header\n2. Sequence\n3. Separator (usually a ‘+’)\n4. Encoded quality value\nTake a look at one of the FASTQ files.\nzless data/MT1_1.fastq.gz | head\nFind out more about FASTQ formats at https://en.wikipedia.org/wiki/FASTQ_format.\n\n\nQuestions\n2.3.1 Q1: Why is there more than one FASTQ file per sample?\nHint: think about why there is a MT1_1.fastq.gz and a MT1_2.fastq.gz\n2.3.2 Q2: How many reads were generated for the MT1 sample?\nHint: we want the total number of reads from both files (MT1_1.fastq.gz and MT1_2.fastq.gz) so perhaps think about the FASTQ format and the number of lines for each read or whether there’s anything you can use in the FASTQ header to search and count…\nNow let’s move on to mapping RNA-Seq reads to the genome using HISAT2.",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#mapping-rna-seq-reads-to-the-genome-using-hisat2",
    "href": "course_modules/Module6/module6_exercises.html#mapping-rna-seq-reads-to-the-genome-using-hisat2",
    "title": "Exercises",
    "section": "Mapping RNA-Seq reads to the genome using HISAT2",
    "text": "Mapping RNA-Seq reads to the genome using HISAT2\n\nIntroduction\nFor this exercise, we have reduced the number of reads in each sample to around 2.5 million to reduce the mapping time. However, this is sufficient to detect most differentially expressed genes.\nThe objectives of this part of the tutorial are:\n• use HISAT2 to build an index from the reference genome\n• use HISAT2 to map RNA-Seq reads to the reference genome\n\n\nMapping RNA-Seq reads to a genome\nBy this stage, you should have already performed a standard NGS quality control check on your reads to see whether there were any issues with the sample preparation or sequencing. In the interest of time, we won’t be doing that as part of this tutorial, but feel free to use the tools from earlier modules to give that a go later if you have time.\nNext, we map our RNA-Seq reads to a reference genome to get context. This allows you to visually inspect your RNA-Seq data, identify contamination, novel exons and splice sites as well as giving you an overall feel for your transcriptome.\nHISAT2\nTo map the RNA-Seq reads from our five samples to the reference genome, we will be using HISAT2, a fast and sensitive splice-aware aligner. HISAT2 compresses the genome using an indexing scheme based on the Burrows-Wheeler transform (BWT) and Ferragina-Manzini (FM) index to reduce the amount of space needed to store the genome. This also makes the genome quick to search, using a whole-genome FM index to anchor each alignment and then tens of thousands local FM indexes for very rapid extensions of these alignments.\nFor more information, and to find the original version of Figure 2, please see the HISAT paper:\nHISAT: a fast spliced aligner with low memory requirements, Daehwan Kim, Ben Langmead and Steven L Salzberg, Nat Methods. 2015 Apr;12(4):357-60. doi:10.1038/nmeth.3317\nHISAT2 is a splice-aware aligner which means it takes into account that when a read is mapped it may be split across multiple exons with (sometimes large) intronic gaps between aligned regions.\n\n\n\nFigure 2. Read types and their relative proportions from 20 million simulated 100-bp reads\n\n\nAs you can see in Figure 2, HISAT2 splits read alignments into five classes based on the number of exons the read alignment is split across and the length of the anchor (longest continuously mapped portion of a split read):\n• Aligns to a single exon (M)\n• Alignment split across 2 exons with long anchors over 15bp (2M_gt_15)\n• Alignment split across 2 exons with intermediate anchors between 8bp and 15bp (2M_8_15)\n• Alignment split across 2 exons with short anchors less than 7bp (2M_1_7)\n• Alignment split across more than 2 exons (gt_2M)\nHISAT2 used the global index to place the longest continuously mapped portion of a read (anchor).\nThis information is then used to identify the relevant local index. In most cases, HISAT2 will only need to use a single local index to place the remaining portion of the read without having to search the rest of the genome.\nFor the human genome, HISAT2 will build a single global index and 48,000 local FM indexes. Each of the local indexes represents a 64kb genomic region. The majority of human introns are significantly shorter than 64kb, so &gt;90% of human introns fall into a single local index. Moreover, each of the local indexes overlaps its neighbour by ~1kb which means that it also has the ability to detect reads spanning multiple indexes.\nFigure 2. Read types and their relative proportions from 20 million simulated 100-bp reads\nThere are five HISAT2 RNA-seq read mapping categories: (i) M, exonic read; (ii) 2M_gt_15, junction reads with long, &gt;15-bp anchors in both exons; (iii) 2M_8_15, junction reads with intermediate, 8- to 15-bp anchors; (iv) 2M_1_7, junction reads with short, 1- to 7-bp, anchors; and (v) gt_2M, junction reads spanning more than two exons (Figure 2A). Exoninc reads span only a single exon and represent over 60% of the read mappings in the 20 million 100-bp simulated read dataset.\n\n\nExercise 2\nBe patient, each of the following steps will take a couple of minutes!\nLook at the usage instructions for hisat2-build.\nhisat2-build -h\nThis not only tells us the version of HISAT2 we’re using (essential for publication methods):\nHISAT2 version 2.1.0 by Daehwan Kim\nBut, that we also need to give histat2-build two pieces of information:\nUsage: hisat2-build [options]* &lt;reference_in&gt; &lt;ht2_index_base&gt;\nThese are:\n• &lt;reference_in&gt; location of our reference sequence file (PccAS_v3_genome.fa)\n• &lt;ht2_index_base&gt; what we want to call our HISAT2 index files (PccAS_v3_hisat2.idx)\nBuild a HISAT2 index for our Plasmodium chabaudi chabaudi AS (P. chabaudi) reference genome using hisat2-build.\nhisat2-build data/PccAS_v3_genome.fa data/PccAS_v3_hisat2.idx\nYou can see the generated index files using:\nls data/PccAS_v3_hisat2.idx*\nLook at the usage for hisat2.\nhisat2 -h\nHere we can see that hisat2 needs several bits of information so that it can do the mapping:\nhisat2 [options]* -x &lt;ht2-idx&gt; {-1 &lt;m1&gt; -2 &lt;m2&gt; | -U &lt;r&gt;} [-S &lt;sam&gt;]\n• -x &lt;ht2-idx&gt; the prefix that we chose for our index files with hisat2-build (PccAS_v3_hisat2.idx)\n• {-1 &lt;m1&gt; -2 &lt;m2&gt; | -U &lt;r&gt;} the left (-1) and right (-2) read files for the sample (MT1_1.fastq and MT1_2.fastq respectively\n• [-S &lt;sam&gt;] the name of the file we want to write the output alignment to (MT1.sam) as, by default, hisat2 will print the results to the terminal (stdout)\nWe will also be adding one more piece of information, the maximum intron length (default 500,000 bases). For this analysis, we want to set the maximum intron length to 10,000. We can do this by adding the option –max-intronlen 10000.\nMap the reads for the MT1 sample using HISAT2.\nhisat2 --max-intronlen 10000 -x data/PccAS_v3_hisat2.idx -1 data/MT1_1.fastq.gz -2 data/MT1_2.fastq.gz -S data/MT1.sam\nHISAT2 has written the alignment in SAM format. This is a format which allows humans to look at our alignments. However, we need to convert the SAM file to its binary version, a BAM file. We do this for several reasons. Mainly we do it because most downstream programs require our alignments to be in BAM format and not SAM format. However, we also do it because the BAM file is smaller and so takes up less (very precious!) storage space. For more information, see the format guide:\nhttp://samtools.github.io/hts-specs/SAMv1.pdf.\nConvert the SAM file to a BAM file.\nsamtools view -b -o data/MT1.bam data/MT1.sam\nWe now need to sort the BAM file ready for indexing. When we aligned our reads with HISAT2, alignments were produced in the same order as the sequences in our FASTQ files. To index the BAM file, we need the alignments ordered by their respective positions in the reference genome. We can do this using samtools sort to sort the alignments by their co-ordinates for each chromosome.\nSort the BAM file.\nsamtools sort -o data/MT1_sorted.bam data/MT1.bam\nNext, we need to index our BAM file. This makes searching the alignments much more efficient.\nIt allows programs like IGV (which we will be using to visualise the alignment) to quickly get the alignments that overlap the genomic regions you’re looking at. We can do this with samtools index which will generate an index file with the extension .bai.\nIndex the BAM file so that it can be read efficiently by IGV.\nsamtools index data/MT1_sorted.bam\nNow, repeat this process of mapping, converting (SAM to BAM), sorting and indexing with the reads from the MT2 sample. You can run the previous steps as a single command.\nhisat2 –max-intronlen 10000 -x data/PccAS_v3_hisat2.idx -1 data/MT2_1.fastq.gz -2 data/MT2_2.fastq.gz | samtools view -b -| samtools sort -o data/MT2_sorted.bam - && samtools index data/MT2_sorted.bam\nLet’s not forget our SBP samples. We’ve already provided the BAM files for these samples.\nPlease check that you can see the BAM files before continuing.\nls -al data/SBP*.bam\nYou should see three files:\ndata/SBP1_sorted.bam\ndata/SBP2_sorted.bam\ndata/SBP3_sorted.bam\nIf you can’t see these files, please let your instructor know!\nWe’ve previously shown you how to run HISAT2 and samtools with individual and one-line commands. For the SBP samples, a bash script was used to generate our genome alignments prior to this tutorial.\nTo take a look at the script you can run:\nless data/map_SBP_samples.sh\nIf you have time at the end of the tutorial, feel free to take a closer look at the script and a more detailed breakdown of what it does in Running commands on multiple samples. Bash scripts and loops are a useful way of automating an analysis and running the same commands for multiple samples. Imagine if you had 50 samples and not 5! In truth, you’d probably want to run that many samples on a compute cluster, in parallel. But that’s outside the scope of this tutorial.\n\n\nQuestions\n3.3.1 Q1: How many index files were generated when you ran hisat2-build?\nHint: look for the files with the .ht2 extension\n3.3.2 Q2: What was the overall alignment rate for each of the MT samples (MT1 and MT2) to the reference genome?\nHint: look at the the output from the hisat2 commands\n3.3.3 Q3: How many MT1 and MT2 reads were not aligned to the reference genome?\nHint: look at the the output from the hisat2 commands, you’re looking for reads (not read pairs) which have aligned 0 times (remember that one read from a pair may map even if the other doesn’t)",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#visualising-transcriptomes-with-igv",
    "href": "course_modules/Module6/module6_exercises.html#visualising-transcriptomes-with-igv",
    "title": "Exercises",
    "section": "Visualising transcriptomes with IGV",
    "text": "Visualising transcriptomes with IGV\n\nIntroduction\nIntegrative Genome Viewer (IGV) allows us to visualise genomic datasets. We have provided a quick start guide which contains the information you need to complete Exercise 3. There is also an IGV user guide online which contains more information on all of the IGV features and functions: http://software.broadinstitute.org/software/igv/UserGuide.\nThe objectives of this part of the tutorial are:\n• load a reference genome into IGV and navigate the genome\n• load an annotation file into IGV and explore gene structure\n• load read alignments into IGV and inspect read alignments\n\n\nExercise 3\nFirst, we will use samtools to create an index for the P. chabaudi reference genome, which IGV will use to traverse the genome. This index file will have the extension .fai and should always be in the same directory as the reference genome.\nFirst, index the genome fasta file (required by IGV).\nsamtools faidx data/PccAS_v3_genome.fa\nNow, start IGV.\nigv &\nThis will open the IGV main window. Now, we need to tell IGV which genome we want to use. IGV has many pre-loaded genomes available, but P. chabaudi is not one of them. This means we will need to load our genome from a file.\nLoad your reference genome into IGV. Go to “Genomes -&gt; Load Genome from File…”. Select “PccAS_v3_genome.fa” and click “Open”. For more information, see Loading a reference genome in our quick start guide.\nWe not only want to see where our reads have mapped, but what genes they have mapped to. For this, we have an annotation file in GFF3 format. This contains a list of features, their co-ordinates and orientations which correspond to our reference genome.\n\n\n\nExample from PccAS_v3 GFF3\n\n\nLoad your annotation file into IGV. Go to ”“File -&gt; Load from File…”. Select “PccAS_v3.gff3” and click “Open”. For more information, see Loading gene annotations in our quick start guide.\nThis will load a new track called “PccAS_v3.gff3”. The track is currently shown as a density plot.\nYou will need to zoom in to see individual genes.\nSearch for the gene PCHAS_0505200 by typing “PCHAS_0505200” in the search box to zoom in and centre the view on PCHAS_0505200.\n\n\n\nIGV - PCHAS_0505200\n\n\nTo get a clearer view of the gene structure, right click on the annotation track and click “Expanded”.\n\n\n\nIGV - PCHAS_0505200 expanded\n\n\nIn the annotation track, genes are presented as blue boxes and lines. These boxes represent exons, while the lines represent intronic regions. Arrows indicate the direction (or strand) of transcription for each of the genes. Now we have our genome and its annotated features, we just need the read alignments for our five samples.\nLoad your alignment file for the MT1 sample into IGV. Go to ”“File -&gt; Load from File…”. Select “MT1_sorted.bam” and click “Open”. For more information, see Loading alignment files in our quick start guide.\nNote: BAM files and their corresponding index files must be in the same directory for IGV to load them properly.\n\n\n\nIGV - MT1 read alignment\n\n\nThis will load a new track called “MT1_sorted.bam” which contains the read alignments for the MT1 sample. We can change how we visualise our data by altering the view options. By default, IGV will display reads individually so they are compactly arranged. If you were to hover over a read in the default view, you will only get the details for that read. However, if we change our view so that the reads are visualised as pairs, the read pairs will be joined together by line and when we hover over either of the reads, we will get information about both of the reads in that pair.\nTo view our reads as pairs, right click on the MT1_sorted.bam alignment track and click “View as pairs”.\n\n\n\nIGV - paired view\n\n\nTo condense the alignment, right click on the MT1_sorted.bam alignment track and click “Squished”.\n\n\n\nIGV - squished view\n\n\nFor more information on sorting, grouping and visualising read alignments, see the IGV user guide.\nLoad the remaining sorted BAM files for the MT2 sample and the three SBP samples.\nUsing the search box in the toolbar, go to PCHAS_1409500. For more information, see Jump to gene or locus in our quick start guide.\n\n\n\nIGV - search PCHAS_1409500\n\n\nThe first thing to look at is the coverage range for this viewing window on the left-hand side. The three SBP samples have 2-3 times more reads mapping to this gene than the two MT samples. While at first glance it may seem like this gene may be differentially expressed between the two conditions,\nremember that some samples may have been sequenced to a greater depth than others. So, if a\nsample has been sequenced to a greater depth we would expect more reads to map in general.\n\n\n\nIGV - coverage PCHAS_1409500\n\n\nFrom the gene annotation at the bottom we can also see that there are three annotated exon/CDS features for this gene. However, the coverage plot suggests there may be a fourth unannotated exon which, given the direction of the gene, could suggest a 5’ untranslated region (UTR). Note the clean drop off of the coveraged at around position 377,070.\n\n\nQuestions\n4.3.1 Q1: How many CDS features are there in “PCHAS_1402500”?\nHint: Look at Jump to gene or locus in our quick start guide.\n4.3.2 Q2: Does the RNA-seq mapping agree with the gene model in blue?\nHint: Look at the coverage track and split read alignments.\n4.3.3 Q3: Do you think this gene is differentially expressed and is looking at the coverage\nplots alone a reliable way to assess differential expression?\nHint: Look at the coverage similarities/differences between the MT and SBP samples.",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#transcript-quantification-with-kallisto",
    "href": "course_modules/Module6/module6_exercises.html#transcript-quantification-with-kallisto",
    "title": "Exercises",
    "section": "Transcript quantification with Kallisto",
    "text": "Transcript quantification with Kallisto\n\nIntroduction\nAfter visually inspecting the genome alignment, the next step in a typical RNA-Seq analysis is to estimate transcript abundance. To do this, reads are assigned to the transcripts they came from. These assignments are then used to quantify gene or transcript abundance (expression level).\nFor this tutorial, we are using Kallisto to assign reads to a set of transcript sequences and quantify transcript abundance. Kallisto does not assemble transcripts and cannot identify novel isoforms. So, when a reference transcriptome isn’t available, the transcripts will need to be assembled de novo from the reads. However, for this tutorial, we already have a reference transcriptome available.\nThe objectives of this part of the tutorial are:\n• use Kallisto to index a transcriptome\n• use Kallisto to estimate transcript abundance\n\n\nQuantifying transcripts with Kallisto\nMany of the existing methods used for estimating transcript abundance are alignment-based. This means they rely on mapping reads onto the reference genome. The gene expression levels are then calculated by counting the number of reads overlapping the transcripts. However, read alignment is a computationally and time intensive process. So, in this tutorial, we will be running Kallisto which uses a fast, alignment-free method for transcript quantification.\nNear-optimal probabilistic RNA-seq quantification, Nicolas L Bray, Harold Pimentel, Páll Melsted and Lior Pachter Nat Biotechnol. 2016 May;34(5):525-7. doi: 10.1038/nbt.3519\n\n\n\nFigure 3. Performance of kallisto and other methods. Total running time in minutes for processing the 20 simulated data sets of 30 million paired-end reads described in a. Please see the Kallisto publication for original figure and more information.\n\n\nKallisto uses pseudoalignment to make it efficient. Rather than looking at where the reads map, Kallisto uses the compatibility between the reads and transcripts to estimate transcript abundance.\nThus, most transcript quantification with Kallisto can be done on a simple laptop (Figure 3).\nStep 1: building a Kallisto index As with alignment-based methods, Kallisto needs an index. To generate the index, Kallisto first builds a transcriptome de Bruijn Graph (T-BDG) from all of the k-mers (short sequences of k nucleotides) that it finds in the transcriptome. Each node in the graph corresponds to a k-mer and each transcript is represented by its path through the graph. Using these paths, each k-mer is assigned a k-compatibility class. Some k-mers will be redundant i.e. shared by the same transcripts. These are skipped to make the index compact and quicker to search. A great worked example of this process can be found here.\nThe command kallisto index can be used to build a Kallisto index from transcript sequences.\nkallisto index\nHere we can see the version of Kallisto that we’re using (useful for publication methods) and the information that we’ll need to give kallisto index. The only information we need to give kallisto index is the location of our transcript sequences (PccAS_v3_transcripts.fa). However, it’s useful to have a meaningful filename for the resulting index. We can add this by using the option -i which expects a value, our index prefix (PccAS_v3_kallisto).\nStep 2: estimating transcript abundance With this Kallisto index, you can use kallisto quantto estimate transcript abundances. You will need to run this command separately for each sample.\nkallisto quant\nWe can see that kallisto quant needs us to tell it where our sample read are. Although we don’t have to, it’s usually a good idea to keep the results of each quantification in a different directory. This is because the output filename are always the same (e.g. abundances.tsv). If we ran a second analysis, these could get overwritten. To use a different output directory, we can use the -o option. We will also be using the -b option for bootstrapping.\nBootstrapping Not all reads will be assigned unambiguously to a single transcript. This means there will be “noise” in our abundance estimates where reads can be assigned to multiple transcripts. Kallisto quantifies the uncertainty in its abundance estimates using random resampling and replacement. This process is called bootstrapping and indicates how reliable the expression estimates are from the observed pseudoalignment. Bootstrap values can be used downstream to distinguish the technical variability from the biological variability in your experiment.\n\n\nExercise 4\nBuild an index called PccAS_v3_kallisto from transcript sequences in PccAS_v3_transcripts.fa.\nkallisto index -i data/PccAS_v3_kallisto data/PccAS_v3_transcripts.fa\nQuantify the transcript expression levels for the MT1 sample with 100 bootstrap samples and calling the output directory MT1.\nkallisto quant -i data/PccAS_v3_kallisto -o data/MT1 -b 100 data/MT1_1.fastq.gz data/MT1_2.fastq.gz\nYou’ll find your Kallisto results in a new output directory which we called MT1. Let’s take a look.\nls data/MT1\nRunning kallisto quant generated three output files in our MT1 folder:\n• abundance.h5\nHDF5 binary file containing run info, abundance esimates, bootstrap estimates, and transcript length information length.\n• abundance.tsv\nPlain text file containing abundance estimates (doesn’t contain bootstrap estimates).\n• run_info.json\nJSON file containing information about the run.\nNote: when the number of bootstrap values (-b) is very high, Kallisto will generate a large amount of data. To help, it outputs bootstrap results in HDF5 format (abundance.h5). This file can be read directly by sleuth.\nIn the MT1/abundance.tsv file we have the abundance estimates for each gene for the MT1 sample.\nLet’s take a quick look.\nhead data/MT1/abundance.tsv\nIn MT1/abundance.tsv there are five columns which give us information about the transcript abundances for our MT1 sample.\n• target_id\nUnique transcript identifier.\n• length\nNumber of bases found in exons.\n• eff_length\nEffective length. Uses fragment length distribution to determine the effective number of positions that can be sampled on each transcript.\n• est_counts\nEstimated counts*. This may not always be an integer as reads which map to multiple tran-\nscripts are fractionally assigned to each of the corresponding transcripts.\n• tpm\nTranscripts per million. Normalised value accounting for length and sequence depth bias.\nIn the last column we have our normalised abundance value for each gene. These are our transcripts per million or TPM. If you have time at the end of this tutorial, see our normalisation guide which covers common normalisation methods and has a bonus exercise.\nTo get the result for a specific gene, we can use grep.\ngrep PCHAS_0100100 data/MT1/abundance.tsv\nIf we wanted to get the TPM value for a particular gene, we can use awk.\nawk -F\"\\t\" '$1==\"PCHAS_0100100\" {print $5}' data/MT1/abundance.tsv\nUse kallisto quant four more times, for the MT2 sample and the three SBP samples.\n\n\nQuestions\n5.3.1 Q1: What k-mer length was used to build the Kallisto index?\nHint: look at the terminal output from kallisto index\n5.3.2 Q2: How many transcript sequences are there in PccAS_v3_transcripts.fa?\nHint: you can use grep or look at the terminal output from kallisto quant or in the run_info.json files\n5.3.3 Q3: What is the transcripts per million (TPM) value for PCHAS_1402500 in each of the samples?\nHint: use grep to look at the abundance.tsv files\n5.3.4 Q4: Do you think PCHAS_1402500 is differentially expressed?",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#identifying-differentially-expressed-genes-with-sleuth",
    "href": "course_modules/Module6/module6_exercises.html#identifying-differentially-expressed-genes-with-sleuth",
    "title": "Exercises",
    "section": "Identifying differentially expressed genes with Sleuth",
    "text": "Identifying differentially expressed genes with Sleuth\n\nIntroduction\nIn the previous sections, we have quantified our transcript abundance and looked at why counts are normalised. In this section, you will be using sleuth to do some simple quality checks and get a first look at the results.\nThe objectives of this part of the tutorial are:\n• use sleuth to perform quality control checks\n• use sleuth to identify differentially expressed (DE) transcripts\n• use sleuth to investigate DE transcripts\n\n\nDifferential expression analysis (DEA)\nDifferential expression analysis tries to identify genes whose expression levels differ between experimental conditions. We don’t normally have enough replicates to do traditional tests of significance for RNA-Seq data. So, most methods look for outliers in the relationship between average abundance and fold change and assume most genes are not differentially expressed.\nRather than just using a fold change threshold to determine which genes are differentially expressed, DEAs use a variety of statistical tests for significance. These tests give us a p-value which is an estimate of how often your observations would occur by chance.\nHowever, we perform these comparisons for each one of the thousands of genes/transcripts in our dataset. A p-value of 0.01 estimates a probability of 1% for seeing our observation just by chance. In an experiment like ours with 5,000 genes we would expect 5 genes to be significantly differentially expressed by chance (i.e. even if there were no difference between our conditions). Instead of using a p-value we use a q-value to account for multiple testing and adjusts the p-value accordingly.\n\n\nsleuth\nsleuth is a companion tool for Kallisto. Unlike most other tools, sleuth can utilize the technical variation information generated by Kallisto so that you can look at both the technical and biological variation in your dataset.\nFor the DEA, sleuth essentially tests two models, one which assumes that the abundances are equal between the two conditions (reduced) and one that does not (full). To identify DE transcripts it identifies those with a significantly better fit to the “full” model. For more information on sleuth and how it works, see Lior Pachter’s blog post A sleuth for RNA-Seq (https://liorpachter.word-press.com/2015/08/17/a-sleuth-for-rna-seq/). sleuth is written in the R statistical programming language, as is almost all RNA-Seq analysis software. Helpfully, it produces a web page that allows interactive graphical analysis of the data. However, we strongly recommend learning R for anyone doing a significant amount of RNA-seq analysis.\nIt is nowhere near as hard to get started with as full-blown programming languages such as Perl or Python!\n\n\nExercise 5\nFor this tutorial, we’ve provided a series of R commands as an R script that will get sleuth running.\n\n\nRunning sleuth\nThe commands we need to run sleuth are in the file sleuth.R. There’s a great overview of the commands and what they do by the developers of sleuth here: https://pachterlab.github.io/sleuth_walk-throughs/trapnell/analysis.html. Using R is not as hard as it seems, most of this script was copied from the manual!\nOpen sleuth.R and have a quick look at the commands.\ncat data/sleuth.R\nYou may also want to have a look at hiseq_info.txt which is where we define which condition each sample is associated with.\ncat data/hiseq_info.txt\nYou can run scripts containing R commands using Rscript followed by the script name. Run sleuth.R.\nRscript data/sleuth.R\nYou won’t see any output from this script in the notebook, just a * next to the command input ([*]) to let you know it’s running.\nIf you were to run the script directly on the command line, sleuth will return a link which you can follow (http://127.0.0.1:42427). This will take you to a web page where you can navigate and explore the sleuth results.\nType the URL below into your a web browser (e.g. chrome or firefox) to open the sleuth results.\nhttp://127.0.0.1:42427\nYou should now see a page with the heading “sleuth live”. If not, just give the script a little longer and then refresh the page.\n\n\nUsing sleuth to quality check (QC) transcript quanification\nQuality control checks are absolutely vital at every step of the experimental process. We can use sleuth to perform simple quality checks (QC) on our dataset.\nAt the top of the page, sleuth provides several tabs which we can use to determine whether the data is of good quality and whether we should trust the results we get.\nFirst, lets take a look at a summary of our dataset.\nIn the web page that has been launched, click on “summaries -&gt; processed data”.\nNotice that the number of reads mapping differs quite a bit between MT and SBP samples? This is why we QC our data. In the MT samples &gt;95% of the reads mapped to the genome, but only 15-30% are assigned to the transcriptome compared to &gt;75% for the SBP samples. This suggests that there may be some residual ribosomal RNA left over from the RNA preparation. It’s not a problem as we have enough reads and replicates for our analysis.\n\n\n\nsleuth - processed data table\n\n\nIn some cases, we can identify samples which don’t agree with other replicates (outliers) and samples which are related by experimental bias (batch effects). If we don’t have many replicates, it’s hard to detect outliers and batch effects meaning our power to detect DE genes is reduced.\nPrincipal component analysis (PCA) plots can be used to look at variation and strong patterns within the dataset. Batch effects and outliers often stand out quite clearly in the PCA plot and mean that you can account for them in any downstream analysis.\n\n\n\nsleuth - PCA plot\n\n\nOur samples form two condition-related clusters with the two MT samples (red) on the left and the three SBP samples on the right (blue). If we look at the variance bar plot, we can see that the first principal component (PC1) accounts for &gt;90% of the variation in our dataset. As the samples are clearly clustered on the x-axis (PC1) this suggests that most of the variation in the dataset is related to our experimental condition (Mt vs SBP).\n\n\n\nsleuth variance plot\n\n\n\n\nUsing sleuth to look at DE transcripts\nWe used the output from Kallisto to identify DE transcripts using sleuth. Let’s take a look and see if we found any.\nTo see the results of the sleuth DEA, go to “analyses -&gt; test table”.\n\n\n\nsleuth - transcript table\n\n\nThe important columns here are the q-value and the beta value (analagous to fold change). By default, the table is sorted by the q-value. We can see that our top transcript is PCHAS_0420800, a hypothetical protein/pseudogene. Now let’s take a closer look at that transcript.\nGo to “analyses -&gt; transcript view”. Enter “PCHAS_0420800” into the “transcript” search box. Click “view”.\n\n\n\nsleuth - transcript view\n\n\nOn the left you have the abundances for the MT replicates and on the right, the SBP replicates. We can see that this transcript is more highly expressed in the MT samples than in the SBP samples. This is also reflected by the fold change in the test table (b = -4.5). The b value is negative as it represents the fold change in SBP samples relative to those in the MT samples.\nFinally, let’s take a look at the gene level.\nTo see the results of the sleuth DEA, go to “analyses -&gt; test_table”. Under “table type” select “gene table”. Click on the column header “qval” in the table to sort the rows by ascending q-value.\n\n\n\nsleuth - gene table\n\n\nThe transcripts have now been grouped by their descriptions. Let’s take a closer look at the CIR proteins.\nGo to “analyses -&gt; gene view”. In the “gene” search box enter “CIR protein” (without the quotes).\n\n\n\nsleuth - gene view\n\n\nHere we can see the individual CIR protein transcript abundances. We can see that PCHAS_1100300 is more highly expressed in the SBP samples while PCHAS_0302100 and PCHAS_0302100 are more highly expressed in the MT samples.\n\n\nQuestions\n6.3.1 Q1: Is our gene from earlier, PCHAS_1402500, significantly differentially expressed?",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#interpreting-the-results",
    "href": "course_modules/Module6/module6_exercises.html#interpreting-the-results",
    "title": "Exercises",
    "section": "Interpreting the results",
    "text": "Interpreting the results\n\nIntroduction\nThe main objective of this part of the tutorial is to use simple Unix commands to get a list of significantly differentially expressed genes. Using this gene list and the quantitative information from our analysis we can then start to make biological inferences about our dataset.\nUsing the R script (sleuth.R), we printed out a file of results describing the differentially expressed genes in our dataset. This file is called kallisto.results.\nThe file contains several columns, of which the most important are:\n• Column 1: target_id (gene id)\n• Column 2: description (some more useful description of the gene than its id)\n• Column 3: pval (p value)\n• Column 4: qval (p value corrected for multiple hypothesis testing)\n• Column 5: b (fold change)\nWith a little Linux magic we can get the list of differentially expressed genes with only the columns\nof interest as above.\n\n\nExercise 6\nTo get the genes which are most highly expressed in our SBP samples, we must first filter our results. There are two columns we want to filter our data on: b (column 5) and qval (column 4).\nThese columns represent whether the gene is differentially expressed and whether that change is significant.\nThe following command will get those genes which have an adjusted p value (qval) less than 0.01 and a positive fold change. These genes are more highly expressed in the SBP samples.\nawk -F \"\\t\" '$4 &lt; 0.01 && $5 &gt; 0' data/kallisto.results | cut -f1,2,3,4,5 | head\nWe used awk to filter the gene list and print only the lines which met our search criteria (qval &gt; 0.01, b &gt; 0). The option -F tells awk what delimiter is used to separate the columns. In this case, it was a tab or its regular expression ””. We then use cut to only print out columns 1-5. You can also do that within the awk command. Finally, we use head to get the first 10 lines of the output.\nAlternatively, we can look for the genes which are more highly expressed in the MT samples.\nawk -F \"\\t\" '$4 &lt; 0.01 && $5 &lt; 0' data/kallisto.results | cut -f1,2,3,4,5 | head\nIt can be useful to have a quick look and compare gene lists. For example, whether a certain gene product is seen more often in the genes most highly expressed in one condition or another. A quick and dirty method would be to use the gene descriptions (or gene products).\nYou could extract the gene products (column 2) for genes which are more highly expressed in the SBP samples using sort and then uniq.\nawk -F \"\\t\" '$4 &lt; 0.01 && $5 &lt; 0 {print $2}' data/kallisto.results | sort | uniq\nWe can count each time these unique gene products occur in the list using uniq -c.\nawk -F \"\\t\" '$4 &lt; 0.01 && $5 &lt; 0 {print $2}' data/kallisto.results | \\\nsort | uniq -c\nAnd, if we wanted to make it a bit easier to see commonly found gene products we can sort this again by the frequency count we got from the uniq command. The sort command will put these in ascending numerical (-n) order.\nawk -F \"\\t\" '$4 &lt; 0.01 && $5 &lt; 0 {print $2}' data/kallisto.results | sort | uniq -c | sort -n\nIf you wanted to look for the frequency of a particular gene product you could also use grep.\nawk -F \"\\t\" '$4 &lt; 0.01 && $5 &lt; 0 {print $2}' data/kallisto.results | grep -c CIR\nOr building on the earlier command:\nawk -F \"\\t\" '$4 &lt; 0.01 && $5 &lt; 0 {print $2}' data/kallisto.results | sort | uniq -c | grep CIR\nIf you want to read more about this work related to this data it is published:\nVector transmission regulates immune control of Plasmodium virulence, Philip J. Spence, William Jarra, Prisca Lévy, Adam J. Reid, Lia Chappell, Thibaut Brugat,\nMandy Sanders, Matthew Berriman and Jean Langhorne, Nature. 2013 Jun 13; 498(7453): 228–231 doi:10.1038/nature12231\nQ1: How many genes are more highly expressed in the SBP samples?\nHint: try replacing head in the earlier command with another unix command to count the number of\n7.2.2 Q2: How many genes are more highly expressed in the MT samples?\nHint: try replacing head in the earlier command with another unix command to count the number of lines\n7.2.3 Q3: Do you notice any particular genes that came up in the analysis?\nHint: look for gene products that are seen more often in genes more highly expressed in the SBP samples than those more highly expressed in the MT samples",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#key-aspects-of-differential-expression-analysis",
    "href": "course_modules/Module6/module6_exercises.html#key-aspects-of-differential-expression-analysis",
    "title": "Exercises",
    "section": "Key aspects of differential expression analysis",
    "text": "Key aspects of differential expression analysis\n\nReplicates and power\nIn order to accurately ascertain which genes are differentially expressed and by how much it is necessary to use replicated data. As with all biological experiments doing it once is simply not enough.\nThere is no simple way to decide how many replicates to do, it is usually a compromise of statistical power and cost. By determining how much variability there is in the sample preparation and sequencing reactions, we can better assess how highly genes are really expressed and more accurately determine any differences. The key to this is performing biological rather than technical replicates.\nThis means, for instance, growing up three batches of parasites, treating them all identically, extracting RNA from each and sequencing the three samples separately. Technical replicates, whereby the same sample is sequenced three times do not account for the variability that really exists in biological systems or the experimental error between batches of parasites and RNA extractions.\nNote: more replicates will help improve power for genes that are already detected at high levels, while deeper sequencing will improve power to detect differential expression for genes which are expressed at low levels.\n\n\np-values vs. q-values\nWhen asking whether a gene is differentially expressed we use statistical tests to assign a p-value. If a gene has a p-value of 0.05, we say that there is only a 5% chance that it is not really differentially expressed. However, if we are asking this question for every gene in the genome (~5500 genes for Plasmodium), then we would expect to see p-values less than 0.05 for many genes even though they are not really differentially expressed. Due to this statistical problem, we must correct the p-values so that we are not tricked into accepting a large number of erroneous results. Q-values are p-values which have been corrected for what is known as multiple hypothesis testing. Therefore, it is a q-value of less than 0.05 that we should be looking for when asking whether a gene is differentially expressed.\n\n\nAlternative software\nIf you have a good quality genome and genome annotation such as for model organisms e.g. human, mouse, Plasmodium; map to the transcriptome to determine transcript abundance. This is even more relevant if you have variant transcripts per gene as you need a tool which will do its best to determine which transcript is really expressed. As well as Kallisto (Bray et al. 2016; PMID: 27043002), there is eXpress (Roberts & Pachter, 2012; PMID: 23160280) which will do this.\nAlternatively, you can map to the genome and then call abundance of genes, essentially ignoring variant transcripts. This is more appropriate where you are less confident about the genome annotation and/or you don’t have variant transcripts because your organism rarely makes them or they are simply not annotated. Tophat2 (Kim et al., 2013; PMID: 23618408), HISAT2 (Pertea et al. 2016; PMID: 27560171), STAR (Dobinet al., 2013; PMID: 23104886) and GSNAP (Wu & Nacu, 2010; PMID: 20147302) are all splice-aware RNA-seq read mappers appropriate for this task. You then need to use a tool which counts the reads overlapping each gene model. HTSeq (Anders et al., 2015; PMID: 25260700) is a popular tool for this purpose. Cufflinks (Trapnell et al. 2012; PMID:22383036) will count reads and determine differentially expressed genes.\nThere are a variety of programs for detecting differentially expressed genes from tables of RNA-seq read counts. DESeq2 (Love et al., 2014; PMID: 25516281), EdgeR (Robinson et al., 2010; PMID: 19910308) and BaySeq (Hardcastle & Kelly, 2010; PMID: 20698981) are good examples.\n\n\nWhat do I do with a gene list?\nDifferential expression analysis results are a list of genes which show differences between two conditions. It can be daunting trying to determine what the results mean. On one hand, you may find that that there are no real differences in your experiment. Is this due to biological reality or noisy data? On the other hand, you may find several thousands of genes are differentially expressed.\nWhat can you say about that?\nOther than looking for genes you expect to be different or unchanged, one of the first things to do is look at Gene Ontology (GO) term enrichment. There are many different algorithms for this, but you could annotate your genes with functional terms from GO using for instance Blast2GO (Conesa et al., 2005; PMID: 16081474) and then use TopGO (Alexa et al., 2005; PMID: 16606683) to determine whether any particular sorts of genes occur more than expected in your differentially expressed genes.",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#normalisation",
    "href": "course_modules/Module6/module6_exercises.html#normalisation",
    "title": "Exercises",
    "section": "Normalisation",
    "text": "Normalisation\n\nIntroduction\nIn the previous section, we looked at estimating transcript abundance with Kallisto. The abundances are reported as transcripts per million (TPM), but what does TPM mean and how is it calculated?\nThe objectives of this part of the tutorial are:\n• understand why RNA-Seq normalisation metrics are used\n• understand the difference between RPKM, FPKM and TPM\n• calculate RPKM and TPM for a gene of interest\nThere are many useful websites, publications and blog posts which go into much more detail about RNA-Seq normalisation methods. Here are just a couple (in no particular order):\n• What the FPKM? A review of RNA-Seq expression units\n• RPKM, FPKM and TPM, clearly explained\n• A survey of best practices for RNA-seq data analysis\n• The RNA-seq abundance zoo\n\n\nWhy do we use normalisation units instead of raw counts?\nRaw reads counts are the number of reads originating from each transcript which can be affected by several factors:\n• sequencing depth (total number of reads)\nThe more we sequence a sample, the more reads we expect to be assigned.\n• gene/transcript length\nThe longer the gene or transcript, the more reads we expect to be assigned to it.\n\n\n\nFigure 4. Effect of sequencing depth and gene length on raw read counts\n\n\nLook at the top part of Figure 4. In which sample, X or Y, is the gene more highly expressed?\nNeither, it’s the same in both. What we didn’t tell you was that the total number of reads generated for sample A was twice the number than for sample B. That meant almost twice the number of reads are assigned to the same gene in sample A than in sample B.\nLook at the bottom part of Figure 4. Which gene, X or Y, has the greatest gene level expression?\nNeither, they are both expressed at the same level. This time we didn’t tell you that gene X is twice the length of gene Y. This meant that almost twice the number reads were assigned to gene X than gene Y.\nIn the top part of Figure 4, the gene in sample X has twice the number of reads assigned to it than the same gene in sample Y. What isn’t shown is that sample X had twice the number or total reads than sample Y so we would expect more reads to be assigned in sample X. Thus, the gene is expressed at roughly the same level in both samples. In the bottom part of Figure 4, gene X has twice the number of reads assigned to it than gene Y. However, gene X is twice the length of gene Y and so we expect more reads to be assigned to gene X. Again, the expression level is roughly the same.\n\n\nReads per kilobase per million (RPKM)\nReads per kilobase (of exon) per million (reads mapped) or RPKM is a within sample normalisation method which takes into account sequencing depth and length biases.\nTo calculate RPKM, you first normalise by sequencing depth and then by gene/transcript length.\n1. Get your per million scaling factor\nCount up the total number of reads which have been assigned (mapped) in the sample. Divide this number by 1,000,000 (1 million) to get your per million scaling factor (N).\n2. Normalise for sequencing depth\nDivide the number of reads which have been assigned to the gene or transcript (C) by the per million scaling factor you calculated in step 1. This will give you your reads per million (RPM).\n3. Get your per kilobase scaling factor\nDivide the total length of the exons in your transcript or gene in base pairs by 1,000 (1 thousand) to get your per kilobase scaling factor (L).\n4. Normalise for length\nDivide your RPM value from step 2 by your per kilobase scaling factor (length of the gene/-transcript in kilobases) from step 3. This will give you your reads per kilobase per million or RPKM.\nThis can be simplified into the following equation:\n\nWhere:\n• C is number of reads mapped to the transcript or gene\n• L is the total exon length of the transcript or gene in kilobases\n• N is the total number of reads mapped in millions\n\n\nFragments per kilobase per million (FPKM)\nFragments per kilobase per million or FPKM is essentially the same as RPKM except that:\n• RPKM is designed for single-end RNA-Seq experiments\n• FPKM is designed for paired-end RNA-Seq experiments\nIn a paired-end RNA-Seq experiment, two reads may be assigned to a single fragment (in any orientation). Also, in some cases, only one of those reads will be assigned to a fragment (singleton).\nThe only difference between RPKM and FPKM is that FPKM takes into consideration that two reads may be assigned to the same fragment.\n\n\nTranscripts per million (TPM)\nCalculating the transcripts per million or TPM is a similar process to RPKM and FPKM. The main difference is that you will first normalise for length bias and then for sequencing depth bias. In a nutshell, we are swapping the order of normalisations.\n1. Get your per kilobase scaling factor\nDivide the total length of the exons in your transcript in base pairs by 1,000 (1 thousand) to get your per kilobase scaling factor.\n2. Normalise for length\nDivide the number of reads which have been assigned to the transcript by the per kilobase scaling factor you calculated in step 1. This will give you your reads per kilobase (RPK).\n3. Get the sum of all RPK values in your sample\nCalculate the RPK value for all of the transcripts in your sample. Add all of these together to get your total RPK value.\n4. Get your per million scaling factor\nDivide your total RPK value from step 3 by 1,000,000 (1 million) to get your per million scaling factor.\n5. Normalise for sequencing depth\nDivide your RPK value calculated in step 2 by the per million scaling factor from step 4. You\nnow have your transcripts per millions value or TPM.\n\n\nCalculating RPKM and TPM values\nTo try and answer this, let’s look at a worked example. Here, we have three genes (A-C) and three biological replicates (1-3).\n\n\n\nCol1\nCol2\nCol3\nCol4\nCol5\n\n\n\n\nGene\nLength\nReplicate 1\nReplicate 2\nReplicate 3\n\n\nA\n2,000 bases\n10\n12\n30\n\n\nB\n4,000 bases\n20\n25\n60\n\n\nC\n1,000 bases\n5\n8\n15\n\n\n\nThere are two things to notice in our dataset:\n• Gene B has twice number reads mapped than gene A, possibly as it’s twice the length\n• Replicate 3 has more reads mapped than any of the other replicates, regardless of which gene we look at\n\n\nCalculating RPKM\nStep 1: get your per million scaling factor In the table below is the total number of reads which mapped for each of the replicates. To get our per million scaling factor, we divide each of these values by 1,000,000 (1 million).\n\n\n\nGene\nReplicate 1\nReplicate 2\nReplicate 3\n\n\n\n\nTotal reads mapped\n3,500,000\n4,500,000\n10,600,000\n\n\nPer million reads\n3.5\n4.5\n10.6\n\n\n\nStep 2: normalise for sequencing depth factor to get our reads per million (RPM).\nBefore:\nWe now divide our read counts by the per million scaling\n\n\n\nGene\nReplicate 1\nReplicate 2\nReplicate 3\n\n\n\n\nA\n10\n12\n30\n\n\nB\n20\n25\n60\n\n\nB\n5\n8\n15\n\n\n\nAfter:\n\n\n\nGene\nReplicate 1\nReplicate 2\nReplicate 3\n\n\n\n\nA\n2.857\n2.667\n2.830\n\n\nB\n5.714\n5.556\n5.660\n\n\nC\n1.429\n1.778\n1.415\n\n\n\nStep 3: get your per kilobase scaling factor Here we have our gene length in base pairs. For our per kilobase scaling factor we need to get our gene length in kilobases by dividing it by 1,000.\n\n\n\nGene\nLength (base pairs)\nLength (kilobases)\n\n\n\n\nA\n2,000\n2\n\n\nB\n4,000\n4\n\n\nC\n1,000\n1\n\n\n\nStep 4: normalise for length Finally, we divide our RPM values from step 2 by our per kilobase scaling factor from step 3 to get our reads per kilobase per million (RPKM).\nBefore:\n\n\n\nGene\nReplicate 1 RPM\nReplicate 2 RPM\nReplicate 3 RPM\n\n\n\n\nA\n2.857\n2.667\n2.830\n\n\nB\n5.714\n5.556\n5.660\n\n\nC\n1.429\n1.778\n1.415\n\n\n\nAfter:\n\n\n\nGene\nReplicate 1 RPM\nReplicate 2 RPM\nReplicate 3 RPM\n\n\n\n\nA\n1.43\n1.33\n1.42\n\n\nB\n1.43\n1.39\n1.42\n\n\nC\n1.43\n1.78\n1.42\n\n\n\nNotice that even though replicate 3 had more reads assigned than the other samples and a greater sequencing depth, its RPKM is quite similar. And, that although gene B had twice the number of reads assigned than gene A, its RPKM is the same. This is because we have normalised by both length and sequencing depth.\n\n\nCalculating TPM\nNow we’re going to calculate the TPM values for the same example data. As a reminder, here are our three genes (A-C) and three biological replicates (1-3).\n\n\n\nGene\nLength\nReplicate 1\nReplicate 2\nReplicate 3\n\n\n\n\nA\n2,000 bases\n10\n12\n30\n\n\nB\n4,000 bases\n20\n25\n60\n\n\nC\n1,000 bases\n5\n8\n15\n\n\n\nStep 1: get your per kilobase scaling factor Again, our gene lengths are in base pairs. For our per kilobase scaling factor we need to get our gene length in kilobases by dividing it by 1,000.\n\n\n\nGene\nLength (base pairs)\nLength (kilobases)\n\n\n\n\nA\n2,000\n2\n\n\nB\n4,000\n4\n\n\nC\n1,000\n1\n\n\n\nStep 2: normalise for length Now we divide the number of reads which have been assigned to each gene by the per kilobase scaling factor we just calculated. This will give us our reads per kilobase (RPK).\nBefore:\n\n\n\nGene\nReplicate 1\nReplicate 2\nReplicate 3\n\n\n\n\nA\n10\n12\n30\n\n\nB\n20\n25\n60\n\n\nC\n5\n8\n15\n\n\n\nAfter:\n\n\n\nGene\nReplicate 1\nReplicate 2\nReplicate 3\n\n\n\n\nA\n5\n6\n15\n\n\nB\n5\n6.25\n15\n\n\nC\n5\n8\n15\n\n\n\nStep 3: get the sum of all RPK values in your sample Next, we sum the RPK values for each of our replices. This will give use our total RPK value for each replicate. To make this example scalable, we assume there are other genes so the total RPK is made up.\n\n\n\nGene\nReplicate 1\nReplicate 2\nReplicate 3\n\n\n\n\nA\n5 6 15\n\n\n\n\nB\n5 6.25 15\n\n\n\n\nC\n5 8 15\n\n\n\n\n…\n\n\n\n\n\nTotal RPK\n150,000\n202,500\n450,000\n\n\n\nStep 4: get your per million scaling factor Here, instead of dividing our total mapped reads by 1,000,000 (1 million) to get our per million scaling factor, we divide our total RPK values by 1,000,000 (1 million).\n\n\n\nGene\nReplicate 1\nReplicate 2\nReplicate 3\n\n\nTotal RPK\n150,000\n202,500\n450,000\n\n\nPer million RPK\n0.1500\n0.2025\n0.4500\n\n\n\nStep 5: normalise for sequencing depth Finally, we divide our individual RPK values from step 2 by the per million scaling factor in step 4 to give us our TPM values.\nBefore:\n\n\n\nGene\nReplicate 1\nReplicate 2\nReplicate 3\n\n\n\n\nA\n5\n6\n15\n\n\nB\n5\n6.25\n15\n\n\nC\n5\n8\n15\n\n\n\nAfter:\n\n\n\nGene\nReplicate 1\nReplicate 2\nReplicate 3\n\n\n\n\nA\n33.33\n29.63\n33.33\n\n\nB\n33.33\n30.86\n33.33\n\n\nC\n33.33\n39.51\n33.33\n\n\n\n\n\nWhich normalisation unit should I use?\nThere’s a lot of debate around this, so let’s look at our total normalised values for each replicate.\n\n\nRPKM\n\n\n\nGene\nReplicate 1 RPKM\nReplicate 2 RPKM\nReplicate 3 RPKM\n\n\n\n\nA\n1.43\n1.33\n1.42\n\n\nB\n1.43\n1.39\n1.42\n\n\nC\n1.43\n1.78\n1.42\n\n\nTotal RPKM\n4.29\n4.50\n4.25\n\n\n\n\n\nTPM\n\n\n\nGene\nReplicate 1\nReplicate 2\nReplicate 3\n\n\n\n\nA\n33.33\n29.63\n33.33\n\n\nB\n33.33\n30.86\n33.33\n\n\nC\n33.33\n39.51\n33.33\n\n\nTotal TPM\n100\n100\n100\n\n\n\nNotice that that total TPM value for each of the replicates is the same. This is not true for RPKM and FPKM where the total values differ. With TPM, having the same total value for each replicate makes it easier to compare the proportion of reads mapping to each gene across replicates (although you shouldn’t really compare across experiments). With RPKM and FPKM, the differing total values make it much harder to compare replicates.\n\n\nQuestions\nBelow is the information for each of the five samples. You will need this information to answer the questions. We have put all of commands used to get this information in the answers.\n\n\n\n\n\n\n\n\n\n\nSample\nTotal mapped reads\nTranscript length\nAssigned reads\nTotal RPK\n\n\nMT1\n2,353,750\n3,697\n2,541\n293,431\n\n\nMT2\n2,292,271\n3,709\n3,392\n675,190\n\n\nSBP1\n2,329,235\n3,699\n14,605\n1,719,970\n\n\nSBP2\n2,187,718\n3,696\n17,302\n1,429,540\n\n\nSBP3\n2,163,979\n3,699\n14,646\n1,561,310\n\n\n\nNote: values have been rounded up to integers to make calculations easier. Assigned reads are the est_count from Kallisto for PCHAS_1402500. Transcript lengths are the est_length from Kallisto for PCHAS_1402500.\nQ1: Using the abundance.tsv files generated by Kallisto and the information above, calculate the RPKM for PCHAS_1402500 in each of our five samples.\n\n\n\n\n\n\n\n\n\n\nSample\nPer million scaling factor\nRPM\nPer kilobase scaling factor\nRPKM\n\n\n\n\nMT1\n\n\n\n\n\n\nMT2\n\n\n\n\n\n\nSBP1\n\n\n\n\n\n\nSBP2\n\n\n\n\n\n\nSBP3\n\n\n\n\n\n\n\nQ2: Using the abundance.tsv files generated by Kallisto and the information above, calculate the TPM for PCHAS_1402500 in each of our five samples.\nHint: don’t forget to get your per million scaling factor.\n\n\n\nSample\nPer million scaling factor\nReads per kilobase (RPK)\nTPM\n\n\n\n\nMT1\n\n\n\n\n\nMT2\n\n\n\n\n\nSBP1\n\n\n\n\n\nSBP2\n\n\n\n\n\nSBP3\n\n\n\n\n\n\nQ3: Do these match the TPM values from Kallisto?\nHint: look at the abundance.tsv files for each of your samples.\nQ4: Do you think PCHAS_1402500 is differentially expressed between the MT and SBP samples?",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#running-commands-on-multiple-samples",
    "href": "course_modules/Module6/module6_exercises.html#running-commands-on-multiple-samples",
    "title": "Exercises",
    "section": "Running commands on multiple samples",
    "text": "Running commands on multiple samples\nNow, fair warning, you’re going to wish we’d told you this earlier on. However, then you wouldn’t have had the fun of running and updating each of the previous commands, growling at typos and generally wishing that you’d gone for that cup of coffee before starting this tutorial.\nHere we go….we can use a loop to run the same commands for multiple samples.\nThere’s a great introduction to bash scripting and loops as part of our Unix module. But let’s take a look at how we could have generated genome alignments for all of our samples using a single loop.\nWhenever you write a loop, it’s always a good idea to build it up slowly to check that it’s doing what you think.\nfor r in data/*.fastq.gz\n\ndo\n\n  echo $r\n\ndone\nThis loop looks for all (*) files which end with “.fastq.gz”. The for loop then executes a sequence of commands for each file name that it finds. In the first iteration its “data/MT1_1.fastq.gz”, then “data/MT1_2.fastq.gz” and so on… In each iteration, we assigned each filename that it found to a variable called “r”. for r in *.fastq.gz\nThen, to check we got what we expected, we printed what the variable “r” represented back to the terminal. Because we want to use the variable (“r”) we created we need to use dollar ($) symbol.\necho $r\nNow, if we left things as they are, we would be running the commands twice for each sample. This is because we have two FASTQ files for each sample i.e. ”_1.fastq.gz” and ”_2.fastq.gz“. Let’s change our loop so that we only get the”_1.fastq.gz” files.\nfor r1 in data/*_1.fastq.gz\n\ndo\n\n  echo $r1\n\ndone\nGreat! Now, the only problem here is that we’re going to want to use both the ”_1.fastq.gz” and the ”_2.fastq.gz” files in our mapping. We can get around this by removing the “data/” directory and ”_1.fastq.gz” suffix from the filename to give us our sample name.\nsample=$(basename $r1) sample=${sample/_1.fastq.gz/}\nThis will get the base filename (e.g. “MT1_1.fastq.gz”) and replace the ”_1.fastq.gz” at the end of the filename we stored as “r1” with nothing.\nWe’ve added a little descriptive message so that when we run our loop we know which iteration it’s on and what it’s doing. Let’s try adding our HISAT2 mapping command.\nNote: we assume that the HISAT2 index has already been generated as that’s a command you’ll only need to run once.\nfor r1 in data/*_1.fastq.gz\n\ndo\n\n  sample=$(basename $r1)\n  \n  sample=${sample/_1.fastq.gz/}\n  \n  echo \"Processing sample: \"$sample\n  \n  echo \"Mapping sample: \"$sample\n  \n  hisat2 --max-intronlen 10000 -x data/PccAS_v3_hisat2.idx -1 \"data/${sample}_1.fastq.gz\" -2 \"data/${sample}_2.fastq.gz\" -S \"data/${sample}.sam\"\n\ndone\nNotice that because we’re using a variable as part of the filename, we need to write the filename in double quotes.\ndata/${sample}_1.fastq.gz\nNow let’s add in our samtools commands.\nfor r1 in data/*_1.fastq.gz\n\ndo\n\n  sample=$(basename $r1)\n  \n  sample=${sample/_1.fastq.gz/}\n  \n  echo \"Processing sample: \"$sample\n  \n  echo \"Mapping sample: \"$sample\n  \n  hisat2 --max-intronlen 10000 -x data/PccAS_v3_hisat2.idx -1 \"data/${sample}_1.fastq.gz\" -2 \"data/${sample}_2.fastq.gz\" -S \"data/${sample}.sam\"\n  \n  echo \"Converting SAM to BAM: \"$sample\n  \n  samtools view -b -o \"data/${sample}.bam\" \"data/${sample}.sam\"\n  \n  echo \"Sorting BAM: \"$sample\n  \n  samtools sort -o \"data/${sample}_sorted.bam\" \"data/${sample}.bam\"\n  \n  echo \"Indexing BAM: \"$sample\n  \n  samtools index \"data/${sample}_sorted.bam\"\n\ndone\nFinally, we don’t really want to keep intermediate SAM and unsorted BAM files if we don’t have to.\nThey just take up precious space. So, let’s make our samtools command a one-liner, passing the stdout from one command to another.\nfor r1 in data/*_1.fastq.gz\n\ndo\n\n  sample=$(basename $r1)\n\n  sample=${sample/_1.fastq.gz/}\n\n  echo \"Processing sample: \"$sample\n\n  hisat2 --max-intronlen 10000 -x data/PccAS_v3_hisat2.idx -1 \"data/${sample}_1.fastq.gz\" -2 \"data/${sample}_2.fastq.gz\" | samtools view -b - | samtools sort -o \"data/${sample}_sorted.bam\" - && samtools index \"data/${sample}_sorted.bam\"\n\ndone\nYou could also have used this approach for transcript quantification with Kallisto, assuming you had already generated the Kallisto index.\nfor r1 in data/*_1.fastq.gz\n\ndo\n\n  sample=$(basename $r1)\n\n  sample=${sample/_1.fastq.gz/}\n\n  echo \"Quantifying transcripts for sample: \"$sample\n\n  kallisto quant -i data/PccAS_v3_kallisto -o \"data/${sample}\" -b 100 \\\n\n  \"data/${sample}_1.fastq.gz\" \"data/${sample}_2.fastq.gz\"\n\ndone\n\nTaking a closer look at the SBP genome mapping bash script\nIn the genome mapping section of this tutorial, we mentioned that the sorted genome alignments had been provided for the three SBP samples and that to generate them, we had run a bash script.\nTo take a look at the script you can run:\nless data/map_SBP_samples.sh\nThe script contains commands to run the mapping, converting, sorting and indexing for all of the SBP samples. There’s a great introduction to bash scripting and loops in your Unix module.\nFirst, the bash script looks for all files in the data directory which start with “SBP” and end with ”_1.fastq.gz”. This is so that we get one filename per sample.\ndata/SBP*_1.fastq.gz\nTo run the commands for each of our SBP samples: SBP1, SBP2 and SBP3, the script uses a for loop. Often, scripts like these can take a while to run and it can be difficult to track what’s going on if there is limited or indistinguisable output. Here, we are printing the file path that gets returned by our search.\nfor r1 in data/SBP*_1.fastq.gz\n\ndo\n\n  echo $r1\n\ndone\nThis will print out:\nSBP1_1.fastq.gz\nSBP2_1.fastq.gz\nSBP3_1.fastq.gz\nNext, the script removes parts of the filename to get the name of the sample it belongs to. It does this because both FASTQ files (r1 and r2) are required to align each sample. There are many different ways to do this. This is one example:\nfor r1 in data/SBP*_1.fastq.gz`\n\ndo\n\n  echo $r1\n\n  sample=\\$(basename \\$r1)\n\n  sample=\\${sample/\\_1.fastq.gz/}\n\n  echo \"Processing sample: \"\\$sample\n\ndone\nWhich will print out:\nProcessing sample: SBP1\nProcessing sample: SBP2\nProcessing sample: SBP3\nFinally, the script runs the single command we were using above for the sample:\nhisat2 --max-intronlen 10000 -x data/PccAS_v3_hisat2.idx -1 \"data/${sample}_1.fastq.gz\" -2 \"data/${sample}_2.fastq.gz\" | samtools view -b - | samtools sort -o \"data/${sample}_sorted.bam\" - && samtools index \"data/${sample}_sorted.bam\"\nNote, when it extracted the sample name in the commands above, it stored it as a variable $sample.\nIt can then use the $sample variable to create a dynamic command which will run for any of the samples.",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_solutions.html",
    "href": "course_modules/Module6/module6_solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "To run the commands alongside the answers you must first have run all of the tutorial commands for that section and generated the output files they reference.\n\nIntroducing the tutorial dataset\nQ1: Why is there more than one FASTQ file per sample?\nThere are 2 FASTQ files for each sample e.g. MT1_1.fastq and MT1_2.fastq. This is because this was paired-end sequence data.\nls data/MT1*.fastq.gz\nls data/MT1*.fastq.gz | wc -l\nWith Illumina paired-end sequencing, you will have a fragment (hopefully longer than your reads) which is sequenced at both ends. This means that there will be a “left” read (sometimes known as r1) and its corresonding “right” read (sometimes known as r2). These are indicated by the /1 and/2 at the end of the read name in the **_1.fastq and _2**.fastq files respectively.\nSo, the left read header _@HS18_08296:8:1101:1352:48181#4_ in MT1_1.fastq has the /1 suffix:\n@HS18_08296:8:1101:1352:48181#4/1\nAnd, the corresponding right read header in MT1_2.fastq has the /2 suffix:\n@HS18_08296:8:1101:1352:48181#4/2\nQ2: How many reads are there for MT1?\nThere are 2.5 million reads for MT1.\nIdeally, you should count the reads in both files. This is because sometimes we have singletons (reads without a mate) after preprocessing steps such as trimming.\nOur reads look like:\n@HS18_08296:8:1101:1352:48181#4/2\nATCCGCCNANTTTNNNNATATAATTANNNGNAANNAANNNNAATNACANNNATTNNNNTAGNANNNGNNAGTNNACAAGGNTNNNNNNNAAAGNN\n+\n9AABE8A!D!DFA!!!!EE@CFCD@B!!!D!F6!!EE!!!!EE4!F5E!!!BEA!!!!B@6!A!!!D!!FD’!!C+D@@*!B!!!!!!!B&gt;DE!\nWith the FASTQ format there are four lines per read. So, as long as our files are not truncated we can count the number of lines and divide them by four.\nzless data/MT1_1.fastq.gz | wc -l\nSo, we can then divide this by four to give us 1.25 million. We will get the same for our r2 reads:\nzless data/MT1_2.fastq.gz | wc -l\nSo, we have 1.25 million read pairs or 2.5 million (1.25 x 2) reads for our MT1 sample.\nYou may also have thought ”aha, I can use the **@** which is at the start of the header”…\nzgrep -c '^@' data/MT1_1.fastq.gz\nzgrep -c '^@' data/MT1_2.fastq.gz\nBut, wait, there are 1,343,714 left reads and 1,250,000 right reads…can that be right…no.\nTake a closer look at the quality scores on the fourth line…they also contain **@**. This is because the quality scores are Phred+33 encoded. For more information on quality score encoding see our Data Formats and QC tutorial or go here.\nInstead, we can use the earlier information about the left and right read suffixes: /1 and /2. This can be with two commands:\nzgrep -c '/1$' data/MT1_1.fastq.gz\nzgrep -c '/2$' data/MT1_2.fastq.gz\nOr, with only one command:\nzgrep -c '/[12]$' data/MT1*.fastq.gz\nNote: Don’t forget to use the -c option for grep to count the occurences and $ to make sure you’re only looking for \\1 or \\2 at the end of the line.\n\n\nMapping RNA-Seq reads to the genome using HISAT2\n\n\nMap, convert (SAM to BAM), sort and index using the reads from the MT2 sample.\nYou can do this with several, individual steps:\nhisat2 --max-intronlen 10000 -x data/PccAS_v3_hisat2.idx \\-1 data/MT2_1.fastq.gz -2 data/MT2_2.fastq.gz -S data/MT2.sam\nsamtools view -b -o data/MT2.bam data/MT2.sam\nsamtools sort -o data/MT2_sorted.bam data/MT2.bam\nsamtools index data/MT2_sorted.bam\nOr, you can do it in one step:\nhisat2 --max-intronlen 10000 -x data/PccAS_v3_hisat2.idx -1 data/MT2_1.fastq.gz -2 data/MT2_2.fastq.gz | samtools view -b - | samtools sort -o data/MT2_sorted.bam - && samtools index data/MT2_sorted.bam\nOr, you could write a for loop to run the command above for all five of your samples:\nfor r1 in data/\\*\\_1.fastq.gz\n\ndo\n  sample=\\$(basename \\$r1)\n\n  sample=\\${sample/\\_1.fastq.gz/}\n\n  echo \"Processing sample: \"\\$sample\n\n  hisat2 --max-intronlen 10000 -x data/PccAS_v3_hisat2.idx -1 \"data/\\${sample}\\_1.fastq.gz\" -2 \"data/\\${sample}\\_2.fastq.gz\" | samtools view -b - | samtools sort -o \"data/\\${sample}\\_sorted.bam\" - && samtools index \"data/\\${sample}\\_sorted.bam\"\ndone\nFor more information on how this loop works, have a look at our Unix tutorial and Running commands on multiple samples.\nQ1: How many index files were generated when you ran hisat2-build?\nThere are 8 HISAT2 index files for our reference genome.\nls data/*.ht2\nls data/*.ht2 | wc -l\nQ2: What was the overall alignment rate for each of the MT samples (MT1 and MT2) to the reference genome?\nThe overall alignment rate for MT1 was 94.16% and % for MT2.\n\n1250000 reads; of these:\n\n1250000 (100.00%) were paired; of these:\n\n105654 (8.45%) aligned concordantly 0 times\n\n329304 (26.34%) aligned concordantly exactly 1 time\n\n815042 (65.20%) aligned concordantly \\&gt;1 times\n\n------------------------------------------------------------------------\n\n105654 pairs aligned concordantly 0 times; of these:\n\n1797 (1.70%) aligned discordantly 1 time\n\n------------------------------------------------------------------------\n\n103857 pairs aligned 0 times concordantly or discordantly; of these:\n\n207714 mates make up the pairs; of these:\n\n146078 (70.33%) aligned 0 times\n\n19877 (9.57%) aligned exactly 1 time\n\n41759 (20.10%) aligned \\&gt;1 times\n\n94.16% overall alignment rate\n\n1250000 reads; of these:\n\n1250000 (100.00%) were paired; of these:\n\n139440 (11.16%) aligned concordantly 0 times\n\n483493 (38.68%) aligned concordantly exactly 1 time\n\n627067 (50.17%) aligned concordantly \\&gt;1 times\n\n------------------------------------------------------------------------\n\n139440 pairs aligned concordantly 0 times; of these:\n\n4967 (3.56%) aligned discordantly 1 time\n\n------------------------------------------------------------------------\n\n134473 pairs aligned 0 times concordantly or discordantly; of these:\n\n268946 mates make up the pairs; of these:\n\n207579 (77.18%) aligned 0 times\n\n28834 (10.72%) aligned exactly 1 time\n\n32533 (12.10%) aligned \\&gt;1 times\n\n91.70% overall alignment rate\nNote: If a read pair is concordantly aligned it means both reads in the pair align with the same chromosome/scaffold/contig, the reads are aligned in a proper orientation (typically —-&gt; &lt;—-) and that the reads have an appropriate insert size.\nQ3: How many MT1 and MT2 reads were not aligned to the reference genome?\n146,250 reads (5.85%) and 207,729 reads (8.31%) did not align to the reference genome for MT1 and MT2 respectively.\nHere is a brief summary of what the HISAT2 summary tells us for our MT2 sample and how we can tell which of the summary lines gives us this information:\n• We have 1,250,000 read pairs or 2,500,000 reads (2 x 1,250,000 pairs)\n1250000 reads; of these:\n• All of our reads (100%) are paired - i.e. no reads without their mate\n1250000 (100.00%) were paired; of these:\n• 1,144,346 pairs (95.24%) align concordantly one (26.34%) or more (65.20%) times\n329304 (26.34%) aligned concordantly exactly 1 time\n815042 (65.20%) aligned concordantly &gt;1 times\n• 105,654 pairs (8.45%) or 211,308 reads (2 x 105,654) did not align concordantly anywhere\nin the genome\n105654 (8.45%) aligned concordantly 0 times\n• Of those 105,654 pairs, 1,797 pairs align discordantly (1.70% of the 105,654 pairs)\n105654 pairs aligned concordantly 0 times; of these:\n1797 (1.70%) aligned discordantly 1 time\n• This leave us with 103,857 pairs (105,654 - 1,797) where both reads in the pair do not align\nto the genome (concordantly or discordantly)\n103857 pairs aligned 0 times concordantly or discordantly; of these:\n• Of those 207,714 reads (2 x 103,857) we have 61,636 reads (29.67%) which align to the\ngenome without their mate\n207714 mates make up the pairs; of these:\n...\n19877 (9.57%) aligned exactly 1 time\n41759 (20.10%) aligned &gt;1 times\n• Leaving us with a 94.16% overall alignment rate\n94.16% overall alignment rate\n• That means 146,078 (5.84%) of the 2,500,000 reads (or 70.33% of the unaligned pairs) do not align anywhere in the genome\n146078 (70.33%) aligned 0 times\n\n\nVisualising transcriptomes with IGV\nQ1: How many CDS features are there in “PCHAS_1402500”?\nThere are 8 CDS features in PCHAS_1402500. You can get this in several ways:\nCount the number of exons/CDS features in the gene annotation.\nCount the number of CDS features in the GFF file.\nFirst, get all of the CDS features for PCHAS_1402500.\ngrep -E \"CDS.*PCHAS_1402500\" data/PccAS_v3.gff3\nThen count the number of the CDS features for PCHAS_1402500.\ngrep -cE \"CDS.*PCHAS_1402500\" data/PccAS_v3.gff3\nQ2: Does the RNA-seq mapping agree with the gene model in blue?\nYes. The peaks of the coverage tracks correspond to the annotated exon/CDS features.\nQ3: Do you think this gene is differentially expressed and is looking at the coverage plots alone a reliable way to assess differential expression?\nPossibly. But, you can’t tell differential expression by the counts alone as there may be differences in the sequencing depths of the samples.\n\n\nTranscript quantification with Kallisto\nUse kallisto quant four more times, for the MT2 sample and the three SBP samples.\nYou can run the individual kallisto quant commands as you did for MT1 for each of the remaining samples:\nkallisto quant -i data/PccAS_v3_kallisto -o data/MT2 -b 100 data/MT2_1.fastq.gz data/MT2_2.fastq.gz\nkallisto quant -i data/PccAS_v3_kallisto -o data/SBP1 -b 100 data/SBP1_1.fastq.gz data/SBP1_2.fastq.gz\nkallisto quant -i data/PccAS_v3_kallisto -o data/SBP2 -b 100 data/SBP2_1.fastq.gz data/SBP2_2.fastq.gz\nkallisto quant -i data/PccAS_v3_kallisto -o data/SBP3 -b 100 data/SBP3_1.fastq.gz data/SBP3_2.fastq.gz\nOr, you can write a for loop which will run kallisto quant on all of the samples:\nfor r1 in data/\\*\\_1.fastq.gz\n\ndo\n\n  echo \\$r1\n\n  sample=\\$(basename \\$r1)\n\n  sample=\\${sample/\\_1.fastq.gz/}\n\n  echo \"Quantifying transcripts for sample: \"\\$sample\n\n  kallisto quant -i data/PccAS_v3_kallisto -o \"data/\\$sample\" -b 100 \\\\\n\n  \"data/\\${sample}\\_1.fastq.gz\" \"data/\\${sample}\\_2.fastq.gz\"\n\ndone\nFor more information on how this loop works, have a look at our Unix tutorial and Running commands on multiple samples.\nQ1: What k-mer length was used to build the Kallisto index?\nA k-mer length of 31 was used.\nLook at the output from kallisto index:\n[build] k-mer length: 31\nOr, look for the -k or –kmer-size option in the kallisto index usage:\nkallisto index\nQ2: How many transcript sequences are there in PccAS_v3_transcripts.fa?\nThere are 5177 transcript sequences.\nLook at the output from kallisto quant:\n[index] number of targets: 5,177\nOr, look for n_targets in one of the run_info.json files:\ncat data/MT1/run_info.json\nOr, you can run a grep on the transcript FASTA file and count the number of header lines:\ngrep -c \"&gt;\" data/PccAS_v3_transcripts.fa\nQ3: What is the transcripts per million (TPM) value for PCHAS_1402500 in each of the samples?\n\n\n\nSample\nTranscripts Per Million (TPM)\n\n\n\n\nMT1\n2342.23\n\n\nMT2\n1354.42\n\n\nSBP1\n2295.24\n\n\nSBP2\n3274.98\n\n\nSBP3\n2536.17\n\n\n\nYou can look at each of the individual abundance files:\ngrep \"^PCHAS_1402500\" data/MT1/abundance.tsv\ngrep \"^PCHAS_1402500\" data/MT2/abundance.tsv\ngrep \"^PCHAS_1402500\" data/SBP1/abundance.tsv\ngrep \"^PCHAS_1402500\" data/SBP2/abundance.tsv\ngrep \"^PCHAS_1402500\" data/SBP3/abundance.tsv\nOr you can use a recursive grep:\ngrep -r \"^PCHAS_1402500\" data/*.tsv\nOr you can use a loop:\nfor r1 in data/\\*\\_1.fastq.gz\n\ndo\n\n  sample=\\$(basename \\$r1)\n\n  sample=\\${sample/\\_1.fastq.gz/}\n\n  echo \\$sample\n\n  grep PCHAS_1402500 \"data/\\${sample}/abundance.tsv\"\n\ndone\nFor more information on how this loop works, have a look at our Unix tutorial and Running commands on multiple samples.\nQ4: Do you think PCHAS_1402500 is differentially expressed?\nProbably not. We would need to run statistical tests to really be sure though.\n\n\nIdentifying differentially expressed genes with Sleuth\nQ1: Is our gene from earlier, PCHAS_1402500, significantly differentially expressed?\nNo.\nLook at the transcript table:\nAnd the transcript view.\nAlthough this gene looked like it was differentially expressed from the plots in IGV, our test did not show it to be so (q-value &gt; 0.05). This might be because some samples tended to have more reads, so based on raw read counts, genes generally look up-regulated in the SBP samples.\nAlternatively, the reliability of only two biological replicates and the strength of the difference between the conditions was not sufficient to be statistically convincing. In the second case, increasing the number of biological replicates would give us more confidence about whether there really was a difference.\nIn this case, it was the lower number of reads mapping to MT samples that mislead us in the IGV view. Luckily, careful normalisation and appropriate use of statistics saved the day!\ngrep PCHAS_1402500 data/kallisto.results | cut -f1,4\n\n\nInterpreting the results\nQ1: How many genes are more highly expressed in the SBP samples?\n127. We can use awk to filter our Kallisto/sleuth results and wc -l to count the number of lines\nreturned.\nawk -F\"\\t\" '$4 &lt; 0.01 && $5 &gt; 0' data/kallisto.results | wc -l\nQ2: How many genes are more highly expressed in the MT samples?\n169. We can use awk to filter our Kallisto/sleuth results and wc -l to count the number of lines returned.\nawk -F\"\\t\" '$4 &lt; 0.01 && $5 &lt; 0' data/kallisto.results | wc -l\nQ3: Do you notice any particular genes that come up in the analysis?\nGenes from the cir family are upregulated in the MT samples.\nTo get this we must first find out which genes (or gene descriptions) are seen most often in the genes\nwhich are more highly expressed in our SBP samples.\nawk -F\"\\t\" '$4 &lt; 0.01 && $5 &gt; 0 {print $2}' data/kallisto.results | sort | uniq -c | sort -n\nThen, we summarise the genes more highly expressed in our MT samples.\nawk -F\"\\t\" '$4 &lt; 0.01 && $5 &lt; 0 {print $2}' data/kallisto.results | sort | uniq -c | sort -n\nPerhaps the CIR proteins are interesting. There are only 2 CIR proteins upregulated in the SBP samples and 25 CIR in the MT samples.\nThe cir family is a large, malaria-specific gene family which had previously been proposed to be involved in immune evasion (Lawton et al., 2012). Here, however, we see many of these genes upregulated in a form of the parasite which seems to cause the immune system to better control the parasite. This suggests that these genes interact with the immune system in a subtler way, preventing the immune system from damaging the host.\n\n\nNormalisation\nHow we got the information to help the questions\nTo answer the questions you needed the following for each sample:\n• Number of reads assigned to PCHAS_1402500\n• Length of exons in PCHAS_1402500 (bp)\n• Total number of reads mapping\n• Total RPK\nFirst, take a quick look at the first five lines of the abundance.tsv for MT1.\nhead -5 data/MT1/abundance.tsv\nThere are five columns which give us information about the transcript abundances for our MT1 sample.\n\n\n\n\n\n\n\nColumn\nDescription\n\n\ntarget_id\nUnique transcript identifier\n\n\nlength\nNumber of bases found in exons.\n\n\neff_length\nEffective length. Uses fragment length distribution to determine the effective number of positions that can be sampled on each transcript.\n\n\nest_counts\nEstimated counts. This may not always be an integer as reads which map to multiple transcripts are fractionally assigned to each of the corresponding transcripts.\n\n\ntpm\nTranscripts per million. Normalised value accounting for length and sequence depth bias.\n\n\n\nFirst, look for your gene of interest, PCHAS_1402500. Run this as a loop to grep the information for all five samples.\nfor r1 in data/\\*\\_1.fastq.gz\n\ndo\n\n  sample=\\$(basename \\$r1)\n\n  sample=\\${sample/\\_1.fastq.gz/}\n\n  echo \\$sample\n\n  grep PCHAS_1402500 \"data/\\$sample/abundance.tsv\"\n\ndone\nNow you have the length (eff_length) and counts (est_counts) for PCHAS_1402500 for each of your samples. Next, you need to get the total number of reads mapped to each of your samples. You can use a loop to do this.\nIn the loop below samtools flagstat gives you the number of mapped paired reads (reads with itself and mate mapped) and those where one read mapped but its mate didn’t (singletons). If then uses grep to get the relevant lines and awk to add the mapped paired and singleton read totals together.\nfor r1 in data/*_1.fastq.gz\ndo\n  sample=$(basename $r1)\n  sample=\\${sample/_1.fastq.gz/}\n  total=` samtools flagstat \"data/${sample}_sorted.bam\" |\n  grep 'singletons \\|with itself and mate mapped' | awk 'BEGIN{ count=0} {count+=\\$1} END{print count}'\\`\n  echo -e \"$sample\\t\\$total\"\ndone\nFinally, to calculate the TPM values, you need the total RPK for each of your samples. Again we use a loop. Notice the use of NR&gt;2 in the awk command which tells it to skip the two header lines at the start of the file. You will also notice that we divide the eff_length by 1,000 so that it’s in kilobases.\nfor r1 in data/*_1.fastq.gz\ndo\n  sample=$(basename $r1)\n\n  sample=${sample/_1.fastq.gz/}\n\n  awk -F\"\\t\" -v sample=\"$sample\" 'BEGIN{total_rpk=0;} NR\\&gt;2 { rpk=$4/($3/1000); total_rpk+=rpk } END{print sample\"\\t\"total_rpk}' \"data/${sample}/abundance.tsv\"\ndone\nQ1: Using the abundance.tsv files generated by Kallisto and the information above, calculate the RPKM for PCHAS_1402500 in each of our five samples.\n\n\n\n\n\n\n\n\n\n\nSample\nPer million scaling factor\nReads per million (RPM)\nPer kilobase scaling factor\nRPKM\n\n\n\n\nMT1\n2.353922\n1079.450\n3.697\n292\n\n\nMT2\n2.292421\n1479.484\n3.709\n399\n\n\nSBP1\n2.329424\n6269.662\n3.699\n1695\n\n\nSBP2\n2.187862\n7908.131\n3.696\n2140\n\n\nSBP3\n2.164148\n6767.421\n3.699\n1830\n\n\n\nQ2: Using the abundance.tsv files generated by Kallisto and the information above, calculate the TPM for PCHAS_1402500 in each of our five samples.\n\n\n\n\n\n\n\n\n\nSample\nPer kilobase scaling factor\nReads per kilobase (RPK)\nTPM\n\n\n\n\nMT1\n3.697\n687.30\n2342\n\n\nMT2\n3.709\n914.50\n1354\n\n\nSBP1\n3.699\n3947.87\n2295\n\n\nSBP2\n3.696\n4681.81\n3275\n\n\nSBP3\n3.699\n3959.78\n2536\n\n\n\nQ3: Do these match the TPM values from Kallisto?\nYes.\nWell, almost. They may be a couple out because we rounded up to make the calculations easier.\nQ4: Do you think PCHAS_1402500 is differentially expressed between the MT and SBP samples?\nProbably not.\nIf we were to look at only the counts and RPKM values then it appears there is an 8 fold difference between the MT and SBP samples. However, when we look at the TPM values, they are much closer and so differential expression is less likely.",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Solutions"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7_solutions.html",
    "href": "course_modules/Module7/module7_solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "To run the commands alongside the answers you must first have run all of the tutorial commands for that section and generated the output files they reference.\n\n\nLet’s go to our data directory.\ncd data\n\n\n\nQ1. How can you distinguish between the header of the SAM format and the actual alignments?\nEach header line begins with the **@** character followed by one of the two-letter header record type codes and each alignment line starts with the read name.\n\nQ2. What information does the header provide you with?\nThe header is categorised using two letter record type codes.\n\n\n\nRecord type\nDescription\n\n\n\n\n@HD\nheader line with general information about the file\n\n\n@SQ \nreference sequence dictionary and alignment order\n\n\n@PG \nPrograms used to generate the alignment file\n\n\n\nWithin each of these categories are a series of tags and values.\n\n\n\nRecord type\nTag\nDescription\n\n\n\n\n@HD\nVN\nformat version\n\n\n@HD\nSO\nalignment sorting order\n\n\n@SQ\nSN\nreference sequence name\n\n\n@SQ\nLN\nreference sequence length\n\n\n@PG\nID\nprogram record identifier\n\n\n@PG\nPN\nprogram name\n\n\n@PG\nVN\nprogram version number\n\n\n@PG\nCL\ncommand used when the program was run\n\n\n\nQ3. Which chromosome are the reads mapped to?\nThe reads are aligned to chromosome 1 (chr1). You’re looking for this line in the header.\n@SQ SN:chr1 LN:249250621\n\n\n\nQ1. Look for gene NASP in the search box. Can you see a PAX5 binding site near the NASP gene?\nWhen you have loaded your PAX5.sorted.bam and PAX5.bw into IGV, made the necessary adjustments and searched for NASP, you should see the following.\nThere is a peak representing a PAX5 binding site approximately 1kb upstream of the NASP gene, in the promoter region.\n\n\nQ2. What is the main difference between the visualisation of BAM and bigWig files?\nbigWig files display dense, continuous data as a graph whereas BAM files display the read alignment pileup which is discrete.\n\n\n\nbowtie2 -k 1 -x bowtie_index/hs19 Control.fastq.gz -S Control.sam\nsamtools view -bSo Control.bam Control.sam\nsamtools sort -T Control.temp.bam -o Control.sorted.bam Control.bam\nsamtools index Control.sorted.bam\n\n\n\nQ1. The simplest bed file contains just three columns (chromosome, start, end) and is often called BED3 format. What extra columns does BED6 contain?\nBED6 contains BED3 columns with the addition of three extra columns: name, score and strand\nQ2. In the above examples, what are the lengths of the intervals?\nThe intervals in the example were 50, 500 and 200 respectively. Remember that the start coordinates are 0-based which means that you can just subtract the start position from the end position.\n\n\n\nchromosome\nstart\nend\nlength\n\n\n\n\nchr1\n50\n100\n100 - 50 = 50\n\n\nchr1\n500\n1000\n1000 - 500 = 500\n\n\nchr2\n600\n800\n800 - 600 = 200\n\n\n\nQ3. Can you output a BED6 format with a transcript called “loc1”, transcribed on the forward strand and having three exons of length 100 starting at positions 1000, 2000 and 3000?\n\n\n\nchromosome\nstart\nend\nname\nscore\nstrand\n\n\n\n\nchr1\n999\n1099\nloc1\n.\n+\n\n\nchr1\n1999\n2099\nloc1\n.\n+\n\n\nchr1\n2999\n3099\nloc1\n.\n+\n\n\n\nQ4. What additional information is given in the narrowPeak file, beside the location of the peaks?\nnarrowPeak files are in BED6+4 format which contains the peak locations together with overall enrichment, peak summit, p-value and q-value.\n\n\n\ncolumn name\ndescription\n\n\n\n\nsignalValue\noverall/average enrichment for the region\n\n\npValue\n-log10(pvalue) for the peak summit\n\n\nqValue\n-log10(qvalue) for the peak summit\n\n\npeak\nsummit position relative to peak start\n\n\n\nQ5. Does the first peak that was called look convincing to you?\nLook at the peak in IGV by entering the coordinates in the search box (i.e. chr1:710543-710713).\nIf you hover over the peak annotation, you can see some of the information produced by MACS2 for this peak.\n\n\nThere are 29 reads piled up under this peak which has a q-value of 10. Remember that this is a negative log and so is equivalent to 1e-10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchromosome\nstart\nend\nname\nscore\nstrand\nsignalValue\npValue\nqValue\npeak\n\n\n\n\nchr1\n710543\n710713\nPAX5_peak_1\n137\n.\n7.50382\n13.75727\n10.47078\n97\n\n\n\nQ6. In the small example table above, why have the coordinates changed from the BED description?\nFrom the BED file:\n\n\n\nchromosome\nstart\nend\n\n\n\n\nchr1\n50\n100\n\n\n\nFrom the GTF file:\n\n\n\nchromosome\nstart\nstop\n\n\n\n\nchr1\n51\n100\n\n\n\nNotice that the start values differ by 1. This is because the start coordinates in BED format are 0-based (i.e. first base is at position 0) while the start coordinates in GTF format are 1-based (i.e. the first base is at position 1).\n\n\n\nQ1. Looking at the output of the bedtools genomecov we ran, what percentage of chromosome1 do the peaks of PAX5 cover?\nThe peaks of PAX5 cover 0.28% of chromosome 1 (chr1).\nThe output of bedtools genomecov contains 5 columns which represent:\n1. chromosome (or entire genome)\n2. depth of coverage from features in input file\n3. number of bases on chromosome (or genome) with depth equal to column 2\n4. size of chromosome (or entire genome) in base pairs\n5. fraction of bases on chromosome (or entire genome) with depth equal to column 2\nThe output for chromosome 1 was:\nchr1 0 248561847 249250621 0.997237\nchr1 1 602947 249250621 0.00241904\nchr1 2 68077 249250621 0.000273127\nchr1 3 15400 249250621 6.17852e-05\nchr1 4 963 249250621 3.86358e-06\nchr1 7 1387 249250621 5.56468e-06\nThe important value here is in column 5 - fraction of bases with depth equal to column 2.\nThere are two ways to get the percentage. The first is to add together the fraction of bases which have a depth &gt; 1 on chr1 and convert it into a percentage.\n0.00241904 + 0.000273127 + 6.17852e-05 + 3.86358e-06 + 5.56468e-06 = 0.00276338\n= 0.28%\nAlternatively, you could subtract the fraction of bases with 0 coverage from 1.\n1 - 0.997237 = 0.002763 = 0.28%\nQ2. Looking at the output from bedtools intersect, what proportion of PAX5 peaks overlap genes?\n72% of the PAX5 peaks overlap genes (a proportion of 0.72).\nYou need to divide the number of peaks overlapping genes (2722) by the total number of peaks (3799).\nQ3. Looking at PAX5_closestTSS.txt, which gene was found to be closest to MACS peak 2?\nThe closest gene to PAX5_peak_2 is a lincRNA called RP11-206L10.\n\n\n\nQ1. Which motif was found to be the most similar to your motif?\nUsing the output from Tomtom, we can see that the most similar motif is PAX5.",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Solutions"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7_solutions.html#answers",
    "href": "course_modules/Module7/module7_solutions.html#answers",
    "title": "Solutions",
    "section": "",
    "text": "To run the commands alongside the answers you must first have run all of the tutorial commands for that section and generated the output files they reference.\n\n\nLet’s go to our data directory.\ncd data\n\n\n\nQ1. How can you distinguish between the header of the SAM format and the actual alignments?\nEach header line begins with the **@** character followed by one of the two-letter header record type codes and each alignment line starts with the read name.\n\nQ2. What information does the header provide you with?\nThe header is categorised using two letter record type codes.\n\n\n\nRecord type\nDescription\n\n\n\n\n@HD\nheader line with general information about the file\n\n\n@SQ \nreference sequence dictionary and alignment order\n\n\n@PG \nPrograms used to generate the alignment file\n\n\n\nWithin each of these categories are a series of tags and values.\n\n\n\nRecord type\nTag\nDescription\n\n\n\n\n@HD\nVN\nformat version\n\n\n@HD\nSO\nalignment sorting order\n\n\n@SQ\nSN\nreference sequence name\n\n\n@SQ\nLN\nreference sequence length\n\n\n@PG\nID\nprogram record identifier\n\n\n@PG\nPN\nprogram name\n\n\n@PG\nVN\nprogram version number\n\n\n@PG\nCL\ncommand used when the program was run\n\n\n\nQ3. Which chromosome are the reads mapped to?\nThe reads are aligned to chromosome 1 (chr1). You’re looking for this line in the header.\n@SQ SN:chr1 LN:249250621\n\n\n\nQ1. Look for gene NASP in the search box. Can you see a PAX5 binding site near the NASP gene?\nWhen you have loaded your PAX5.sorted.bam and PAX5.bw into IGV, made the necessary adjustments and searched for NASP, you should see the following.\nThere is a peak representing a PAX5 binding site approximately 1kb upstream of the NASP gene, in the promoter region.\n\n\nQ2. What is the main difference between the visualisation of BAM and bigWig files?\nbigWig files display dense, continuous data as a graph whereas BAM files display the read alignment pileup which is discrete.\n\n\n\nbowtie2 -k 1 -x bowtie_index/hs19 Control.fastq.gz -S Control.sam\nsamtools view -bSo Control.bam Control.sam\nsamtools sort -T Control.temp.bam -o Control.sorted.bam Control.bam\nsamtools index Control.sorted.bam\n\n\n\nQ1. The simplest bed file contains just three columns (chromosome, start, end) and is often called BED3 format. What extra columns does BED6 contain?\nBED6 contains BED3 columns with the addition of three extra columns: name, score and strand\nQ2. In the above examples, what are the lengths of the intervals?\nThe intervals in the example were 50, 500 and 200 respectively. Remember that the start coordinates are 0-based which means that you can just subtract the start position from the end position.\n\n\n\nchromosome\nstart\nend\nlength\n\n\n\n\nchr1\n50\n100\n100 - 50 = 50\n\n\nchr1\n500\n1000\n1000 - 500 = 500\n\n\nchr2\n600\n800\n800 - 600 = 200\n\n\n\nQ3. Can you output a BED6 format with a transcript called “loc1”, transcribed on the forward strand and having three exons of length 100 starting at positions 1000, 2000 and 3000?\n\n\n\nchromosome\nstart\nend\nname\nscore\nstrand\n\n\n\n\nchr1\n999\n1099\nloc1\n.\n+\n\n\nchr1\n1999\n2099\nloc1\n.\n+\n\n\nchr1\n2999\n3099\nloc1\n.\n+\n\n\n\nQ4. What additional information is given in the narrowPeak file, beside the location of the peaks?\nnarrowPeak files are in BED6+4 format which contains the peak locations together with overall enrichment, peak summit, p-value and q-value.\n\n\n\ncolumn name\ndescription\n\n\n\n\nsignalValue\noverall/average enrichment for the region\n\n\npValue\n-log10(pvalue) for the peak summit\n\n\nqValue\n-log10(qvalue) for the peak summit\n\n\npeak\nsummit position relative to peak start\n\n\n\nQ5. Does the first peak that was called look convincing to you?\nLook at the peak in IGV by entering the coordinates in the search box (i.e. chr1:710543-710713).\nIf you hover over the peak annotation, you can see some of the information produced by MACS2 for this peak.\n\n\nThere are 29 reads piled up under this peak which has a q-value of 10. Remember that this is a negative log and so is equivalent to 1e-10.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nchromosome\nstart\nend\nname\nscore\nstrand\nsignalValue\npValue\nqValue\npeak\n\n\n\n\nchr1\n710543\n710713\nPAX5_peak_1\n137\n.\n7.50382\n13.75727\n10.47078\n97\n\n\n\nQ6. In the small example table above, why have the coordinates changed from the BED description?\nFrom the BED file:\n\n\n\nchromosome\nstart\nend\n\n\n\n\nchr1\n50\n100\n\n\n\nFrom the GTF file:\n\n\n\nchromosome\nstart\nstop\n\n\n\n\nchr1\n51\n100\n\n\n\nNotice that the start values differ by 1. This is because the start coordinates in BED format are 0-based (i.e. first base is at position 0) while the start coordinates in GTF format are 1-based (i.e. the first base is at position 1).\n\n\n\nQ1. Looking at the output of the bedtools genomecov we ran, what percentage of chromosome1 do the peaks of PAX5 cover?\nThe peaks of PAX5 cover 0.28% of chromosome 1 (chr1).\nThe output of bedtools genomecov contains 5 columns which represent:\n1. chromosome (or entire genome)\n2. depth of coverage from features in input file\n3. number of bases on chromosome (or genome) with depth equal to column 2\n4. size of chromosome (or entire genome) in base pairs\n5. fraction of bases on chromosome (or entire genome) with depth equal to column 2\nThe output for chromosome 1 was:\nchr1 0 248561847 249250621 0.997237\nchr1 1 602947 249250621 0.00241904\nchr1 2 68077 249250621 0.000273127\nchr1 3 15400 249250621 6.17852e-05\nchr1 4 963 249250621 3.86358e-06\nchr1 7 1387 249250621 5.56468e-06\nThe important value here is in column 5 - fraction of bases with depth equal to column 2.\nThere are two ways to get the percentage. The first is to add together the fraction of bases which have a depth &gt; 1 on chr1 and convert it into a percentage.\n0.00241904 + 0.000273127 + 6.17852e-05 + 3.86358e-06 + 5.56468e-06 = 0.00276338\n= 0.28%\nAlternatively, you could subtract the fraction of bases with 0 coverage from 1.\n1 - 0.997237 = 0.002763 = 0.28%\nQ2. Looking at the output from bedtools intersect, what proportion of PAX5 peaks overlap genes?\n72% of the PAX5 peaks overlap genes (a proportion of 0.72).\nYou need to divide the number of peaks overlapping genes (2722) by the total number of peaks (3799).\nQ3. Looking at PAX5_closestTSS.txt, which gene was found to be closest to MACS peak 2?\nThe closest gene to PAX5_peak_2 is a lincRNA called RP11-206L10.\n\n\n\nQ1. Which motif was found to be the most similar to your motif?\nUsing the output from Tomtom, we can see that the most similar motif is PAX5.",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Solutions"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7_exercises.html",
    "href": "course_modules/Module7/module7_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "ChIP-Seq is the combination of chromatin immunoprecipitation (ChIP) assays with high-throughput sequencing (Seq) and can be used to identify DNA binding sites for transcription factors and other proteins. The goal of this hands-on session is to perform the basic steps of the analysis of ChIP-Seq data, as well as some downstream analysis. Throughout this practical we will try to identify potential transcription factor binding sites of PAX5 in human lymphoblastoid cells.\n\n\nBy the end of this tutorial you can expect to be able to:\n• generate an unspliced alignment by aligning raw sequencing data to the human genome using Bowtie2\n• manipulate the SAM output in order to visualise the alignment in IGV\n• based on the aligned reads, find immuno-enriched areas using the peak caller MACS2\n• perform functional annotation and motif analysis on the predicted binding regions\n\n\n\nThis tutorial comprises the following sections:\n1. Introducing the tutorial dataset\n2. Aligning the PAX5 sample to the genome\n3. Manipulating SAM output\n4. Visualising alignments in IGV\n5. Aligning the control sample to the genome\n6. Identifying enriched areas using MACS\n7. File formats\n8. Inspecting genomic regions using bedtools\n9. Motif analysis\n\n\n\nThis tutorial was converted into a Jupyter notebook by Victoria Offord based on materials developed by Angela Goncalves, Myrto Kostadima, Steven Wilder and Maria Xenophontos.\n1.4 Running the commands from this tutorial\nYou can run the commands in this tutorial either directly from the Jupyter notebook (if using Jupyter), or by typing the commands in your terminal window.\npwd\nls -l\n\n\n\nYou can also follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, select the cell below with the mouse and then either press control and enter or choose Cell -&gt; Run in the menu at the top of the page.\necho cd $PWD\nOpen a new terminal on your computer and type the command that was output by the previous cell followed by the enter key. The command will look similar to this:\ncd /home/manager/course_data/chip_seq\nNow you can follow the instructions in the tutorial from here.\n\n\n\nThis tutorial assumes that you have the following software or packages and their dependencies installed on your computer. The software or packages used in this tutorial may be updated from time to time so, we have also given you the version which was used when writing the tutorial.\n\n\n\nPackage\nLink for download/installation instructions\nVersion\n\n\n\n\nbedtools\nhttp://bedtools.readthedocs.io/en/latest/content/installation.html\n2.25.0\n\n\nBowtie2\nhttp://bowtie-bio.sourceforge.net/bowtie2\n2.2.6\n\n\nIGV\nhttp://software.broadinstitute.org/software/igv\n2.3.8\n\n\nMACS2\nhttps://github.com/taoliu/MACS\n2.1.0.20150420\n\n\nmeme\nhttp://meme-suite.org/tools/meme\n4.10.0\n\n\nsamtools\nhttps://github.com/samtools/samtools\n1.6\n\n\ntomtom\nhttp://web.mit.edu/meme_v4.11.4/share/doc/tomtom.html\n4.10.0\n\n\nUCSC tools\nhttp://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64\nNA\n\n\n\n\n\n\nTo get started with the tutorial, head to the first section: introducing the tutorial dataset. The answers to all questions in the tutorial can be found in answers.ipynb.\n\n\n\nThe data we will use for this practical comes from the ENCODE (Encyclopedia of DNA Elements) Consortium, a big international collaboration aimed at building a comprehensive catalogue of functional elements in the human genome. As part of this project, many human tissues and cell lines were studied using high-throughput sequencing technologies.\nIn this tutorial, we will work on datasets from, GM12878, a lymphoblastoid cell line produced from the blood of a female donor of European ancestry. Specifically, we will look at binding data for the transcription factor PAX5. PAX5 is a known regulator of B-cell differentiation. Aberrant expression of PAX5 is linked to lymphoblastoid leukaemia. If there is time, we will also look at ChIP-seq data for Polymerase II and the histone modification H3K36me3.\nThe .fastq file that we will align is called PAX5.fastq. This file is based on PAX5 ChIP-Seq data produced by the Myers lab in the context of the ENCODE project. We will align these reads to the human genome.\nThe tutorial files can be found in the data directory. Let’s go there now!\nMove into the directory containing the tutorial data files.\ncd data\nCheck to see if the tutorial files are there.\nls *.fastq.gz\nIf the previous ls command didn’t return anything, download and uncompress the tutorial files.\nwget ftp://ftp.sanger.ac.uk/pub/project/pathogens/workshops/chipseq_data.tar.gz\ntar -xf chipseq_data.tar.gz\nmv chipseq_data/* .\nTake a look at one of the FASTQ files.\nzless PAX5.fastq.gz | head\n\n\n\nFor a quick recap of what the tutorial covers and the software you will need, head back to the introduction.\nOtherwise, let’s get started with aligning the PAX5 sample to the genome.",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7_exercises.html#introduction",
    "href": "course_modules/Module7/module7_exercises.html#introduction",
    "title": "Exercises",
    "section": "",
    "text": "ChIP-Seq is the combination of chromatin immunoprecipitation (ChIP) assays with high-throughput sequencing (Seq) and can be used to identify DNA binding sites for transcription factors and other proteins. The goal of this hands-on session is to perform the basic steps of the analysis of ChIP-Seq data, as well as some downstream analysis. Throughout this practical we will try to identify potential transcription factor binding sites of PAX5 in human lymphoblastoid cells.\n\n\nBy the end of this tutorial you can expect to be able to:\n• generate an unspliced alignment by aligning raw sequencing data to the human genome using Bowtie2\n• manipulate the SAM output in order to visualise the alignment in IGV\n• based on the aligned reads, find immuno-enriched areas using the peak caller MACS2\n• perform functional annotation and motif analysis on the predicted binding regions\n\n\n\nThis tutorial comprises the following sections:\n1. Introducing the tutorial dataset\n2. Aligning the PAX5 sample to the genome\n3. Manipulating SAM output\n4. Visualising alignments in IGV\n5. Aligning the control sample to the genome\n6. Identifying enriched areas using MACS\n7. File formats\n8. Inspecting genomic regions using bedtools\n9. Motif analysis\n\n\n\nThis tutorial was converted into a Jupyter notebook by Victoria Offord based on materials developed by Angela Goncalves, Myrto Kostadima, Steven Wilder and Maria Xenophontos.\n1.4 Running the commands from this tutorial\nYou can run the commands in this tutorial either directly from the Jupyter notebook (if using Jupyter), or by typing the commands in your terminal window.\npwd\nls -l\n\n\n\nYou can also follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, select the cell below with the mouse and then either press control and enter or choose Cell -&gt; Run in the menu at the top of the page.\necho cd $PWD\nOpen a new terminal on your computer and type the command that was output by the previous cell followed by the enter key. The command will look similar to this:\ncd /home/manager/course_data/chip_seq\nNow you can follow the instructions in the tutorial from here.\n\n\n\nThis tutorial assumes that you have the following software or packages and their dependencies installed on your computer. The software or packages used in this tutorial may be updated from time to time so, we have also given you the version which was used when writing the tutorial.\n\n\n\nPackage\nLink for download/installation instructions\nVersion\n\n\n\n\nbedtools\nhttp://bedtools.readthedocs.io/en/latest/content/installation.html\n2.25.0\n\n\nBowtie2\nhttp://bowtie-bio.sourceforge.net/bowtie2\n2.2.6\n\n\nIGV\nhttp://software.broadinstitute.org/software/igv\n2.3.8\n\n\nMACS2\nhttps://github.com/taoliu/MACS\n2.1.0.20150420\n\n\nmeme\nhttp://meme-suite.org/tools/meme\n4.10.0\n\n\nsamtools\nhttps://github.com/samtools/samtools\n1.6\n\n\ntomtom\nhttp://web.mit.edu/meme_v4.11.4/share/doc/tomtom.html\n4.10.0\n\n\nUCSC tools\nhttp://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64\nNA\n\n\n\n\n\n\nTo get started with the tutorial, head to the first section: introducing the tutorial dataset. The answers to all questions in the tutorial can be found in answers.ipynb.\n\n\n\nThe data we will use for this practical comes from the ENCODE (Encyclopedia of DNA Elements) Consortium, a big international collaboration aimed at building a comprehensive catalogue of functional elements in the human genome. As part of this project, many human tissues and cell lines were studied using high-throughput sequencing technologies.\nIn this tutorial, we will work on datasets from, GM12878, a lymphoblastoid cell line produced from the blood of a female donor of European ancestry. Specifically, we will look at binding data for the transcription factor PAX5. PAX5 is a known regulator of B-cell differentiation. Aberrant expression of PAX5 is linked to lymphoblastoid leukaemia. If there is time, we will also look at ChIP-seq data for Polymerase II and the histone modification H3K36me3.\nThe .fastq file that we will align is called PAX5.fastq. This file is based on PAX5 ChIP-Seq data produced by the Myers lab in the context of the ENCODE project. We will align these reads to the human genome.\nThe tutorial files can be found in the data directory. Let’s go there now!\nMove into the directory containing the tutorial data files.\ncd data\nCheck to see if the tutorial files are there.\nls *.fastq.gz\nIf the previous ls command didn’t return anything, download and uncompress the tutorial files.\nwget ftp://ftp.sanger.ac.uk/pub/project/pathogens/workshops/chipseq_data.tar.gz\ntar -xf chipseq_data.tar.gz\nmv chipseq_data/* .\nTake a look at one of the FASTQ files.\nzless PAX5.fastq.gz | head\n\n\n\nFor a quick recap of what the tutorial covers and the software you will need, head back to the introduction.\nOtherwise, let’s get started with aligning the PAX5 sample to the genome.",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7_exercises.html#aligning-the-pax5-sample-to-the-genome",
    "href": "course_modules/Module7/module7_exercises.html#aligning-the-pax5-sample-to-the-genome",
    "title": "Exercises",
    "section": "3 Aligning the PAX5 sample to the genome",
    "text": "3 Aligning the PAX5 sample to the genome\nThere are a number of competing tools for short read alignment, each with its own set of strengths, weaknesses, and caveats. Here we will use Bowtie2, a widely used ultrafast, memory efficient short read aligner.\nBowtie2 has a number of parameters in order to perform the alignment. To view them all type:\nbowtie2 --help\nBowtie2 uses indexed genome for the alignment in order to keep its memory footprint small. Because of time constraints we will build the index only for one chromosome of the human genome. For this we need the chromosome sequence in fasta format. This is stored in a file named HS19.fa, under the subdirectory genome.\nIf you are not in there already, change into the data directory.\ncd data\nWe will be storing our indexed genome in a folder called bowtie_index.\nCheck if the bowtie_index folder already exists.\nls bowtie_index\nIf it doesn’t exist already, create the folder bowtie_index.\nmkdir bowtie_index\nThen, index the chromosome using the command:\nbowtie2-build genome/HS19.fa.gz bowtie_index/hs19\nBe patient, building the index may take 5-10 minutes!\nThis command will output 6 files that constitute the index. These files that have the prefix hs19 and are stored in the bowtie_index directory.\nTo check the files have been successfully created type:\nls -l bowtie_index\nNow that the genome is indexed we can move on to the actual alignment. In the following command the first argument (-k) instructs Bowtie2 to report only uniquely mapped reads. The following argument (-x) specifies the basename of the index for the genome to be searched; in our case is hs19. Then there is the name of the FASTQ file and the last argument (-S) that ensures that the output is in SAM format.\nAlign the PAX5 reads using Bowtie2:\nbowtie2 -k 1 -x bowtie_index/hs19 PAX5.fastq -S PAX5.sam\nThe above command outputs the alignments in SAM format and stores them in the file PAX5.sam.\nIn general before you run Bowtie2, you have to know which FASTQ format you have. The available FASTQ formats in Bowtie2 are:\n--phred33 input quals are Phred+33 (default)\n--phred64 input quals are Phred+64\n--int-quals input quals are specified as space-delimited integers\nSee http://en.wikipedia.org/wiki/FASTQ_format to find more detailed information about the different quality encodings.\nThe PAX5.fastq file we are working on uses encoding Phred+33 (the default). Bowtie2 will take 2-3 minutes to align the file. This is fast compared to other aligners that sacrifice some speed to obtain higher sensitivity.\nLook at the file in the SAM format by typing:\nhead -n 10 PAX5.sam\nYou can find more information on the SAM format by looking at https://samtools.github.io/hts-specs/SAMv1.pdf.\n\n3.1 Questions\nQ1. How can you distinguish between the header of the SAM format and the actual alignments?\nHint: look at section 1.3 in the documentation (https://samtools.github.io/hts-specs/SAMv1.pdf).\nQ2. What information does the header provide you with?\nHint: use the documentation to work out what the header tags mean\nQ3. Which chromosome are the reads mapped to?\n\n\n3.2 What’s next?\nFor a quick recap of what the tutorial covers head back to the introduction.\nIf you want a reintroduction to the tutorial dataset, head back to introducing the tutorial dataset.\nOtherwise, let’s continue on to manipulating SAM output.",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7_exercises.html#manipulating-sam-output",
    "href": "course_modules/Module7/module7_exercises.html#manipulating-sam-output",
    "title": "Exercises",
    "section": "4 Manipulating SAM output",
    "text": "4 Manipulating SAM output\nSAM files are rather big and when dealing with a high volume of HTS data, storage space can become an issue. Using samtools we can convert SAM files to BAM files (their binary equivalent files that are not human readable) that occupy much less space.\nTo convert your SAM file to a BAM file, you have to instruct samtools that the input is in SAM format (-S), the output should be in BAM format (-b) and that you want the output to be stored in the file specified by the -o option.\nIf you are not in there already, change into the data directory.\ncd data\nConvert SAM to BAM using samtools and store the output in the file PAX5.bam:\nsamtools view -bSo PAX5.bam PAX5.sam\n\n4.1 What’s next?\nFor a quick recap of what the tutorial covers head back to the introduction.\nIf you want a reintroduction to the tutorial dataset, head back to aligning the PAX5 sample to the genome.\nOtherwise, let’s continue on to visualising alignments in IGV.",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7_exercises.html#visualising-alignments-in-igv",
    "href": "course_modules/Module7/module7_exercises.html#visualising-alignments-in-igv",
    "title": "Exercises",
    "section": "5 Visualising alignments in IGV",
    "text": "5 Visualising alignments in IGV\nIt is often instructive to look at your data in a genome browser. Here, we use IGV, a stand-alone browser, which has the advantage of being installed locally and providing fast access. Please check their website (http://www.broadinstitute.org/igv) for all the formats that IGV can display.\nWeb-based genome browsers, like Ensembl or the UCSC browser, are slower, but provide more functionality. They do not only allow for more polished and flexible visualisation, but also provide easy access to a wealth of annotations and external data sources. This makes it straightforward to relate your data with information about repeat regions, known genes, epigenetic features or areas of cross-species conservation, to name just a few. As such, they are useful tools for exploratory analysis.\nVisualisation will allow you to get a “feel” for the data, as well as detecting abnormalities and problems. Also, exploring the data in such a way may give you ideas for further analyses. For our visualization purposes we will use the BAM and bigWig formats.\nWhen uploading a BAM file into the genome browser, the browser will look for the index of the BAMvfile in the same folder where the BAM files is. The index file should have the same name as the BAMvfile and the suffix .bai. Finally, to create the index of a BAM file you need to make sure that the file is sorted according to chromosomal coordinates.\nIf you are not in there already, change into the data directory.\ncd data\nSort alignments according to chromosome position and store the result in the file with the prefix PAX5.sorted:\nsamtools sort -T PAX5.temp.bam -o PAX5.sorted.bam PAX5.bam\nIndex the sorted file.\nsamtools index PAX5.sorted.bam\nThe indexing will create a file called PAX5.sorted.bam.bai. Note that you don’t have to specify the name of the index file when running samtools index.\nAnother way to visualise the alignments is to convert the BAM file into a bigWig file. The bigWig format is for display of dense, continuous data and the data will be displayed as a graph. The resulting bigWig files are in an indexed binary format.\nThe BAM to bigWig conversion takes place in two steps. First, we convert the BAM file into a bedgraph, called PAX5.bedgraph, using the tool genomeCoverageBed from bedtools.\nTo find the structure of the command and the mandatory arguments type:\ngenomeCoverageBed\nApart from the BAM file, we also need to provide the size of the chromosomes for the organism of interest in order to generate the bedgraph file. These have to be stored in a tab-delimited file. When using the UCSC Genome Browser, Ensembl, or Galaxy, you typically indicate which species or genome build you are working with. The way you do this for bedtools is to create a “genome” file, which simply lists the names of the chromosomes (or scaffolds, etc.) and their size (in basepairs).\nTo obtain chromosome lengths for the human genome, type:\nfetchChromSizes hg19 &gt; genome/hg19.all.chrom.sizes\nWe next want to remove any chromosome length information for the patched chromosomes, which are accessioned scaffold sequences that represent assembly updates. That way we will only keep the information of the current assembly.\nRemove this information using awk:\nawk '$1 !~ /[_.]/' genome/hg19.all.chrom.sizes &gt; genome/hg19.chrom.sizes\nNow generate the bedgraph file, called PAX5.bedgraph, by typing:\ngenomeCoverageBed -bg -ibam PAX5.sorted.bam -g genome/hg19.chrom.sizes &gt; PAX5.bedgraph\nWe then need to convert the bedgraph into a binary graph, called PAX5.bw, using the tool bedGraphToBigWig from the UCSC tools.\nTo convert the bedgraph type:\nbedGraphToBigWig PAX5.bedgraph genome/hg19.chrom.sizes PAX5.bw\nNow we will load the data into the IGV browser for visualisation.\nTo launch IGV:\nigv.sh &\nOn the top left of your screen choose “Human hg19” from the drop down menu. Then in order to load the desired files go to “File –&gt; Load from File”.\nOn the pop up window navigate to the tutorial folder and select the file PAX5.sorted.bam.\nRepeat these steps in order to load PAX5.bw as well.\nSelect “chr1” from the drop down menu on the top left.\nRight click on the name of PAX5.bw and choose “Maximum” under the “Windowing Function”.\nRight click again and select “Autoscale”.\n\n5.1 Questions\nQ1. Look for gene NASP in the search box. Can you see a PAX5 binding site near the NASP gene?\nHint: use the “+” button on the top right zoom in more to see the details of the alignment\nQ2. What is the main difference between the visualisation of BAM and bigWig files?\n\n\n5.2 What’s next?\nYou can head back to manipulating SAM output or continue on to aligning the control sample to the genome.\n\n\n6 Aligning the control sample to the genome\nIn the ChIP-Seq folder you will find another .fastq file called Control.fastq.\nIf you are not in there already, change into the data directory.\ncd data\nUse the head command to look at this file:\nzless Control.fastq.gz | head\nUse the information on the FASTQ Wikipedia page (http://en.wikipedia.org/wiki/FASTQ_format) to determine the quality encoding this FASTQ file is using. Then, adapting your commands to the quality encoding where needed, follow the steps you used to align the PAX5 sample to the genome and manipulate the SAM file in order to align the control reads to the human genome.\n\n\n6.1 What’s next?\nYou can head back to visualising alignments in IGV or continue on to identifying enriched areas using MACS.",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7_exercises.html#finding-enriched-areas-using-macs",
    "href": "course_modules/Module7/module7_exercises.html#finding-enriched-areas-using-macs",
    "title": "Exercises",
    "section": "7 Finding enriched areas using MACS",
    "text": "7 Finding enriched areas using MACS\nMACS2 stands for model-based analysis of ChIP-Seq. It was designed for identifying transcription factor binding sites. MACS2 captures the influence of genome complexity to evaluate the significance of enriched ChIP regions, and improves the spatial resolution of binding sites through combining the information of both sequencing tag position and orientation. MACS2 can be easily used for ChIP-Seq data alone, or with a control sample to increase specificity.\nIf you are not in there already, change into the data directory.\ncd data\nConsult the MACS2 help file to see the options and parameters:\nmacs2 --help\nmacs2 callpeak --help\nThe input for MACS2 can be in ELAND, BED, SAM, BAM or BOWTIE formats (you just have to set the --format flag).\nOptions that you will have to use include:\n-t to indicate the input ChIP file\n-c to indicate the name of the control file\n--format the tag file format (if this option is not set MACS automatically detects which format the file is)\n--name to set the name of the output files\n--gsize to set the mappable genome size (with the read length we have, 70% of the genome is a fair estimation)\n--call-summits to detect all subpeaks in each enriched region and return their summits\n--pvalue the P-value cutoff for peak detection.\nNow run macs using the following command:\nmacs2 callpeak -t PAX5.sorted.bam -c Control.sorted.bam --format BAM --name PAX5 --gsize 138000000 --pvalue 1e-3 --call-summits\nMACS2 generates its peak files in a file format called .narrowPeak file. This is a BED format describing genomic locations. Many types of genomic data can be represented as (sets of) genomic regions.\nIn the following section we will look into the BED format in more detail, and we will perform simple operations on genomic interval data.\n\n7.1 What’s next?\nYou can head back to aligning the control sample to the genome or continue on to file formats.",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7_exercises.html#file-formats",
    "href": "course_modules/Module7/module7_exercises.html#file-formats",
    "title": "Exercises",
    "section": "8 File Formats",
    "text": "8 File Formats\n\n8.1 BED files\nOver the years a set of commonly used file formats for genomic intervals have emerged. Most of these file formats are tabular where each row consists of an interval and columns have a pre-defined meaning, describing chromosomes, locations, scores, etc. The UCSC web browser has an informative list of these at http://genome.ucsc.edu/FAQ/FAQformat.html.\nThe BED format is the simplest file format of these. A minimal bed file has at least three columns denoting chromosome, start and end of an interval. The following example denotes three intervals, two on chromosome chr1 and one on chr2.\n\n\n\nchromosome\nstart\nend\n\n\n\n\nchr1\n50\n100\n\n\nchr1\n500\n1000\n\n\nchr2\n600\n800\n\n\n\nBED files follow the UCSC Genome Browser’s convention of making the start position 0-based and the end position 1-based. In other words, you should interpret the “start” column as being 1 base pair higher than what is represented in the file. For example, the following BED feature represents a single base on chromosome 1; namely, the 1st base.\n\n\n\nchromosome\nstart\nend\ndescription\n\n\n\n\nchr1\n0\n1\nI-am-the-first-position-on-chrom-1\n\n\n\nUsing the bed format documentation found at http://genome.ucsc.edu/FAQ/FAQformat.html#format1 answer the following questions.\n\n\n8.1.1 Questions\nQ1. The simplest bed file contains just three columns (chromosome, start, end) and is often called BED3 format. What extra columns does BED6 contain?\nHint: look for information about columns 4 to 6 in the documentation http://genome.ucsc.edu/FAQ/FAQformat.html#format1\nQ2. In the above examples, what are the lengths of the intervals?\nQ3. Can you output a BED6 format with a transcript called “loc1”, transcribed on the forwardstrand and having three exons of length 100 starting at positions 1000, 2000 and 3000?\nHint: you will need one line per exon\n\n\n8.2 narrowPeak files\nThe narrowPeak format is a BED6+4 format used to describe and visualise called peaks. Previously, we have used MACS2 to call peaks on the PAX5 ChIP-seq data set.\nIf you are not in there already, change into the data directory.\ncd data\nView the first 10 lines in PAX5_peaks.narrowPeak using the head command:\nhead -10 PAX5_peaks.narrowPeak\nNarrowPeak files can also be uploaded to IGV or other genome browsers.\nTry uploading the peak file generated by MACS2 to IGV.\n\n\n8.2.1 Questions\nQ4. What additional information is given in the narrowPeak file, beside the location of the peaks?\nHint: See http://genome.ucsc.edu/FAQ/FAQformat.html#format12 for details\nQ5. Does the first peak that was called look convincing to you?\n\n\n8.3 GTF files\nA second popular format is the GTF format. Each row in a GTF formatted file denotes a genomic interval. The GTF format documentation can be found at http://mblab.wustl.edu/GTF2.html.\nThe three intervals from above might be:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nseqid\nsource\ntype\nstart\nstop\nscore\nstrand\nphase\nattributes\n\n\n\n\nchr1\ngene\nexon\n51\n100\n.\n+\n0\ngene_id “001”;transcript_id “001.1”;\n\n\nchr1\ngene\nexon\n501\n1000\n.\n+\n2\ngene_id “001”;transcript_id “001.1”;\n\n\nchr2\nrepeat\nexon\n601\n800\n.\n+\n.\n\n\n\n\nThe 9th column permits intervals to be grouped and linked in a hierarchical fashion. This format is thus popular to describe gene models. Note how the first two intervals are linked through a common transcript_id and gene_id.\nThe aim of the GENCODE project is to annotate all evidence-based genes and gene features in the entire human genome at a high accuracy. Annotation of the GENCODE gene set is carried out using a mix of manual annotation, experimental analysis and computational biology methods. The GENCODE v18 geneset is available in the genome folder.\nLook at the first 10 lines of the GENCODE annotation file:\nzcat genome/gencode.v18.annotation.gtf | head -n10\nzless -S genome/gencode.v18.annotation.gtf\n\n\n8.3.1 Questions\nQ6. In the small example table above, why have the coordinates changed from the BED description?\n\n\n8.4 What’s next?\nYou can head back to identifying enriched areas using MACS or continue on to inspecting genomic regions using bedtools.",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7_exercises.html#inspecting-genomic-regions-using-bedtools",
    "href": "course_modules/Module7/module7_exercises.html#inspecting-genomic-regions-using-bedtools",
    "title": "Exercises",
    "section": "9 Inspecting genomic regions using bedtools",
    "text": "9 Inspecting genomic regions using bedtools\nIn this section we perform simple functions, such as overlaps, on the most common file type used for describing genomic regions, the BED file. We will examine the results of the ChIP-Seq peak calling you have performed on the transcription factor PAX5 and perform simple operations on these files, using the bedtools suite of programs. You will then annotate the MACS2 peaks with respect to genomic annotations. Finally, we will select the most significantly enriched peaks, and extract the genomic sequence flanking their summits, the point of highest enrichment.\nIf you are not in there already, change into the data directory.\ncd data\nThe bedtools package permits complex, interval-based manipulation of BED and GTF files. They are also very fast. The general invocation of bedtools is bedtools &lt;COMMAND&gt;.\nTo get an overview of the available commands, simply call bedtools without any command or options in the terminal window.\nbedtools\nTo get help for a command, type bedtools &lt;COMMAND&gt;. Extensive documentation and examples are available at https://bedtools.readthedocs.org/en/latest/. We will now use bedtools to calculate simple coverage statistics of the peak calls over the genome (keep in mind that only peaks on Chromosome 1 are in the file).\nTo bring up the help page for the bedtools genomecov command, type:\nbedtools genomecov\nCalculate the genome coverage of the PAX5 peaks:\nbedtools genomecov -i PAX5_peaks.narrowPeak -g genome/hg19.chrom.sizes\nIn order to biologically interpret the results of ChIP-Seq experiments, it is useful to look at the genes and other annotated elements that are located in proximity to the identified enriched regions. We will now use bedtools to identify how many PAX5 peaks overlap GENCODE genes.\nFirst we use awk to filter out only the genes from the GTF file:\ngunzip genome/gencode.v18.annotation.gtf.gz\nawk '$3==\"gene\"' genome/gencode.v18.annotation.gtf &gt; genome/gencode.v18.annotation.genes.gtf\nNext, count the total number of PAX5 peaks:\nwc -l PAX5_peaks.narrowPeak\nThen use bedtools to find the number overlapping GENCODE genes:\nbedtools intersect -a PAX5_peaks.narrowPeak -b genome/gencode.v18.annotation.genes.gtf | wc -l\nYou can use the bedtools closest command to find the closest gene to each peak.\nbedtools closest -a PAX5_peaks.narrowPeak -b genome/gencode.v18.annotation.genes.gtf | head\nTranscription factor binding near to the transcript start sites (TSS) of genes is known to drive gene expression or repression, so it is of interest to know which TSS regions are bound by PAX5. To determine this, we will first create a BED file of the GENCODE TSS using the GTF.\nYou can use this awk command to create the TSS BED file:\nawk 'BEGIN {FS=OFS=\"\\t\"} { if($7==\"+\"){tss=$4-1} else { tss = $5 } \\\nprint $1,tss, tss+1, \".\", \".\", $7, $9}' genome/gencode.v18.annotation.genes.gtf &gt; genome/gencode.tss.bed\nNow use the bedtools closest command again to find the closest TSS to each peak:\nsortBed -i genome/gencode.tss.bed &gt; genome/gencode.tss.sorted.bed\nbedtools closest -a PAX5_peaks.narrowPeak -b genome/gencode.tss.sorted.bed &gt; PAX5_closestTSS.txt\nUse head to inspect the results:\nhead PAX5_closestTSS.txt\nYou have now matched up all the PAX5 transcription factor peaks to their nearest gene transcription start site\n\n9.1 Questions\nQ1. Looking at the output of the genomecov bedtools we ran, what percentage of chromosome 1 do the peaks of PAX5 cover?\nQ2. Looking at the output from bedtools intersect, what proportion of PAX5 peaks overlap genes?\nQ3. Looking at PAX5_closestTSS.txt, which gene was found to be closest to MACS peak 2?\n\n\n9.2 What’s next?\nYou can head back to file formats or continue on to motif analysis.",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7_exercises.html#motif-analysis",
    "href": "course_modules/Module7/module7_exercises.html#motif-analysis",
    "title": "Exercises",
    "section": "10 Motif analysis",
    "text": "10 Motif analysis\nIt is often interesting to find out whether we can associate the identified binding sites with a sequence pattern or motif. To do so, we will identify the summit regions of the strongest PAX5 binding sites, retrieve the sequences associated with these regions, and use MEME for motif analysis.\nSince many peak-finding tools merge overlapping areas of enrichment, the resulting peaks tend to be much wider than the actual binding sites. The summit and its vicinity are the best estimate for the true protein binding site, and so it is here where we look for repeated sequence patterns, called motifs, to which the transcription factor may preferentially bind.\nSub-dividing the enriched areas by accurately partitioning enriched loci into a finer-resolution set of individual binding sites, and fetching sequences from the summit region where binding motifs are most likely to appear enhances the quality of the motif analysis. Sub-peak summit sequences have already been called by MACS2 with the --call-summits option.\nDe novo motif finding programs take as input a set of sequences in which to search for repeated short sequences. Since motif discovery is computationally heavy, we will restrict our search for the Oct4 motif to the genome regions around the summits of the 300 most significant PAX5 subpeaks on Chromosome 1.\nIf you are not in there already, change into the data directory.\ncd data\nSort the PAX5 peaks by the height of the summit (the maximum number of overlapping reads).\nsort -k5 -nr PAX5_summits.bed &gt; PAX5_summits.sorted.bed\nUsing the sorted file, select the top 300 peaks and create a BED file for the regions of 60 base pairs centred around the peak summit.\nawk 'BEGIN{FS=OFS=\"\\t\"}; NR &lt; 301 { print $1, $2-30, $3+29 }' PAX5_summits.sorted.bed &gt; PAX5_top300_summits.bed\nThe human genome sequence is available in FASTA format in the bowtie_index directory.\nUse bedtools to extract the sequences around the PAX5 peak summits in FASTA format, which we save in a file named PAX5_top300_summits.fa.\ngunzip genome/HS19.fa.gz\nsamtools faidx genome/HS19.fa\nbedtools getfasta -fi genome/HS19.fa -bed PAX5_top300_summits.bed -fo PAX5_top300_summits.fa\nWe are now ready to perform de novo motif discovery, for which we will use the tool MEME.\nOpen a web bowser, go to the MEME website at http://meme-suite.org/, and choose the “MEME” tool.\nFill in the necessary details, such as:\n• the sub-peaks fasta file PAX5_top300_summits.fa (will need uploading), or just paste in the sequences.\n• the number of motifs we expect to find (1 per sequence)\n• the width of the desired motif (between 6 to 20) in the “Advanced” options\n• the maximum number of motifs to find (3 by default).\nFor PAX5 one classical motif is known.\nStart Search.\nYour MEME analysis will now be queued and will run on a server in the US. The results page will refresh automatically and once the tool has finished running there will be a link to the results.\nDepending on how busy the servers are your analysis may take a longer or shorter time to run.\nYou can check the load of the server here:\nhttp://meme-suite.org/opal2/dashboard?command=statistics\n\n10.1 Analyse the results from MEME\nWe would like to know if this motif is similar to any other known motif. We will use the results from TOMTOM for this.\nOn either the results from the web MEME run or the local run please follow the link “MEME html output”. Scroll down until you see the first motif logo.\nClick under the option Submit/Download and choose the TOMTOM button to compare to known motifs in motif databases, and on the new page choose to compare your motif to those in the JASPAR CORE and UniPROBE Mouse database.\n\n\n10.2 Running MEME locally\nIf you want to speed things up you may want to run MEME on your own machine. You can try to do this as well if you wish, or skip the following bonus exercise and go to the next section.\nTo bring up the help page for the local installation of MEME, type:\nmeme\nRun MEME locally, setting the output directory with the option -o (e.g. -o meme_out).\nmeme PAX5_top300_summits.fa -o meme_out -dna -nmotifs 1 -minw 6 -maxw 20\nOnce MEME has finished running look in this directory for the file meme.html and open it in a web browser. You can do this by either copying the path to the file to the address bar in Firefox or double click on the .html file.\nAlternatively, you can run the following command to automatically open the HTML file in Firefox:\nfirefox meme_out/meme.html\nScroll down until you see the first motif logo.\nWe would like to know if this motif is similar to any other known motif. We will use TOMTOM and a set of known motif databases stored in motif_databases for this.\nTo compare your newly found motifs to the motif databases JASPAR CORE and UniPROBE Mouse you can run:\ntomtom -o tomtom_out meme_out/meme.html motif_databases/JASPAR/JASPAR_CORE_2016_vertebrates.meme motif_databases/MOUSE/uniprobe_mouse.meme\nOnce again, once TOMTOM has finished running look in tomtom_out for the file tomtom.html.\nOpen tomtom.html in a web browser.\nfirefox tomtom_out/tomtom.html\n\n\n10.3 Questions\nQ1. Which motif was found to be the most similar to your motif?\n\n\n10.4 Congratulations, you have reached the end of this tutorial!\nWe hope you’ve enjoyed our ChIP-Seq tutorial. You can find the answers to all of the questions in this tutorial in answers.ipynb. You can revisit inspecting genomic regions using bedtools or, go back to the beginning.",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Train-the-trainer/elements.html",
    "href": "course_modules/Train-the-trainer/elements.html",
    "title": "Elements of effective training",
    "section": "",
    "text": "Within the context of this course we are mainly concentrating on teaching, training and mentoring as ways of conveying information to peers and colleagues. Many of the techniques and approaches to training are also relevant to other types of information sharing, such as communicating data or procedures to colleagues. \nThe following activities discuss effective communication in general, and then move on to visualising and communicating data and information.\n\n\n\n\n\n\nFurther reading:\nBest Practice Strategies for Effective Use of Questions as a Teaching Tool - PMC (nih.gov)\nLet’s enjoy the Q&A session",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Elements of effective training"
    ]
  },
  {
    "objectID": "course_modules/Train-the-trainer/elements.html#sharing-knowledgeskills-and-information-in-genomics-and-bioinformatics",
    "href": "course_modules/Train-the-trainer/elements.html#sharing-knowledgeskills-and-information-in-genomics-and-bioinformatics",
    "title": "Elements of effective training",
    "section": "",
    "text": "Within the context of this course we are mainly concentrating on teaching, training and mentoring as ways of conveying information to peers and colleagues. Many of the techniques and approaches to training are also relevant to other types of information sharing, such as communicating data or procedures to colleagues. \nThe following activities discuss effective communication in general, and then move on to visualising and communicating data and information.\n\n\n\n\n\n\nFurther reading:\nBest Practice Strategies for Effective Use of Questions as a Teaching Tool - PMC (nih.gov)\nLet’s enjoy the Q&A session",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Elements of effective training"
    ]
  },
  {
    "objectID": "course_modules/Train-the-trainer/elements.html#group-work-and-collaboration",
    "href": "course_modules/Train-the-trainer/elements.html#group-work-and-collaboration",
    "title": "Elements of effective training",
    "section": "Group work and collaboration",
    "text": "Group work and collaboration\nSeveral studies have shown the advantages of collaborative learning. Group work is one of the most studied and implemented training techniques in the world.",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Elements of effective training"
    ]
  },
  {
    "objectID": "course_modules/Train-the-trainer/elements.html#visualising-and-presenting-data-and-information",
    "href": "course_modules/Train-the-trainer/elements.html#visualising-and-presenting-data-and-information",
    "title": "Elements of effective training",
    "section": "Visualising and presenting data and information",
    "text": "Visualising and presenting data and information\nWith ever increasing amount and complexity of biological and genomic data and information, there is a growing need for visualising it in a clear and appealing manner and communicating it often to different audiences. To visualise data means to transform it into a picture reflecting the information, so that people can understand it. Data visualisation is also important for communicating research in publications.  \nHere are excerpts from an interview with Andy Kirk, specialist on data visualising and presenting. He suggests three main principles for data/information visualisation:\n1. It should be trustworthy, so the audience consumes reliable and accurate information and knowledge.\n2. It should be accessible. This means clarification, removing unnecessary obstacles to understanding. Accessible data presentation is about removing unnecessary confusion, things that people don’t understand how to read. \n3. The third principle is about elegance and appearance. Can we make our work as aesthetically appealing and as attractive as possible?\nSome important aspect when presenting data and information to an audience: \nThe audience Think about the audience you are presenting to, about the receiver of your message. What are the characteristics of your audience: What do they need to know? What do they currently not know? What are their motivations for what they will do with this? Is it about direct decision making or actions? Or is it just an extra grain of knowledge about something? Are they in a situation that’s under pressure? We should always try to put ourselves into the mindset of the recipient and think about their capabilities and how they will encounter our work.\n\nThe same data can be presented in many different ways to many different audiences. It’s not always practical to be able to share it in many different ways. We have to decide: there’ll be things that this person needs to know, and this person wants to know over here. We sometimes may have only one chance to present. We should think about the sense of prioritisation . If we try to present to the different audiences at the same time, there is a danger that potentially neither will receive the message. When we say: design for your audience, what it means is quite subtle, because it is about their characteristics, the capabilities, the confidence. \n\nPresenting research We might be thinking that the only people who will be reading our work are experts with the exact same knowledge, and the exact same interest in a subject as us, while that’s rarely the case. Even if we are sharing information with scientists, not everybody has the immediacy to understand what’s being portrayed. We should start off simplifying things, removing redundancies. Removing the things that are overly technical and that can be explained in a more accessible fashion. \nWhat tools should be used to present data? It is very easy for us to look around at all the new, innovative ideas, and think that there’s no role for the bar chart or for the line chart anymore, because they’re old charts. We still need those. There’s a time and place for every chart. But crucially, every chart answers a data question. One of the key things that we always need to establish, before we get excited about the ways that we might present our data, is to determine what it is we are trying to answer through an individual chart panel, what is it that someone looking at this work will be able to get an answer to. So, questions are crucial, questions are everything. In fact, questions come before data, because we collect data in response to a question.\nSelectivity A lot of scientists feel that every single data point is important. It may be that every data point is valuable, but there has to be a hierarchy. There must be some things that are more relevant, more important than others, an audience may have to look at something, and to understand something. We shouldn’t treat all data points equally. Some need to be suppressed, some need to be elevated. We can get a bit doubtful about the notion of editorial thinking, assuming that somehow it’s massaging results, or hiding things. But it is about editing, it’s about choosing what to include, and what to exclude.\nFeedback and collaboration The formats we can use to present can be different: is it a journal article, with very small size graphic? Is it for the mobile, or a tablet, on the move? Is it a huge display in a poster, where there is a presenter alongside the poster, and you can have a conversation about the piece. There’s lots of dynamics about the situation that someone would encounter our presentation. Asking collaborator or teamwork for a feedback before presenting is really important. Because sometimes, we try and do everything ourselves, but even if we can just introduce a second person’s mindset, ask a colleague, ask a critical friend about it, we can receive a different insight. Feedback is really something we reluctantly seek out, but we should. We should ask people to test things, run things past people. Sometimes, we ourselves as creators, get too close to our work, we lose sight of what it is we’re trying to say. Collaborating with others, getting other people to check things and test things, at the very least, is something that is going to give us the best chance of having the biggest impact with what it is we present.\nFurther reading\nHere is Andy’s website, presenting many examples and resources on data visualisation and presentation. \nTips for improving your visualisation design.",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Elements of effective training"
    ]
  },
  {
    "objectID": "course_modules/Train-the-trainer/training_design.html",
    "href": "course_modules/Train-the-trainer/training_design.html",
    "title": "Training design",
    "section": "",
    "text": "Motivation and inclusion of learners\nThe main motivation in the context of domain specific training such as genomics and bioinformatics, will be ‘competence motivation’. Although extrinsic rewards can affect behaviour, adult learners work/learn hard for intrinsic reasons as well.\nTo motivate and include your learners:\n\nintroduce social opportunities - feeling that one is contributing towards helping others, or one’s community can be very powerful motivation\nallow learners to see the usefulness of their learning – and impact on others, especially the local community (including teaching others, presenting learning to others, working effectively in groups).\nconsider different learners’ styles and needs: some learners will prefer learning through observing, some through discussing or collaboration with others, or ‘hands-on’ exercise. Accessibility is a very important factor to consider. When designing courses/training, different learning needs should be recognised and opportunities created for everyone to take part in the process of learning/training. Universal Design for Learning (UDL) principles offer a set of concrete suggestions that can be applied to any discipline or domain to ensure that all learners can access and participate in meaningful, challenging learning opportunities. UDL principles website contains many details on each of the following  main principles, which you can take with you from this course as a resource that you can study and apply later.   \n\nUDL proposes three main principles:\nProvide multiple means of engagement with the subject and learning environment, to support learners’ interests. For example, provide varied classroom environments and opportunities to work both collaboratively and alone. Offer learners a choice of ways to learn.\nProvide multiple means of representation of learning materials, for example, by offering learning content in different formats so that learners can choose the format that suits them. Same content can be offered in text-based, audio and video formats, or learners could be asked to explore a subject using whatever resources they can find through an online search.\nProvide multiple means of action and expression in learning, to provide learners alternatives for demonstrating what they know. For example, by giving learners a choice to write an essay, give a presentation or record a video.\n\n\n\n\nFurther study: UDL (Universal Design for Learning)\n\n\nMain principles and process of outcome driven training design\nA common approach to designing a course is known as outcome driven or “backward design”. This means starting with your training goals and working backwards to decide how best to achieve them. In this course we applied the backward design approach, firstly focusing on the outcomes for the course and then on how to demonstrate that the skills have been achieved. In an outcome driven design approach, the content and activities are developed once the learning outcomes are defined.\n\n\nFirst, you need to break down your goals into specific Learning Outcomes (LOs).\nLearning Outcomes define the knowledge, skills and attitudes that learners should be able to demonstrate after instruction or a learning intervention, the tangible evidence that the teaching goals have been achieved. In one of the next step of this course, you will learn how to write effective learning outcomes. The key point is to decide what you want your learners to be able to do, to fulfill the aims and goals of your training.\nAs you are formulating your Learning Outcomes, consider how exactly you will measure or assess whether the outcomes have been achieved, and how learners will demonstrate that they have new knowledge and skills. This could be through some kind of testing, but it doesn’t have to be. In many cases, the outcomes can be demonstrated by learners in different ways, and we talked about this in the previous step on Universal Design for Learning.\nOnce you have decided the Learning Outcomes and how they will be demonstrated, you can design the learning activities which will enable students to acquire, practise and apply the knowledge and skills needed to be able to do what you have defined. Importantly, this will include finding out what the students already know and giving them the opportunity to build on that. The activities you design will enable the students to learn the subject content to be able to achieve the Learning Outcomes.\n\nAlthough this design process is described as being “backward” it is really an iterative process. You may find that you revise your Learning Outcomes as you consider the practicalities of assessment or of designing learning activities. As you design particular elements of training, you will refer back to your Learning Outcomes to make sure that everything is aligned: the Learning Outcomes support the goals, the assessments allow the Learning Outcomes to be demonstrated, the learning activities allow students to practise the skills defined by the Learning Outcomes.\nIn summary, when focusing on learners’ needs and using the outcome-based design, you should:\n\nIdentify desired results\nDetermine what is the evidence of understanding or how will you assess that learners have achieved the desired outcome\nPlan the learning based on knowledge required about key concepts, skills and strategies required to perform the work and\nDesign the activities that will achieve the required outcome.\n\n\n\nIntroduction to competencies\nBefore we move into the next topic of how to write learning outcomes, let’s briefly mention here one of the factors that can influence a  choice of learning outcomes for a specific training that you might be trying to design. \nIn each field of work, there is usually a set of competencies (behaviours or technical attributes) that individuals should possess to successfully fulfil their professional roles within that field. When these competencies are  grouped together for a field or a profession, they are called competency frameworks.  In a course providing training for the roles defined in this way, the training designer should include the set of competencies required for each roles, and make sure that the intended learning outcomes can support/address specific or the competency framework requirements. \nCompetency mapping is a process of identifying key competencies needed to successfully carry out a specific job or set of tasks. Several competency frameworks exist in the field of genomics and bioinformatics, based on professional roles, from researchers to healthcare professionals (you will find some included in the References for this module). In this article we will briefly discuss the role of competency frameworks in designing a specific course, although they can also be used wider, to map existing courses within a programme, to determine which competencies are not addressed and thus determine the need for a new course.\nEach competency can be divided into knowledge, skills and attitudes (KSA) needed to successfully fulfill that competency within a role. Knowledge, skills and attitudes can further be defined in terms of the level (i.e. from basic to advanced) adequate for a specific role. \nIf your field has a set of competencies defined for specific roles/professions and you would like to incorporate them into the design of your training, you would normally follow the  process below:\n\nIdentify competencies that your training will address \nIdentify the level required for each competency\nConsider your target audience and any pre-requisite needed (these might in some cases be already defined by the competency framework you are using)\nDefine objectives and learning outcomes for your training which will address the knowledge, skills and attitude defined by the chosen competencies\nFlesh out curriculum and depth (level) of content \nAdd proposed content to cover the intended learning outcomes\nThink of ways to assess the intended learning outcomes \nDevelop activities and training material  \n\nThe iterative nature of course design and development assumes that this process is never one off and never a strict algorithmic procedure. There should also exist different evaluation mechanisms that can be planned at the stage of course design, to help with deciding on changes and improvements.\nIn the next step, we will concentrate on how to write learning outcomes, as part of the outcome driven design we talked about in the previous step and considered here in the context of using a competency framework.\n\n\nUse of Bloom’s taxonomy to write SMART Learning Outcomes\n\n\nFrom novices to experts\nLearning has a ‘vertical’ dimension - to develop competence in some area, a learner first has to have a deep foundation of factual knowledge, then to understand the material taught and be able to organise their knowledge in a way that facilitates retrieval and application across different contexts. Further progression on this cognitive ladder builds upon these foundation levels – with the development of the higher order cognitive abilities, which include analysis, evaluation and creation.\n\nBloom’s taxonomy\nBloom’s simplified model of cognitive development, in practice better known as Bloom’s taxonomy, is a theoretical framework widely used in education. It consists of six levels, with the three lower cognitive levels (knowledge, comprehension, and application) upon which higher levels (analysis, evaluation, creation) are built, as illustrated in the picture.\n\n\n\nBloom’s digital taxonomy\nBloom’s Digital Taxonomy. The addition of new verbs that represent actions in digital world, such as blog, programme etc. helps incorporate and create digitally enhanced learning using appropriate digital tools. Watch the video to find out more.\n\n\n\n\n\n\nLearning outcomes\nLearning outcomes (LOs) describe what course participants should be able to do or demonstrate – in terms of particular knowledge, skills, and attitudes – by the end of the course.\nFor example: By the end of this programme/ course participants will be able to:\n• Describe and critically evaluate a range of up-to-date genomic technologies and platforms used to sequence targeted parts of the genome or whole genomes\n• Discuss and critically appraise approaches to the bioinformatics\nIdentifying Learning Outcomes\n\nWhen identifying LOs consider what knowledge, understanding and skills you intend participants to learn through the course. The following questions may help with this:\n\n• What do you want participants to know and be able to do by the end of the course?\n• How will participants be able to use their learning? Doing what? What contexts?\n• What will participants need to do in order to demonstrate if / how well they have achieved these\noutcomes?\n• If participants are asked ‘what did you learn during this course?’ how would you like them to answer?\n\n\nFormulating learning outcomes\n\n\n\n\nWriting learning outcomes\nIt is helpful to express LOs using an active verb (what participants will be able to do) + object + qualifying phrase to provide a context. \nBy the end of the course, learners will be able to:\n\nCritically evaluate (verb) a range of up-to-date genomic technologies and platforms (object) used to sequence targeted parts of the genome or whole genomes (qualifying phrase)\nSynthesise (verb) information obtained from whole genome analysis with patient information (object) to determine diagnosis, penetrance or prognosis for a number of common and rare diseases (qualifying phrase)\n\n\n\nVocabulary for LOs\nBloom’s taxonomy provides a potential vocabulary for articulating different kinds and level of outcome. The following table is based on Bloom’s Taxonomy of cognitive learning and provides ways of describing outcomes at different levels of knowing and understanding.\n\n\n\nIntroduction to training evaluation\nEvaluation is the process of systematically generating knowledge that can support learning, quality improvement and good judgement in decision-making. Evaluation of training is important as it allows trainers to reflect on the strategy, format and content of the course and identify potential changes to improve the training impact.\nThe scope of the evaluation will influence what approach is most useful. You might be evaluating anything from a single learning activity, tool or technology, to a whole course or program, to the impact of educational policy at national or even international level.\nOne of the main principles of evaluation is that evaluation should start as soon as the initiative or strategy is being planned and implemented allowing the evaluation to inform its development and implementation from the start.\nEvaluation should also be adapted to the specific context where it takes place, and be flexible to adjustment based on different contextual changes, such as staff turnover, policy change or perhaps political and economic changes that might arise.\nMost importantly, evaluation should build the skills, knowledge and perspectives of all individuals involved, to self-reflect, communicate and act based on data and knowledge acquired throughout the evaluation process.\n\n\nTypes of evaluation\nEvaluation can start before the course to form part of a needs assessment to inform the scope and content of the training event. The evaluation process can then continue during and after the course to indicate whether specific objectives have been achieved.\nFormative evaluation is run during the course in order to identify problems and obstacles and make necessary adjustments in real-time. That way, it feeds back into the course usually immediately. This type of evaluation is normally carried out internally, and can focus on different aspects of the course, including performance monitoring.\nSummative evaluation is run after the course and aims to look at how effectively the course has achieved the expected outcomes and impact.\nImpact evaluations are a type of summative evaluation, and look at what has happened or changed as a result of an initiative and how that change occurred. It will typically look at the impact on people directly involved in an initiative (e.g. educators and learners) but also on other stakeholders.\nEvaluation is often a combination of different types mentioned above, and may include pre-course needs analysis, formative evaluation during the course as well as summative evaluation of the impact or some expected course outcome.\nWho benefits from evaluation?\n\nThe teachers or trainers, as evaluation is inherent to good teaching practice and allows them to see what works and what doesn’t.\nThe learners currently taking the course as their performance and improvement can be monitored and responded to appropriately.\nLearners who have completed the course, helping them to reflect on and evidence the impact of the course, in order to demonstrate CPD requirements or further their careers.\nFuture learners on the course who will benefit from improvements\nThe organisation running the course who can address accountability and QA benchmarks.\n\nFurther reading: Kellogg’s Evaluation Guide",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Training design"
    ]
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/lecture/src/hts-qc.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/lecture/src/hts-qc.html",
    "title": "only for printing",
    "section": "",
    "text": "% subset-slides\n\\vskip2em\n\\vskip10em\n\nData Formats\nFASTQ\n\nUnaligned read sequences with base qualities\n\nSAM/BAM\n\nUnaligned or aligned reads\nText and binary formats\n\nCRAM\n\nBetter compression than BAM\n\nVCF/BCF\n\nFlexible variant call format\nArbitrary types of sequence variation\nSNPs, indels, structural variations\n\n\\vskip2em\nSpecifications maintained by the Global Alliance for Genomics and Health\n\n\nFASTQ\n\nSimple format for raw unaligned sequencing reads\nExtension to the FASTA file format\nSequence and an associated per base quality score\n\n\nQuality encoded in ASCII characters with decimal codes 33-126\n\nASCII code of “A” is 65, the corresponding quality is Q\\(= 65 - 33 = 32\\)\n\n\\vskip0em\n\nPhred quality score: \\(P = 10^{-Q/10}\\) \n\nBeware: multiple quality scores were in use!\n\nSanger, Solexa, Illumina 1.3+\n\nPaired-end sequencing produces two FASTQ files\n\n{A: Q=30, one error in 1000 bases}\n\n\nSAM / BAM\nSAM (Sequence Alignment/Map) format\n\nUnified format for storing read alignments to a reference genome\nDeveloped by the 1000 Genomes Project group (2009)\n\n\nOne record (a single DNA fragment alignment) per line describing alignment between fragment and reference\n11 fixed columns + optional key:type:value tuples\n\n\\vskip0.5em\n\\vskip0.5em\n\\vskip1em\nNote that BAM can contain\n\nunmapped reads\nmultiple alignments of the same read\nsupplementary (chimeric) reads\n\n\n\nSAM\nSAM fields\n\\vskip1em \nCCCTAACCCTAACCATAGCCCTAACCCTAACCCTAACCCTAACCCT[…]CAAACCCACCCCCAAACCCAAAACCTCACCAC\nFFFFFJJJJJJJJFJJJJFJAJJJJJ-JJAAAJFJJFFJJF&lt;FJJFFJJJJFJJJJFF[…]&lt;—F—–A7-J-&lt;J-A–77AF—J7–\nMD:Z:1G24C2A76 PG:Z:MarkDuplicates RG:Z:1 NM:i:3 MQ:i:0 AS:i:94 XS:i:94 \\end{verbatim}}}\n\n\nCIGAR string\nCompact representation of sequence alignment\n\\vskip1em \nExamples:\n\n\nFlags\n\\vskip0.5em\n\\vskip0.5em\nBit operations made easy\n\npython \nsamtools flags \n\n\\vskip5em\n\n\nOptional tags\nEach lane has a unique RG tag that contains meta-data for the lane\nRG tags\n\nID: SRR/ERR number\nPL: Sequencing platform\nPU: Run name\nLB: Library name\nPI: Insert fragment size\nSM: Individual\nCN: Sequencing center\n\n\\vskip10em\n\n\nBAM\nBAM (Binary Alignment/Map) format\n\nBinary version of SAM\nDeveloped for fast processing and random access\n\nBGZF (Block GZIP) compression for indexing\n\n\nKey features\n\nCan store alignments from most mappers\nSupports multiple sequencing technologies\nSupports indexing for quick retrieval/viewing\nCompact size (e.g. 112Gbp Illumina = 116GB disk space)\nReads can be grouped into logical groups e.g. lanes, libraries, samples\nWidely supported by variant calling packages and viewers\n\n\n\nReference based Compression\nBAM files are too large\n\n~1.5-2 bytes per base pair\n\nIncreases in disk capacity are being far outstripped by sequencing technologies\nBAM stores all of the data\n\nEvery read base\nEvery base quality\nUsing a single conventional compression technique for all types of data\n\n\\vskip14em\n\n\nCRAM\nThree important concepts\n\nReference based compression\nControlled loss of quality information\nDifferent compression methods to suit the type of data, e.g. base qualities vs. metadata vs. extra tags\n\nIn lossless mode: 60% of BAM size\nArchives and sequencing centers moving from BAM to CRAM\n\nSupport for CRAM added to Samtools/HTSlib in 2014\nSoon to be available in Picard/GATK\n\n\\vskip1em\n\n\nVCF: Variant Call Format\nFile format for storing variation data\n\nTab-delimited text, parsable by standard UNIX commands\nFlexible and user-extensible\nCompressed with BGZF (bgzip), indexed with TBI or CSI (tabix)\n\n\\vskip1em\n\\vskip3em\n\n\nVCF / BCF\nVCFs can be very big\n\ncompressed VCF with 3781 samples, human data:\n\n54 GB for chromosome 1\n680 GB whole genome\n\n\nVCFs can be slow to parse\n\ntext conversion is slow\nmain bottleneck: FORMAT fields\n\n\\vskip0.5em\n\\vskip1em\nBCF\n\nbinary representation of VCF\nfields rearranged for fast access\n\n\\vskip0.5em\n\n\ngVCF\nOften it is not enough not know variant sites only\n\nwas a site dropped because of a reference call or because of missing data?\nwe need evidence for both variant and non-variant positions in the genome\n\n\\vskip0.5em\ngVCF\n\nblocks of reference-only sites can be represented in a single record using the INFO/END tag\nsymbolic alleles &lt;*&gt; for incremental calling\n\nraw, “callable” gVCF\ncalculate genotype likelihoods only once (an expensive step)\nthen call incrementally as more samples come in\n\n\n\\vskip0.5em\n\n\nOptimizing variant calls for speed\n\\vskip2em\n\\vskip3em\nNew TWK format by Marcus Klarqvist (under development)\n\nBCF still too slow for querying hundreds of thousands and millions of samples\nbigger but 100x faster for certain operations on GTs\n\n\n\nCustom formats for custom tasks\n\\centerline{ \\hskip1em } \\vskip1em \\centerline{ \\hskip2em }\n\n\nGlobal Alliance for Genomics and Health\nInternational coalition dedicated to improving human health\nMission\n\nestablish a common framework to enable sharing of genomic and clinical data\n\nWorking groups\n\nclinical\nregulatory and ethics\nsecurity\ndata\n\nData working group\n\nbeacon project .. test the willingness of international sites to share genetic data \nBRCA challenge .. advance understanding of the genetic basis of breast and other cancers \nmatchmaker exchange .. locate data on rare phenotypes or genotypes \nreference variation .. describe how genomes differ so researchers can assemble and interpret them \nbenchmarking .. develop variant calling benchmark toolkits for germline, cancer, and transcripts \nfile formats .. CRAM, SAM/BAM, VCF/BCF \n\nFile formats\n\nhttp://samtools.github.io/hts-specs/\n\n% # Coffee break and questions %\n% \\vskip7em %\n\n\nQuality Control\nBiases in sequencing\n\nBase calling accuracy\nRead cycle vs. base content\nGC vs. depth\nIndel ratio\n\n\\vskip0.5em\nBiases in mapping\n\\vskip1em\nGenotype checking\n\nSample swaps\nContaminations\n\n\n\nBase quality\nSequencing by synthesis: dephasing\n\ngrowing sequences in a cluster gradually desynchronize\nerror rate increases with read length\n\n\\vskip1em\nCalculate the average quality at each position across all reads\n\\vskip0.5em\n\\vskip0.5em\n\n\nBase calling errors\n\n\n\nBase quality\n\\centerline{ \\hskip5em } \n\n\nMismatches per cycle\nMismatches in aligned reads (requires reference sequence)\n\ndetect cycle-specific errors\nbase qualities are informative!\n\n\\vskip1em\n\n\nGC bias\nGC- and AT-rich regions are more difficult to amplify\n\ncompare the GC content against the expected distribution (reference sequence)\n\n\\vskip2em\n\\vskip3em\n\n\nGC content by cycle\nWas the adapter sequence trimmed?\n\\vskip1em\n\\vskip3em\n\n\nFragment size\nPaired-end sequencing: the size of DNA fragments matters\n\\vskip1em \\vskip3em\n\n\nQuiz\n\\vskip1em \\centerline{\\hskip2em} \\vskip1em\n\\vskip3em\n\n\nInsertions / Deletions per cycle\nFalse indels\n\nair bubbles in the flow cell can manifest as false indels\n\n\\vskip1em\n\\vskip3em\n\n\nAuto QC tests\nA suggestion for human data: \\vskip1em\n\\vskip2em\n\n\n\nDetecting sample swaps\nCheck the identity against a known set of variants\n\\vskip1em \\centerline{ \\hskip1em } \\vskip3em\n\n\nHow many markers are necessary?\n\\vskip1em Number of sites required to identify non-related human samples\n\\vskip1em \\centerline{ \\hskip1em }\n\n\nSoftware\nSoftware used to produce graphs in these slides\n\nsamtools stats and plot-bamstats\nbcftools gtcheck\nmatplotlib\n\n% # xxx %\n% Pipelining %\n% http://seqanswers.com % http://www.cbs.dtu.dk/courses/27626/Exercises/BAM-postprocessing.php %\n% Schwartz: Detection and Removal of Biases in the Analysis of Next- Generation Sequencing Reads % % # Biases in Next Generation Sequencing data %\n% * nucleotide per cycle bias % * mostly in RNA-seq, sometimes in ChIP-seq % * cannot be attributed to biased PCR-amplification % * partial explanation: random hexamer priming during reverse transcription? % - more references in the paper above: 14,15,16,17 %\n% dephasing %\n% The illumina platform uses a so called sequencing by synthesis process. Bases are added one at a time and the consensus is determined in a cluster of identical sequences. %\n% The source of errors can be numerous, here is one review that discusses the issues in more detail: %\n% The challenges of sequencing by synthesis, Nature Biotech, 2009 %\n% In a nuthsell a short answer to the best of my understanding is this: Not % all sequences in a cluster will grow at the same rate, this will slowly % lead to a desynchronization as the errors accumulate. This is why the % quality dips towards the end."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination-answers.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination-answers.html",
    "title": "Identifying contamination - Answers",
    "section": "",
    "text": "1. Streptococcus pneumoniae\n2. No\n3. ~7% of the reads. Look for “unclassified” at the top of the file."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion-answers.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion-answers.html",
    "title": "File conversion - Answers",
    "section": "",
    "text": "1. The CRAM file is ~18 MB. We can check this using:\nls -lh data/yeast.cram\n2. Yes, the BAM file is ~16 MB bigger than the CRAM file. We can check this using:\nls -lh data/yeast*"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment-answers.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment-answers.html",
    "title": "QC assessment of NGS data",
    "section": "",
    "text": "1. The peak is at 140 bp, and the read length is 100 bp. This means that the forward and reverse reads overlap with 60 bp.\n2. There are 400252 reads in total.\nLook inside the file and locate the field “raw total sequences”. To extract the information quickly from multiple files, commands similar to the following can be used:\ngrep ^SN lane*.sorted.bam.bchk | awk -F'\\t' '$2==\"raw total sequences:\"'\n3. 76% of the reads were mapped. Divide “reads mapped” (303036) by “raw total sequences” (400252).\n4. 2235 pairs mapped to a different chromosome. Look for “pairs on different chromosomes”\n5. The mean insert size is 275.9 and the standard deviation is 47.7. Look for “insert size mean” and “insert size standard deviation”.\n6. 282478 reads were properly paired. Look for “reads properly paired”.\n7. 23,803 (7.9%) of the reads have zero mapping quality. Look for “zero MQ” in the “Reads” section.\n8. The forward reads. Look at the “Quality per cycle” graphs."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats-answers.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats-answers.html",
    "title": "Data formats for NGS data - Answers",
    "section": "",
    "text": "1. There are 10 sequences in this file. To count all the header lines, we can use grep -c \"&gt;\" data/example.fasta\n2. There are 8 reads in this file. We can use grep to search for /1 or /2:\ngrep -c \"/1\" data/example.fastq\nAlternatively, we can use wc -l to count the lines in the file and then divide this by 4.\n3. RG = Read Group\n4. Illumina. See the __PL__field.\n5. SC. See the CN field.\n6. ERR003612. See the ID field.\n7. 2kbp. See the PI field.\n8. The quality is 48. We can use grep to find the id, followed by awk to print the fifth column:\ngrep \"ERR003762.5016205\" data/example.sam | awk '{print $5}'\n9. The CIGAR is 37M. We can use grep and awk to find it:\ngrep ERR003814.6979522 data/example.sam | awk '{print $6}'\n10. 213. The ninth column holds the insert size, so we can use awk to get this:\ngrep ERR003814.1408899 data/example.sam | awk '{print $9}'\n11. The CIGAR in Q9 was 37M, meaning all 37 bases in the read are either matches or mismatches to the reference.\n12. CIGAR: 4M 4I 8M. The first four bases in the read are the same as in the reference, so we can represent these as 4M in the CIGAR string. Next comes 4 insertions, represented by 4I, followed by 8 alignment matches, represented by 8M.\n13. NCBI build v37\n14. There are 15 lanes in the file. We can count the @RG lines manually, or use standard UNIX commands such as:\nsamtools view -H data/NA20538.bam | grep ^@RG | wc -l\nor\nsamtools view -H data/NA20538.bam | awk '{if($1==\"@RG\")n++}END{print n}'\n15. Looking at the @PG records ID tags, we see that three programs were used: GATK IndelRealigner, GATK TableRecalibration and bwa.\n16. The @PG records contain a the tag VN. From this we see that bwa version 0.5.5 was used.\n17. The first collumn holds the name of the read: ERR003814.1408899\n18. Chromosome 1, position 19999970. Column three contains the name of the reference sequenceand the fourth column holds the leftmost position of the clipped alignment.\n19. 320 reads are mapped to this region. We have already sorted and indexed the BAM file, so now we can search for the reagion using samtools view. Then we can pipe the output to wc to count the number of reads in this region:\nsamtools view data/NA20538_sorted.bam 1:20025000-20030000 | wc -l\n20. The reference version is 37. In the same way that we can use -h in samtools to include the header in the output, we can also use this with bcftools:\nbcftools view -h data/1kg.bcf | grep \"##reference\"\n21. There are 50 samples in the file. The -l option will list all samples in the file:\nbcftools query -l data/1kg.bcf | wc -l\n22. The genotype is A/T. With -f we specify the format of the output, -r is used to specify the region we are looking for, and with -s we select the sample.\nbcftools query -f'%POS [ %TGT]\\n' -r 20:24019472 -s HG00107 data/1kg.bcf\n23. There are 4778 positions with more than 10 alternate alleles. We can use -i to specify that we are looking for instances where the value of the INFO:AC tag (Allele Count) is greater than 10:\nbcftools query -f'%POS\\n' -i 'AC[0]&gt;10' data/1kg.bcf | wc -l\n24. There are 451 such positions. The first command picks out sample HG00107. We can then pipe the output to the second command to filter by depth and non-reference genotype. Then use wc to count the lines:\nbcftools view -s HG00107 data/1kg.bcf | bcftools query -i'FMT/DP&gt;10 & FMT/GT!=\"0/0\"' -f'%POS[ %GT %DP]\\n' | wc -l\n25. 26. The first base is at position 9923 and the last is at 9948.\n26. G. To reduce file size, only the first base is provided in the REF field.\n27. 10. See the MinDP tag in the INFO field."
  },
  {
    "objectID": "course_modules/template_module.html",
    "href": "course_modules/template_module.html",
    "title": "template_module",
    "section": "",
    "text": "Title of session/module\n  - Duration \n  - Key topics\n  - Activities (lectures, hands-on, discussions) - Use GHRU template here \n- Presentation slides (PPT/PDF)\n- Lecture notes or scripts\n- Videos (prerecorded lectures, screencasts, youtube etc, with transcripts)\n- Practical/lab exercises (e.g., Jupyter notebooks, Google Colab walkthrough, command line walkthroughs)\n- Datasets (example files or open data repositories)\n- Case studies or examples"
  },
  {
    "objectID": "course_modules/Module5/module5_manual.html",
    "href": "course_modules/Module5/module5_manual.html",
    "title": "Manual",
    "section": "",
    "text": "Genome assembly is the process of reconstructing an organism’s genome from millions or billions of short DNA sequences generated by sequencing technologies. Because current technologies cannot read entire chromosomes in one piece, genome assembly stitches these short reads together into longer continuous sequences that represent the genome. There are two main strategies for assembly: clone-based and whole-genome shotgun approaches. In the clone-based method, large DNA fragments are cloned, selected using genetic markers, and then sequenced individually to build the genome gradually. In contrast, the whole-genome shotgun approach involves randomly fragmenting the entire genome, sequencing all fragments, and using computational methods to assemble them into contiguous sequences. The ultimate goal is to produce an accurate, ideally complete, linear or circular representation of the genome.\nWhen performing genome assembly, several key considerations influence the quality and accuracy of the final assembly. Sequencing coverage is critical—higher coverage improves assembly reliability and helps resolve ambiguities. However, errors in reads (particularly with long-read technologies) can complicate alignment and contig formation. Read length also plays a vital role; longer reads span repetitive regions more effectively, improving assembly continuity. The presence of repeats or non-unique regions in the genome poses major challenges, as they can lead to misassemblies or gaps. Additionally, factors like genome size, heterozygosity, and ploidy affect assembly complexity—larger or more variable genomes require more sophisticated algorithms. Finally, the computational resources—including runtime and memory usage—must be considered, especially for large or complex genomes, to ensure efficient and successful assembly.\n\n\nRead: A short DNA sequence generated by a sequencing machine. Reads are the basic input for genome assembly.\nContig: A contiguous stretch of assembled sequence formed by overlapping reads with no gaps. Represents the first level of genome assembly.\nScaffold: An ordered set of contigs that are connected by gaps (usually filled with Ns) representing unknown or unsequenced regions. Scaffolds are often built using paired-end or long-range information.\nGap (N): A placeholder for missing or unresolvable sequence in scaffolds. Represented by the letter “N” in assembled genomes.\nChromosome: A complete DNA molecule, either linear or circular, containing a portion or all of the organism’s genome. The final goal of assembly is to reconstruct chromosomes or large portions of them from scaffolds.\nSheared Fragment: A random piece of genomic DNA produced during sequencing library preparation by physically or enzymatically breaking the genome into smaller segments.\nSize Fractionation: A method to select DNA fragments of a particular length range, improving consistency of read lengths for sequencing.\nSupercontig: Another term for scaffold; a larger sequence formed by linking multiple contigs based on additional information such as read pairs or long reads.\n\n\n\nSequencing produces many short reads, which are first assembled into contigs—contiguous stretches of DNA without gaps. These contigs are then organized into larger structures called scaffolds, which are ordered sets of contigs separated by gaps filled with Ns to represent unknown sequences.\n\n\n\n\n\nPacific Biosciences (PacBio) sequencing technology relies on single-molecule real-time (SMRT) sequencing, where a single DNA molecule is sequenced by a single polymerase enzyme within a zero-mode waveguide (ZMW) well. As the polymerase incorporates nucleotides, each base emits a distinct color flash, allowing real-time base detection. Uniquely, methylated bases produce characteristic signal patterns, enabling detection of epigenetic modifications. The system has no theoretical limit on DNA fragment length, enabling long read sequencing. Key advantages include long read lengths, low systematic error, and the ability to generate high-accuracy consensus reads through circular consensus sequencing (CCS), where the same molecule is read multiple times. However, disadvantages include a high per-base error rate and higher sequencing costs. As of 2019, the Sequel II system significantly improved performance, offering an 8-fold increase in yield and enhanced CCS accuracy, producing HiFi reads with both long lengths and high fidelity.\n\n\n\nNanopore sequencing, developed by Oxford Nanopore Technologies, offers a unique platform for real-time, portable DNA and RNA sequencing. Its key advantages include low capital cost, portability, and the ability to generate ultra-long reads, with lengths up to 1 megabase reported. The technology also enables the detection of base modifications (e.g., methylation) and can sequence RNA molecules directly, bypassing reverse transcription. These features make it particularly useful for de novo genome assembly, structural variant detection, and epigenetic studies. However, a major limitation is its high raw error rate, and it is particularly prone to systematic errors in homopolymer regions (stretches of the same base), which can affect accuracy in certain applications. Despite these challenges, nanopore sequencing continues to improve and provides valuable capabilities for field-based and large-scale genomic studies.\n\n\n\nRead clouds, also known as linked-reads, provide a cost-effective way to obtain long-range sequencing information using standard short-read platforms. Technologies such as 10X Genomics Chromium generate barcoded libraries in which multiple short reads are derived from the same long (~100 kb) DNA fragment and tagged with a unique barcode. This allows researchers to group reads from the same molecule, enabling improved resolution of complex repeats, structural variants, and haplotypes, and facilitating more accurate scaffolding of genome assemblies. These “read clouds” are particularly useful for bridging large genomic gaps or repeats that cannot be resolved using conventional short reads. The ARCS tool, developed by Yeo et al. (2018), leverages linked-read data to scaffold genome assemblies and has been shown to significantly improve contiguity and assembly quality. Reference: Yeo S, Coombe L, Warren RL, Chu J, Birol I. (2018). ARCS: scaffolding genome drafts with linked reads. Bioinformatics, 34(5), 725–731. https://doi.org/10.1093/bioinformatics/btx675\n\n\n\nHi-C is a powerful sequencing technique that captures the three-dimensional organization of chromatin through proximity ligation, linking DNA fragments that are spatially close in the nucleus. Although initially developed for studying chromatin architecture, Hi-C has proven highly effective for scaffolding genome assemblies, as most Hi-C interactions occur within the same chromosome, with a higher frequency between nearby regions. This spatial signal can be used to accurately order and orient contigs into chromosome-scale scaffolds. Automated scaffolding tools such as SALSA2 and 3D-DNA utilize Hi-C contact data for this purpose. Visualization tools like Juicer and HiGlass, and editing platforms such as Pretext, allow for manual inspection and refinement of genome assemblies based on Hi-C contact matrices. Reference: Dudchenko O, Batra SS, Omer AD, et al. (2017). De novo assembly of the Aedes aegypti genome using Hi-C yields chromosome-length scaffolds. Science, 356(6333), 92–95. https://doi.org/10.1126/science.aal3327\n\n\n\nOptical mapping is a genome analysis technique that uses high molecular weight DNA fragments to generate a physical map of the genome based on the location of specific sequence motifs. The process involves labeling short DNA motifs, linearizing the DNA molecules, and capturing high-resolution images to determine the spacing between labels. This method produces restriction digest maps that can span large regions of the genome and are especially useful for navigating repetitive regions, where traditional sequencing methods struggle. Optical mapping provides long-range genomic information, making it valuable for scaffolding assemblies, validating structural variants, and resolving complex genomic regions. Modern platforms like BioNano Genomics have scaled up this technology, enabling high-throughput, automated optical mapping for large genomes.\nReference: Lam ET, Hastie A, Lin C, et al. (2012). Genome mapping on nanochannel arrays for structural variation analysis and sequence assembly. Nature Biotechnology, 30(8), 771–776. https://doi.org/10.1038/nbt.2303\n\n\n\n\n\n\nLong range technologies. The bar chart shows the contributions of PacBio subreads (&gt;1 kb) (black), 10X Genomics linked-reads (green), and two BioNano optical mapping chemistries: BspQI/BssSI (light blue) and DLE1 (dark blue), used for scaffolding and assembly enhancement. Log-scaled x-axis represents scaffold N50 length (a common assembly continuity metric). BioNano DLE1 technology consistently produces the longest scaffold N50s, demonstrating its effectiveness in generating chromosome-scale assemblies when integrated with other technologies.\n\n\n\n\n\n\n\n\nAn example of a genome project workflow. PacBio long reads are used for initial contigging and purging; 10X Genomics linked reads, BioNano optical mapping, and Hi-C chromatin interaction data are used sequentially for scaffolding. The right panel shows downstream steps: gap-filling and polishing using long reads to improve sequence continuity and correct base errors; manual curation using read depth and annotation tracks; and finally, the generation of a primary assembly with alternate haplotypes. Together, these steps produce chromosome-scale, highly accurate genome assemblies.\n\n\n\n\n\nContig generation is the first step in genome assembly, where overlapping sequencing reads are stitched together into continuous sequences. Two primary strategies are used: Overlap-Layout-Consensus (OLC) and de Bruijn Graph (DBG). The OLC approach identifies overlaps between all pairs of reads, builds a layout graph based on these overlaps, and derives consensus sequences to form contigs. While effective for long reads, OLC is computationally intensive due to its quadratic scaling with the number of reads. Assemblers like Falcon, Canu, and minimap/miniasm are widely used for long-read data (PacBio, ONT). The DBG method, commonly used for short-read data, constructs a graph from all k-mers (subsequences of length k) found in the dataset, and uses this structure to assemble contigs. Tools like Velvet, ABySS, SPAdes, and wtdbg2 implement this approach. Both methods face challenges from heterozygosity, sequencing errors, repeats, low-complexity regions, and base composition or sequencing biases, all of which can complicate accurate contig generation.\n\n\n\nAssembly Metrics are key quantitative measures used to assess the quality and completeness of a genome assembly. Important metrics include the total assembly length, the number of sequences (both contigs and scaffolds), and the average, largest, and smallest sequence lengths. A commonly used statistic is the N50, which represents the length X such that 50% of the total assembly is contained in contigs or scaffolds of at least that length—indicating the assembly’s continuity. NG50 is a related metric, but is calculated with respect to the expected genome size, making it more informative for comparing assemblies of different organisms. Another important measure is gene content, which refers to the percentage of conserved core genes (e.g., using BUSCO or CEGMA) that are present and correctly assembled, giving insight into the assembly’s biological completeness.\n\n\n\nTotal Length: The combined length of all contigs or scaffolds in the genome assembly.\nContig: A contiguous sequence assembled from reads without gaps.\nScaffold: An ordered set of contigs linked by additional data (e.g., paired-end reads), with gaps often filled by Ns.\nN50: The length X such that 50% of the assembly is contained in contigs or scaffolds of length ≥ X.\nNG50: A variant of N50 that is calculated using the expected genome size rather than the total assembly length.\nGene Content: The percentage of conserved single-copy genes (e.g., from BUSCO datasets) present in the assembly, used to assess completeness.\nAverage Length: The mean length of all contigs or scaffolds in the assembly.\nLargest/Smallest Sequence: The lengths of the longest and shortest contig or scaffold in the assembly.\n\n\n\nN50\n\n\n\n\n\nScaffolding is a crucial step in genome assembly that aims to order and orient contigs into larger, more complete sequences called scaffolds. To achieve this, sequencing libraries with varying insert sizes are constructed. These range from short insert libraries (e.g., 2–6 kb), to intermediate (10–40 kb), and even large insert libraries (&gt;100 kb). By sequencing the ends of these longer fragments, researchers obtain information on the relative positions and orientations of contigs. Various sources of scaffolding evidence include Illumina mate-pair libraries, fosmid ends, bacterial artificial chromosomes (BACs), 10X Genomics linked reads, Hi-C proximity ligation data, and optical maps. During scaffolding, contigs are connected with gaps represented by Ns, indicating unknown sequences that separate the contigs but are supported by spatial linkage information. This process helps improve assembly continuity and bring the genome closer to its true chromosomal structure.\n\n\n\nScaffolding\n\n\n\n\n\n(Pseudo)chromosome assignment refers to the process of assembling scaffolds into chromosome-scale sequences, often referred to as pseudomolecules. This step is essential for organizing genome assemblies into biologically meaningful units that reflect true chromosome structure. The process relies on external sources of evidence such as a closely related reference genome, outgroup genomes, or genetic maps with markers. One commonly used tool for this purpose is RACA (Reference-Assisted Chromosome Assembly), which integrates data from a reference genome, a de novo assembly, and one or more outgroup genomes. RACA works by identifying syntenic fragments—conserved blocks of sequence across species—and then computes the likelihood of scaffold adjacencies based on evolutionary conservation and assembly data. This results in chromosome-scale assemblies even in species where true karyotypes are unavailable, facilitating downstream comparative genomics and annotation.\nReference: Kim J, Larkin DM, Cai Q, et al. (2013). Reference-assisted chromosome assembly. Proceedings of the National Academy of Sciences, 110(5), 1785–1790. https://doi.org/10.1073/pnas.1220349110\n\n\n\nMost traditional genome assemblers operate under the assumption that the genome is haploid, meaning they aim to produce a single, consensus sequence while ignoring allelic variation between the two parental chromosomes. This simplification can mask biologically important differences, especially in highly heterozygous organisms. In contrast, diploid assembly seeks to reconstruct both sets of chromosomes separately, generating two haplotype-phased genomes that preserve the unique sequence of each parental lineage. This approach improves the resolution of heterozygous variants, structural differences, and gene content across haplotypes. Tools such as Falcon-unzip have been developed for this purpose, using long-read data to identify phase blocks and separate haplotypes during the assembly process. Diploid assembly is particularly valuable in studies of genetic diversity, disease allele discovery, and functional genomics.\nReference: Chin CS, Peluso P, Sedlazeck FJ, et al. (2016). Phased diploid genome assembly with single-molecule real-time sequencing. Nature Methods, 13(12), 1050–1054. https://doi.org/10.1038/nmeth.4035\n\n\n\n\n\n\nHeterozygosity and allelic duplication. The bar graph shows assembled genome size as a percentage of the expected genome size for various species (black = primary assembly, blue = alternate haplotigs), plotted alongside heterozygosity rates (grey line). Assemblies with higher heterozygosity tend to include more alternate haplotigs, resulting in a total assembly size greater than 100%. The top panel illustrates how diploid assembly captures both primary and alternate haplotypes. The diagram at right shows how low heterozygosity regions are collapsed into a single contig, while high heterozygosity regions are resolved into separate primary (p-contig) and alternate (h-contig) sequences. Data from Chin et al., Nature Methods (2016). https://doi.org/10.1038/nmeth.4035\n\n\n\n\n\n\n\n\nHaplotype duplication\n\n\nHaplotype duplication represents a common challenge in assembling high-heterozygosity diploid genomes. Assemblers like Falcon-unzip may unintentionally include both haplotypes in the primary assembly, leading to allelic duplication and overestimation of genome size. The tool purge_haplotigs helps resolve this by identifying these redundant sequences (called haplotigs) and relocating them to a separate bin, improving the representation of the true haploid genome. In addition, Falcon-phase uses Hi-C data to properly phase the assembly by switching segments between the primary and alternate haplotypes to reflect accurate inheritance. These tools work together to reduce redundancy and improve the structural and biological correctness of genome assemblies. For more details, see:\nReferences:\n\nRoach MJ, Schmidt SA, Borneman AR. (2018). Purge Haplotigs: Synteny Reduction for Third-gen Diploid Genome Assemblies. https://doi.org/10.1186/s13059-018-1564-6\nGuan D et al. (2020). Identifying and removing haplotypic duplication in primary genome assemblies. Bioinformatics, 36(9), 2896–2898. https://doi.org/10.1093/bioinformatics/btaa025\n\n\n\n\nFALCON-Phase is a genome phasing tool designed to resolve diploid genomes by combining long-read sequencing data (e.g., from PacBio) with Hi-C chromatin contact data. While tools like FALCON-Unzip can separate haplotypes locally using SNP information from long reads, FALCON-Phase extends phasing over longer genomic distances by using Hi-C’s long-range linkage information. The process involves several steps: (1) FALCON-Unzip generates primary contigs and associated haplotigs; (2) haplotigs are aligned back to their positions on the primary contig; (3) the sequence is “minced” into phased blocks; (4) Hi-C reads are mapped to these blocks; (5) a phasing algorithm determines which haplotigs belong to each haplotype based on contact patterns; and (6) the tool emits two phased haplotigs—phase 0 and phase 1—representing the maternal and paternal chromosomes. This process improves the accuracy and completeness of haplotype-resolved assemblies, particularly in regions of high heterozygosity.\nReference: Kronenberg ZN et al. (2021). Extended haplotype-phasing of long-read de novo genome assemblies using Hi-C. https://www.nature.com/articles/s41467-020-20536-y\n\n\n\nFALCON-Phase\n\n\n\n\n\nGenome assembly quality control (QC) is a critical step in ensuring the accuracy, completeness, and usability of assembled genomes. One important metric is base accuracy, which can be assessed by realigning reads from the same species and identifying SNPs and indels—with indels often being more frequent in PacBio and Nanopore assemblies. Tools like KAT can evaluate k-mer completeness using Illumina data to estimate how much of the expected sequence content is present in the assembly. Local structural accuracy can be assessed using external evidence, such as read alignment patterns (via tools like REAPR or QUAST) or through known physical linkages like PCR-validated adjacencies. Another key QC aspect is gene content, which involves verifying the order and orientation of genes and exons, especially for well-characterized genes such as housekeeping genes.\n\n\n\nBUSCO (Benchmarking Universal Single-Copy Orthologs) is a widely used tool for evaluating the completeness of genome assemblies, transcriptomes, and gene annotations. It works by identifying a set of evolutionarily conserved genes that are expected to be present as single copies in nearly all species within a specific clade. BUSCO scans the input dataset to detect these orthologs and classifies them as Complete (C), Duplicated (D), Fragmented (F), or Missing (M). A high percentage of complete single-copy genes indicates a high-quality and complete assembly. The BUSCO pipeline combines tBLASTn, Augustus gene prediction, and HMMER3 profile searches to accurately locate and classify each ortholog, and can be run on genome assemblies, transcriptome data, or gene sets. It has become a standard benchmark for genome quality assessment in both de novo assembly and annotation pipelines.\nReference: Simão FA, Waterhouse RM, Ioannidis P, Kriventseva EV, Zdobnov EM. (2015). BUSCO: assessing genome assembly and annotation completeness with single-copy orthologs. Bioinformatics, 31(19), 3210–3212. https://doi.org/10.1093/bioinformatics/btv351\n\n\n\nMetagenomic assembly involves reconstructing genomic sequences from DNA extracted from complex environmental samples, typically containing multiple uncultured microbial species. Unlike traditional genome assembly, metagenomic assembly must address the challenge of mixed organisms with varying abundance levels, making it difficult to distinguish low-abundance genomes from sequencing errors or contaminants. Additionally, sequence divergence across species and strain-level variation adds complexity to assembly and binning. The typical output is a collection of contigs with associated coverage information, which provides a simplified yet informative representation of the community’s genetic content. However, assembling complete genomes is often difficult, and scaffolding approaches must be carefully selected or adapted to avoid chimeric assemblies. Some recent methods integrate abundance profiles, sequence composition, and differential coverage across samples to improve binning and contig linkage. New strategies are discussed by Ayling et al. (2019), who review tools and frameworks designed specifically for short-read metagenomic assembly and highlight the need for scalable, hybrid approaches.\n\nReference: Ayling M, Clark MD, Leggett RM. (2019). New approaches for metagenome assembly with short reads. Brief Bioinform. https://doi.org/10.1093/bib/bbz020\n\n\n\nTranscriptome assembly focuses on reconstructing expressed RNA sequences (cDNA) from a biological sample, rather than the entire genome. This form of assembly is complicated by the fact that, like in metagenomics, there is no single linear reference sequence—instead, it must handle a diverse and dynamic set of transcripts. The varying abundance of transcripts, along with alternative splicing, results in complex branching structures during assembly. Transcriptome assemblers must distinguish true isoforms from assembly errors and resolve exon skipping, intron retention, and other splicing events to accurately reconstruct full-length transcripts. De novo transcriptome assembly tools, such as Trinity, address this by using k-mer graphs to represent transcript variation. The resulting transcript contigs can then be quantified, annotated, and used for downstream analyses like differential expression, even in species lacking a reference genome.",
    "crumbs": [
      "Home",
      "Genome Assembly",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module5/module5_manual.html#overview-of-genome-assembly",
    "href": "course_modules/Module5/module5_manual.html#overview-of-genome-assembly",
    "title": "Manual",
    "section": "",
    "text": "Genome assembly is the process of reconstructing an organism’s genome from millions or billions of short DNA sequences generated by sequencing technologies. Because current technologies cannot read entire chromosomes in one piece, genome assembly stitches these short reads together into longer continuous sequences that represent the genome. There are two main strategies for assembly: clone-based and whole-genome shotgun approaches. In the clone-based method, large DNA fragments are cloned, selected using genetic markers, and then sequenced individually to build the genome gradually. In contrast, the whole-genome shotgun approach involves randomly fragmenting the entire genome, sequencing all fragments, and using computational methods to assemble them into contiguous sequences. The ultimate goal is to produce an accurate, ideally complete, linear or circular representation of the genome.\nWhen performing genome assembly, several key considerations influence the quality and accuracy of the final assembly. Sequencing coverage is critical—higher coverage improves assembly reliability and helps resolve ambiguities. However, errors in reads (particularly with long-read technologies) can complicate alignment and contig formation. Read length also plays a vital role; longer reads span repetitive regions more effectively, improving assembly continuity. The presence of repeats or non-unique regions in the genome poses major challenges, as they can lead to misassemblies or gaps. Additionally, factors like genome size, heterozygosity, and ploidy affect assembly complexity—larger or more variable genomes require more sophisticated algorithms. Finally, the computational resources—including runtime and memory usage—must be considered, especially for large or complex genomes, to ensure efficient and successful assembly.\n\n\nRead: A short DNA sequence generated by a sequencing machine. Reads are the basic input for genome assembly.\nContig: A contiguous stretch of assembled sequence formed by overlapping reads with no gaps. Represents the first level of genome assembly.\nScaffold: An ordered set of contigs that are connected by gaps (usually filled with Ns) representing unknown or unsequenced regions. Scaffolds are often built using paired-end or long-range information.\nGap (N): A placeholder for missing or unresolvable sequence in scaffolds. Represented by the letter “N” in assembled genomes.\nChromosome: A complete DNA molecule, either linear or circular, containing a portion or all of the organism’s genome. The final goal of assembly is to reconstruct chromosomes or large portions of them from scaffolds.\nSheared Fragment: A random piece of genomic DNA produced during sequencing library preparation by physically or enzymatically breaking the genome into smaller segments.\nSize Fractionation: A method to select DNA fragments of a particular length range, improving consistency of read lengths for sequencing.\nSupercontig: Another term for scaffold; a larger sequence formed by linking multiple contigs based on additional information such as read pairs or long reads.\n\n\n\nSequencing produces many short reads, which are first assembled into contigs—contiguous stretches of DNA without gaps. These contigs are then organized into larger structures called scaffolds, which are ordered sets of contigs separated by gaps filled with Ns to represent unknown sequences.\n\n\n\n\n\nPacific Biosciences (PacBio) sequencing technology relies on single-molecule real-time (SMRT) sequencing, where a single DNA molecule is sequenced by a single polymerase enzyme within a zero-mode waveguide (ZMW) well. As the polymerase incorporates nucleotides, each base emits a distinct color flash, allowing real-time base detection. Uniquely, methylated bases produce characteristic signal patterns, enabling detection of epigenetic modifications. The system has no theoretical limit on DNA fragment length, enabling long read sequencing. Key advantages include long read lengths, low systematic error, and the ability to generate high-accuracy consensus reads through circular consensus sequencing (CCS), where the same molecule is read multiple times. However, disadvantages include a high per-base error rate and higher sequencing costs. As of 2019, the Sequel II system significantly improved performance, offering an 8-fold increase in yield and enhanced CCS accuracy, producing HiFi reads with both long lengths and high fidelity.\n\n\n\nNanopore sequencing, developed by Oxford Nanopore Technologies, offers a unique platform for real-time, portable DNA and RNA sequencing. Its key advantages include low capital cost, portability, and the ability to generate ultra-long reads, with lengths up to 1 megabase reported. The technology also enables the detection of base modifications (e.g., methylation) and can sequence RNA molecules directly, bypassing reverse transcription. These features make it particularly useful for de novo genome assembly, structural variant detection, and epigenetic studies. However, a major limitation is its high raw error rate, and it is particularly prone to systematic errors in homopolymer regions (stretches of the same base), which can affect accuracy in certain applications. Despite these challenges, nanopore sequencing continues to improve and provides valuable capabilities for field-based and large-scale genomic studies.\n\n\n\nRead clouds, also known as linked-reads, provide a cost-effective way to obtain long-range sequencing information using standard short-read platforms. Technologies such as 10X Genomics Chromium generate barcoded libraries in which multiple short reads are derived from the same long (~100 kb) DNA fragment and tagged with a unique barcode. This allows researchers to group reads from the same molecule, enabling improved resolution of complex repeats, structural variants, and haplotypes, and facilitating more accurate scaffolding of genome assemblies. These “read clouds” are particularly useful for bridging large genomic gaps or repeats that cannot be resolved using conventional short reads. The ARCS tool, developed by Yeo et al. (2018), leverages linked-read data to scaffold genome assemblies and has been shown to significantly improve contiguity and assembly quality. Reference: Yeo S, Coombe L, Warren RL, Chu J, Birol I. (2018). ARCS: scaffolding genome drafts with linked reads. Bioinformatics, 34(5), 725–731. https://doi.org/10.1093/bioinformatics/btx675\n\n\n\nHi-C is a powerful sequencing technique that captures the three-dimensional organization of chromatin through proximity ligation, linking DNA fragments that are spatially close in the nucleus. Although initially developed for studying chromatin architecture, Hi-C has proven highly effective for scaffolding genome assemblies, as most Hi-C interactions occur within the same chromosome, with a higher frequency between nearby regions. This spatial signal can be used to accurately order and orient contigs into chromosome-scale scaffolds. Automated scaffolding tools such as SALSA2 and 3D-DNA utilize Hi-C contact data for this purpose. Visualization tools like Juicer and HiGlass, and editing platforms such as Pretext, allow for manual inspection and refinement of genome assemblies based on Hi-C contact matrices. Reference: Dudchenko O, Batra SS, Omer AD, et al. (2017). De novo assembly of the Aedes aegypti genome using Hi-C yields chromosome-length scaffolds. Science, 356(6333), 92–95. https://doi.org/10.1126/science.aal3327\n\n\n\nOptical mapping is a genome analysis technique that uses high molecular weight DNA fragments to generate a physical map of the genome based on the location of specific sequence motifs. The process involves labeling short DNA motifs, linearizing the DNA molecules, and capturing high-resolution images to determine the spacing between labels. This method produces restriction digest maps that can span large regions of the genome and are especially useful for navigating repetitive regions, where traditional sequencing methods struggle. Optical mapping provides long-range genomic information, making it valuable for scaffolding assemblies, validating structural variants, and resolving complex genomic regions. Modern platforms like BioNano Genomics have scaled up this technology, enabling high-throughput, automated optical mapping for large genomes.\nReference: Lam ET, Hastie A, Lin C, et al. (2012). Genome mapping on nanochannel arrays for structural variation analysis and sequence assembly. Nature Biotechnology, 30(8), 771–776. https://doi.org/10.1038/nbt.2303\n\n\n\n\n\n\nLong range technologies. The bar chart shows the contributions of PacBio subreads (&gt;1 kb) (black), 10X Genomics linked-reads (green), and two BioNano optical mapping chemistries: BspQI/BssSI (light blue) and DLE1 (dark blue), used for scaffolding and assembly enhancement. Log-scaled x-axis represents scaffold N50 length (a common assembly continuity metric). BioNano DLE1 technology consistently produces the longest scaffold N50s, demonstrating its effectiveness in generating chromosome-scale assemblies when integrated with other technologies.\n\n\n\n\n\n\n\n\nAn example of a genome project workflow. PacBio long reads are used for initial contigging and purging; 10X Genomics linked reads, BioNano optical mapping, and Hi-C chromatin interaction data are used sequentially for scaffolding. The right panel shows downstream steps: gap-filling and polishing using long reads to improve sequence continuity and correct base errors; manual curation using read depth and annotation tracks; and finally, the generation of a primary assembly with alternate haplotypes. Together, these steps produce chromosome-scale, highly accurate genome assemblies.\n\n\n\n\n\nContig generation is the first step in genome assembly, where overlapping sequencing reads are stitched together into continuous sequences. Two primary strategies are used: Overlap-Layout-Consensus (OLC) and de Bruijn Graph (DBG). The OLC approach identifies overlaps between all pairs of reads, builds a layout graph based on these overlaps, and derives consensus sequences to form contigs. While effective for long reads, OLC is computationally intensive due to its quadratic scaling with the number of reads. Assemblers like Falcon, Canu, and minimap/miniasm are widely used for long-read data (PacBio, ONT). The DBG method, commonly used for short-read data, constructs a graph from all k-mers (subsequences of length k) found in the dataset, and uses this structure to assemble contigs. Tools like Velvet, ABySS, SPAdes, and wtdbg2 implement this approach. Both methods face challenges from heterozygosity, sequencing errors, repeats, low-complexity regions, and base composition or sequencing biases, all of which can complicate accurate contig generation.\n\n\n\nAssembly Metrics are key quantitative measures used to assess the quality and completeness of a genome assembly. Important metrics include the total assembly length, the number of sequences (both contigs and scaffolds), and the average, largest, and smallest sequence lengths. A commonly used statistic is the N50, which represents the length X such that 50% of the total assembly is contained in contigs or scaffolds of at least that length—indicating the assembly’s continuity. NG50 is a related metric, but is calculated with respect to the expected genome size, making it more informative for comparing assemblies of different organisms. Another important measure is gene content, which refers to the percentage of conserved core genes (e.g., using BUSCO or CEGMA) that are present and correctly assembled, giving insight into the assembly’s biological completeness.\n\n\n\nTotal Length: The combined length of all contigs or scaffolds in the genome assembly.\nContig: A contiguous sequence assembled from reads without gaps.\nScaffold: An ordered set of contigs linked by additional data (e.g., paired-end reads), with gaps often filled by Ns.\nN50: The length X such that 50% of the assembly is contained in contigs or scaffolds of length ≥ X.\nNG50: A variant of N50 that is calculated using the expected genome size rather than the total assembly length.\nGene Content: The percentage of conserved single-copy genes (e.g., from BUSCO datasets) present in the assembly, used to assess completeness.\nAverage Length: The mean length of all contigs or scaffolds in the assembly.\nLargest/Smallest Sequence: The lengths of the longest and shortest contig or scaffold in the assembly.\n\n\n\nN50\n\n\n\n\n\nScaffolding is a crucial step in genome assembly that aims to order and orient contigs into larger, more complete sequences called scaffolds. To achieve this, sequencing libraries with varying insert sizes are constructed. These range from short insert libraries (e.g., 2–6 kb), to intermediate (10–40 kb), and even large insert libraries (&gt;100 kb). By sequencing the ends of these longer fragments, researchers obtain information on the relative positions and orientations of contigs. Various sources of scaffolding evidence include Illumina mate-pair libraries, fosmid ends, bacterial artificial chromosomes (BACs), 10X Genomics linked reads, Hi-C proximity ligation data, and optical maps. During scaffolding, contigs are connected with gaps represented by Ns, indicating unknown sequences that separate the contigs but are supported by spatial linkage information. This process helps improve assembly continuity and bring the genome closer to its true chromosomal structure.\n\n\n\nScaffolding\n\n\n\n\n\n(Pseudo)chromosome assignment refers to the process of assembling scaffolds into chromosome-scale sequences, often referred to as pseudomolecules. This step is essential for organizing genome assemblies into biologically meaningful units that reflect true chromosome structure. The process relies on external sources of evidence such as a closely related reference genome, outgroup genomes, or genetic maps with markers. One commonly used tool for this purpose is RACA (Reference-Assisted Chromosome Assembly), which integrates data from a reference genome, a de novo assembly, and one or more outgroup genomes. RACA works by identifying syntenic fragments—conserved blocks of sequence across species—and then computes the likelihood of scaffold adjacencies based on evolutionary conservation and assembly data. This results in chromosome-scale assemblies even in species where true karyotypes are unavailable, facilitating downstream comparative genomics and annotation.\nReference: Kim J, Larkin DM, Cai Q, et al. (2013). Reference-assisted chromosome assembly. Proceedings of the National Academy of Sciences, 110(5), 1785–1790. https://doi.org/10.1073/pnas.1220349110\n\n\n\nMost traditional genome assemblers operate under the assumption that the genome is haploid, meaning they aim to produce a single, consensus sequence while ignoring allelic variation between the two parental chromosomes. This simplification can mask biologically important differences, especially in highly heterozygous organisms. In contrast, diploid assembly seeks to reconstruct both sets of chromosomes separately, generating two haplotype-phased genomes that preserve the unique sequence of each parental lineage. This approach improves the resolution of heterozygous variants, structural differences, and gene content across haplotypes. Tools such as Falcon-unzip have been developed for this purpose, using long-read data to identify phase blocks and separate haplotypes during the assembly process. Diploid assembly is particularly valuable in studies of genetic diversity, disease allele discovery, and functional genomics.\nReference: Chin CS, Peluso P, Sedlazeck FJ, et al. (2016). Phased diploid genome assembly with single-molecule real-time sequencing. Nature Methods, 13(12), 1050–1054. https://doi.org/10.1038/nmeth.4035\n\n\n\n\n\n\nHeterozygosity and allelic duplication. The bar graph shows assembled genome size as a percentage of the expected genome size for various species (black = primary assembly, blue = alternate haplotigs), plotted alongside heterozygosity rates (grey line). Assemblies with higher heterozygosity tend to include more alternate haplotigs, resulting in a total assembly size greater than 100%. The top panel illustrates how diploid assembly captures both primary and alternate haplotypes. The diagram at right shows how low heterozygosity regions are collapsed into a single contig, while high heterozygosity regions are resolved into separate primary (p-contig) and alternate (h-contig) sequences. Data from Chin et al., Nature Methods (2016). https://doi.org/10.1038/nmeth.4035\n\n\n\n\n\n\n\n\nHaplotype duplication\n\n\nHaplotype duplication represents a common challenge in assembling high-heterozygosity diploid genomes. Assemblers like Falcon-unzip may unintentionally include both haplotypes in the primary assembly, leading to allelic duplication and overestimation of genome size. The tool purge_haplotigs helps resolve this by identifying these redundant sequences (called haplotigs) and relocating them to a separate bin, improving the representation of the true haploid genome. In addition, Falcon-phase uses Hi-C data to properly phase the assembly by switching segments between the primary and alternate haplotypes to reflect accurate inheritance. These tools work together to reduce redundancy and improve the structural and biological correctness of genome assemblies. For more details, see:\nReferences:\n\nRoach MJ, Schmidt SA, Borneman AR. (2018). Purge Haplotigs: Synteny Reduction for Third-gen Diploid Genome Assemblies. https://doi.org/10.1186/s13059-018-1564-6\nGuan D et al. (2020). Identifying and removing haplotypic duplication in primary genome assemblies. Bioinformatics, 36(9), 2896–2898. https://doi.org/10.1093/bioinformatics/btaa025\n\n\n\n\nFALCON-Phase is a genome phasing tool designed to resolve diploid genomes by combining long-read sequencing data (e.g., from PacBio) with Hi-C chromatin contact data. While tools like FALCON-Unzip can separate haplotypes locally using SNP information from long reads, FALCON-Phase extends phasing over longer genomic distances by using Hi-C’s long-range linkage information. The process involves several steps: (1) FALCON-Unzip generates primary contigs and associated haplotigs; (2) haplotigs are aligned back to their positions on the primary contig; (3) the sequence is “minced” into phased blocks; (4) Hi-C reads are mapped to these blocks; (5) a phasing algorithm determines which haplotigs belong to each haplotype based on contact patterns; and (6) the tool emits two phased haplotigs—phase 0 and phase 1—representing the maternal and paternal chromosomes. This process improves the accuracy and completeness of haplotype-resolved assemblies, particularly in regions of high heterozygosity.\nReference: Kronenberg ZN et al. (2021). Extended haplotype-phasing of long-read de novo genome assemblies using Hi-C. https://www.nature.com/articles/s41467-020-20536-y\n\n\n\nFALCON-Phase\n\n\n\n\n\nGenome assembly quality control (QC) is a critical step in ensuring the accuracy, completeness, and usability of assembled genomes. One important metric is base accuracy, which can be assessed by realigning reads from the same species and identifying SNPs and indels—with indels often being more frequent in PacBio and Nanopore assemblies. Tools like KAT can evaluate k-mer completeness using Illumina data to estimate how much of the expected sequence content is present in the assembly. Local structural accuracy can be assessed using external evidence, such as read alignment patterns (via tools like REAPR or QUAST) or through known physical linkages like PCR-validated adjacencies. Another key QC aspect is gene content, which involves verifying the order and orientation of genes and exons, especially for well-characterized genes such as housekeeping genes.\n\n\n\nBUSCO (Benchmarking Universal Single-Copy Orthologs) is a widely used tool for evaluating the completeness of genome assemblies, transcriptomes, and gene annotations. It works by identifying a set of evolutionarily conserved genes that are expected to be present as single copies in nearly all species within a specific clade. BUSCO scans the input dataset to detect these orthologs and classifies them as Complete (C), Duplicated (D), Fragmented (F), or Missing (M). A high percentage of complete single-copy genes indicates a high-quality and complete assembly. The BUSCO pipeline combines tBLASTn, Augustus gene prediction, and HMMER3 profile searches to accurately locate and classify each ortholog, and can be run on genome assemblies, transcriptome data, or gene sets. It has become a standard benchmark for genome quality assessment in both de novo assembly and annotation pipelines.\nReference: Simão FA, Waterhouse RM, Ioannidis P, Kriventseva EV, Zdobnov EM. (2015). BUSCO: assessing genome assembly and annotation completeness with single-copy orthologs. Bioinformatics, 31(19), 3210–3212. https://doi.org/10.1093/bioinformatics/btv351\n\n\n\nMetagenomic assembly involves reconstructing genomic sequences from DNA extracted from complex environmental samples, typically containing multiple uncultured microbial species. Unlike traditional genome assembly, metagenomic assembly must address the challenge of mixed organisms with varying abundance levels, making it difficult to distinguish low-abundance genomes from sequencing errors or contaminants. Additionally, sequence divergence across species and strain-level variation adds complexity to assembly and binning. The typical output is a collection of contigs with associated coverage information, which provides a simplified yet informative representation of the community’s genetic content. However, assembling complete genomes is often difficult, and scaffolding approaches must be carefully selected or adapted to avoid chimeric assemblies. Some recent methods integrate abundance profiles, sequence composition, and differential coverage across samples to improve binning and contig linkage. New strategies are discussed by Ayling et al. (2019), who review tools and frameworks designed specifically for short-read metagenomic assembly and highlight the need for scalable, hybrid approaches.\n\nReference: Ayling M, Clark MD, Leggett RM. (2019). New approaches for metagenome assembly with short reads. Brief Bioinform. https://doi.org/10.1093/bib/bbz020\n\n\n\nTranscriptome assembly focuses on reconstructing expressed RNA sequences (cDNA) from a biological sample, rather than the entire genome. This form of assembly is complicated by the fact that, like in metagenomics, there is no single linear reference sequence—instead, it must handle a diverse and dynamic set of transcripts. The varying abundance of transcripts, along with alternative splicing, results in complex branching structures during assembly. Transcriptome assemblers must distinguish true isoforms from assembly errors and resolve exon skipping, intron retention, and other splicing events to accurately reconstruct full-length transcripts. De novo transcriptome assembly tools, such as Trinity, address this by using k-mer graphs to represent transcript variation. The resulting transcript contigs can then be quantified, annotated, and used for downstream analyses like differential expression, even in species lacking a reference genome.",
    "crumbs": [
      "Home",
      "Genome Assembly",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module2/module2_exercises.html",
    "href": "course_modules/Module2/module2_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Sequence alignment in NGS is the process of determining the most likely source of the observed DNA sequencing read within the reference genome sequence.\n\n\n\n\n\nThere are typical inferences you can make from an alignment of NGS data against a reference genome:\n• Variation from the reference – could have functional consequence.\n• Transcript abundance: Instead of a microarray, you could use alignment to genome to quantify expression: more sensitive\n• Ab-initio transcript discovery: you can see a pileup from RNA seq data showing evidence for an exon which was previously missed or an exon which is being skipped in a transcript.\n\n\n\n\n\n\n\nThis tutorial comprises the following sections:\n1. Performing read alignment\n2. Alignment visualisation\nThere is also an additional (optional) section: 3. Alignment workflows\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal on your computer and type the command below:\ncd /home/manager/course_data/read_alignment Now you can follow the instructions in the tutorial from here.\n\n\n\nThis tutorial assumes that you have samtools, bwa, Picard tools and IGV installed on your computer.\nThese are already installed on the VM you are using. To check that these are installed, you can run the following commands:\nsamtools --help\nbwa\npicard -h\nigv\nThis should return the help message for samtools, bwa and Picard tools respectively. The final command should launch the genome viewer IGV. You can close the IGV software, we will use it later in this tutorial to visualise alignments.\nTo get started with the tutorial, head to the first section: Performing read alignment",
    "crumbs": [
      "Home",
      "Read alignment",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module2/module2_exercises.html#read-alignment",
    "href": "course_modules/Module2/module2_exercises.html#read-alignment",
    "title": "Exercises",
    "section": "",
    "text": "Sequence alignment in NGS is the process of determining the most likely source of the observed DNA sequencing read within the reference genome sequence.\n\n\n\n\n\nThere are typical inferences you can make from an alignment of NGS data against a reference genome:\n• Variation from the reference – could have functional consequence.\n• Transcript abundance: Instead of a microarray, you could use alignment to genome to quantify expression: more sensitive\n• Ab-initio transcript discovery: you can see a pileup from RNA seq data showing evidence for an exon which was previously missed or an exon which is being skipped in a transcript.\n\n\n\n\n\n\n\nThis tutorial comprises the following sections:\n1. Performing read alignment\n2. Alignment visualisation\nThere is also an additional (optional) section: 3. Alignment workflows\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal on your computer and type the command below:\ncd /home/manager/course_data/read_alignment Now you can follow the instructions in the tutorial from here.\n\n\n\nThis tutorial assumes that you have samtools, bwa, Picard tools and IGV installed on your computer.\nThese are already installed on the VM you are using. To check that these are installed, you can run the following commands:\nsamtools --help\nbwa\npicard -h\nigv\nThis should return the help message for samtools, bwa and Picard tools respectively. The final command should launch the genome viewer IGV. You can close the IGV software, we will use it later in this tutorial to visualise alignments.\nTo get started with the tutorial, head to the first section: Performing read alignment",
    "crumbs": [
      "Home",
      "Read alignment",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module2/module2_exercises.html#performing-read-alignment",
    "href": "course_modules/Module2/module2_exercises.html#performing-read-alignment",
    "title": "Exercises",
    "section": "Performing Read Alignment",
    "text": "Performing Read Alignment\nHere we will use the BWA aligner to align a smll set of Illumina sequencing data to the Mus Musculus reference genome. We will align genomic sequence (from Whole-Genome Sequencing) from a mouse embryo which has been mutagenised while the one-cell stage using CRISPR-Cas9 and a gRNA targeting an exon of the Tyr gene. The successful mutation of the gene will delete one or both alleles. A bi-allelic null Tyr mouse will be albino, but otherwise healthy.\nFirst, check you are in the correct directory.\npwd\nIt should display something like:\n/home/manager/course_data/read_alignment\n\nViewing the reference genome\nGo to the ref directory that contains the fasta files of the reference genomes: cd ~/course_data/read_alignment/data/ref\nFasta files (.fa) are used to store raw sequencing information before aligning data. A single chromosome from the mouse genome is contained in the file GRCm38.68.dna.toplevel.chr7.fa.gz\nView the file with zless (we use zless instead of less because the file is compressed):\nzless GRCm38.68.dna.toplevel.chr7.fa.gz\nQ1: What is the length of chromosome 7 of the mouse genome? (Hint: Look at the fasta header for chromosome 7)\n................................................................................................\nSimilar to a BAM file, to allow fast retrieval of data, an index file is often required. You should check for the presence of fasta indexes for the genome in the ‘ref’ directory:\nGRCm38.68.dna.toplevel.chr7.fa.gz.amb … GRCm38.68.dna.toplevel.chr7.fa.gz.sa\nThese are created by BWA: suffixtrees, bwt transform etc.\nIf these index files don’t exist, then you can run the indexing with the command\nbwa index GRCm38.68.dna.toplevel.chr7.fa.gz\nBeware – this indexing process can take 3-5 minutes so please only run it if the index files do not exist!\n\n\nAlign the data with bwa\nGo to the ~/course_data/read_alignment/data/Exercise1/fastq/ directory - you can use this command:\ncd ../Exercise1/fastq\nUse the bwa mem command to align the fastq files to the mouse reference genome. By default bwa outputs SAM format directly to the standard output (in this case your terminal window), therefore you will have to redirect the result into a SAM file.\nbwa mem ../../ref/GRCm38.68.dna.toplevel.chr7.fa.gz md5638a_7_87000000_R1.fastq.gz md5638a_7_87000000_R2.fastq.gz &gt; md5638.sam\nThis may take a few minutes, please be patient.\n\n\nConvert a SAM file to a BAM file\nNow use samtools to convert the SAM file md5638.sam created in the previous step into a BAM file called md5638.bam.\nsamtools view -O BAM -o md5638.bam md5638.sam\nQ2: How much space is saved by using a BAM file instead of a SAM file?\n................................................................................................\n\n\nSort and index the BAM file\nThe BAM files produced by BWA are sorted by read name (same order as the original fastq files). However, most viewing and variant calling software require the BAM files to be sorted by reference coordinate position and indexed for rapid retrieval. Therefore, use ‘samtools sort’ to produce a new BAM file called md5638.sorted.bam that is sorted by position.\nsamtools sort -T temp -O bam -o md5638.sorted.bam md5638.bam\nFinally index the sorted BAM file using ‘samtools index’ command.\nNote: indexing a BAM file is also a good way to check that the BAM file has not been truncated (e.g. your disk becomes full when writing the BAM file). At the end of every BAM file, a special end of file (EOF) marker is written. The Samtools index command will first check for this and produce an error message if it is not found.\nsamtools index md5638.sorted.bam\n\n\nUnix pipes to combine the commands together\nTo produce the sorted BAM file above we had to carry out several separate commands and produce intermediate files. The Unix pipe command allows you to feed the output of one command into the next command.\nYou can combine all of these commands together using unix pipes, and do all of this data processing together and avoid writing intermediate files. To do this type:\nbwa mem ../../ref/GRCm38.68.dna.toplevel.chr7.fa.gz md5638a_7_87000000_R1.fastq..gz md5638a_7_87000000_R2.fastq.gz | samtools view -O BAM - | samtools sort -T temp -O bam -o md5638_2.sorted.bam -\nNow index the BAM file:\nsamtools index md5638_2.sorted.bam\nNote: When the symbol - is used above, Unix will automatically replace - with the output produced by the preceding command (i.e. the command before the | symbol).\n\n\nMark PCR Duplicates\nWe will use a program called ‘MarkDuplicates’ that is part of Picard tools (http://picard.source-forge.net) to remove PCR duplicates that may have been introduced during the library construction stage. To find the options for ‘MarkDuplicates’ – type:\npicard MarkDuplicates\nNow run MarkDuplicates using the ‘I=’ option to specify the input BAM file and the ‘O=’ option to specify the output file (e.g. md5638.markdup.bam). You will also need to specify the duplication metrics output file using ‘M=’ (e.g. md5638.markdup.metrics).\npicard MarkDuplicates I=md5638.sorted.bam O=md5638.markdup.bam M=md5638.metrics.txt\nQ3: From looking at the output metrics file - how many reads were marked as duplicates? What was the percent duplication?\n................................................................................................\nDon’t forget to generate an index for the new bam file using samtools.\nsamtools index md5638.markdup.bam\n\n\nGenerate QC Stats\nUse samtools to collect some statistics and generate QC plots from the alignment in the BAM file from the previous step. Make sure you save the output of the stats command to a file (e.g. md5638.markdup.stats).\nsamtools stats md5638.markdup.bam &gt; md5638.markdup.stats\nplot-bamstats -p md5638_plot/ md5638.markdup.stats\n\n\nExercises\nNow look at the output and answer the following questions:\nQ4: What is the total number of reads?\nQ5: What proportion of the reads were mapped?\nQ6: How many reads were paired correctly/properly?\nQ7: How many read pairs mapped to a different chromosome?\nQ8: What is the insert size mean and standard deviation?\nIn your web browser open the file called md5638_plot.html to view the QC information and answer the following questions:\nQ9: How many reads have zero mapping quality?\nQ10: Which of the first fragments or second fragments are higher base quality on average?\nCongratulations you have succesfully aligned some NGS data to a reference genome! Now continue to the next section of the tutorial: Alignment visualisation.",
    "crumbs": [
      "Home",
      "Read alignment",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module2/module2_exercises.html#alignment-visualisation",
    "href": "course_modules/Module2/module2_exercises.html#alignment-visualisation",
    "title": "Exercises",
    "section": "Alignment Visualisation",
    "text": "Alignment Visualisation\nYou have now made it to the interesting part!\nIntegrative Genome Viewer (IGV) http://www.broadinstitute.org/igv/ allows you to visualise genomic datasets and is a very useful tool for looking at the alignment of reads onto a reference genome from BAM files.\nStart IGV by typing:\nigv &\n\nIGV main window\nWhen you start IGV, it will open a main window. At the top of this window you have a toolbar and genome ruler for navigation. The largest area in the main window is the data viewer where your alignments, annotations and other data will be displayed. To do this, IGV uses horizontal rows called tracks. Finally, at the bottom, there is a sequence viewer which contains the base level information for your reference genome.\n\n\n\nLoad the reference genome\nIGV provides several genomes which can be selected with the “Genome drop-down box” on the toolbar.\nGo to ’ Genomes -&gt; Load Genome From Server… ’ and select “Mouse mm10”. This is a synonym for GRCm38, which is the current mouse assembly (reference genome)\n\n\n\n\nIGV toolbar and genome ruler\nOnce the genome has loaded, the chromosomes will be shown on the genome ruler with their names/numbers above. When a region is selected, a red box will appear. This represents the visible region of the genome.\nAbove the genome ruler is the toolbar which has a variety controls for navigating the genome:\n\nGenome drop-down - load a genome\nChromosome drop-down - zoom to a chromosome\nSearch - zoom to a chromosome, locus or gene\n\nThere are several other buttons which can be used to control the visible portion of the genome.\n\nWhole genome - zoom back out to whole genome view\nPrevious/next view - move backward/forward through views (like the back/forward buttons in a web browser)\nRefresh - refresh the display\nZoom - zooms in (+) / out (-) on a chromosome\n\n\n\n\nSequence viewer\nThe sequence viewer shows the genome at the single nucleotide level. You won’t be able to see the sequence until you are zomed in. Let’s try it, select the zooom in (+) option in the top right of the screeen. As you start to zoom in (+), you will see that each nucleotide is represented by a coloured bar (red=T, yellow=G, blue=C and green=A). This makes it easier to spot repetitive regions in the genome. Carry on zooming in (+) and you will see the individual nucleotides.\n\n\n\n3.3 Navigation in IGV\nThere are several views in IGV\n\nGenome view\nChromosome view\nRegion view\n\nThere are several ways to to zoom in and out to these views to look at specific regions or base level information.\n\n\nWhole genome view\nTo get a view of the entire genome select the zoom to whole genome icon (house icon) found in the toolbar at the top of the IGV window.\n\n\n\nChromosome view\nTo get a view of a specific chromosome select the chromosome from the chromosome drop down list in the toolbar of the IGV window.\n\n\n\nRegion view\nJump to region If you know the co-ordinates of the region you want to view, you can enter them into the “Search” and click “Go”. The format is chromosome:start-stop. For example, to view from 100,000 to 100,100 on chr7, you would enter chr7:100,000-100,100 in the search box. We will practice this later in an exercise.\nSelect region If you don’t know the specific co-ordinates of the region you want to look at, you can click and drag to select a region on the genome toolbar.\n\nNote: the visible region of the chromosome is indicated by the red box on the genome ruler.\n\n\nZooming in and out\nYou can zoom in and out from each view by using the “+” and “-” buttons on the zoom control at the right-hand side of the toolbar. This will also work with the “+” and “-” keys on your keyboard.\n\n\n\nLoad the alignment\nIGV can be used to visualise many different types of data, including read alignments. Each time you load an alignment file it will be added to the data viewer as a new major track.\nGo to ’ File -&gt; Load from File… ‘. Select the “md5638.markdup.bam” BAM file that you created in the previous section and click’ Open ’.\nNote: BAM files and their corresponding index files must be in the same directory for IGV to load them properly.\n\n\n\n\nVisualising alignments\nFor each read alignment, a major track will appear containing two minor tracks for that sample:\n\ncoverage information\nread alignments\n\nFor the total number of visible tracks, see the bottom left of main window.\nAt the genome level view, there will be no coverage plot or read alignments visible. At the chromosome level view, there are two messages displayed: Zoom in to see coverage/alignments. Finally, once you have zoomed in (+) you will see a density plot in the coverage track and your read alignments.\n\n\n\nCoverage information\nWhen zoomed in to view a region, you can get alignment information for each position in the genome by hovering over the coverage track. This will open a yellow box which tells you the total number of reads mapped at that position, a breakdown of the mapped nucleotide frequencies and the number of reads mapping in a forward/reverse orientation. In the example shown below, 95 reads mapped, 50 forward and 45 reverse, all of which called A at position 202,768 on chromosome PccAS_05_v3.\nThis is just an example for illustrative purposes, please do not try to look at this position in IGV here.\n\n\n\nViewing individual read alignment information\nRead are represented by grey or transparent/white bars which are stacked together where they align to the reference genome. Reads are pointed to indicate the orientation in which they mapped i.e. on the forward or reverse strand. Hovering over an individual read will display information about its alignment.\n(images/igv-read-information.png “IGV - read information”)\nMismatches occur where the nucleotide in the aligned read is not the same as the nucleotide in that position on the reference genome. A mismatch is indicated by a coloured bar at the relevant position on the read. The colour of the bar represents the mismatched base in the read (red=T, yellow=G, blue=C and green=A).\n\n\n\nIGV configuration\nFollow the instructions that follow to set up your IGV view:\nSelect the little yellow “speech bubble” icon in the toolbar and set the option to “Show Details on Click” (or you will go mad, I promise!).\n\nZoom in so you can see sequence reads and go to region chr7:87480000-87485000 using the navigation bar at the top.\n\nControl-click or right-click in the data view window. Choose sort alignments by insert size, then choose colour alignments by insert size and finally choose “View as pairs”.\nGo to ’ View -&gt; Preferences… ’ select the ’ Alignments ’ tab and ensure the “Show soft-clipped bases” option is ticked. This colour highlighting emphasises soft-clips on the read itself.\n\n\nYour IGV session should look similar to:\n\n\n\n3.7 Exercises\nGo to chromosome chr7, positions 87,483,625-87,484,330 using the navigation bar across the top. Take in the glorious view of a genome pileup. Stop and smell the roses! Click on stuff!\nScroll around, zoom in and out a bit!\n2. Go back to chromosome 7:87,483,625-87,484,330. What is the (rough) coverage across this region? (Hint: Look at the coverage track)\nCan you spot the three mutant variants (two small and one larger) in this region? State what the evidence is for them?\nHints\n• Hint1: Look around 87,483,960 for an insertion. How large is it? How many reads does it occur in?\n• Hint2: Look around 87,483,960 for a deletion. How large is it? How many reads does it occur in?\n• Hint3: Zoom out slightly and watch the coverage track between 87,483,700 - 87,484,200.\nOnce you’ve spotted the large change look at reference sequence the edges of the mutation to hazard a guess as to its mechanism.\n4. Can you venture a guess as to what happened here? Why are these mutations present?\nCongratulations you have completed the Read Alignment tutorial. If you have time left then continue to the next (optional) section of the tutorial: Alignment workflows.\nHere is an additional IGV tutorial and refresher: https://github.com/sanger-pathogens/pathogen-informatics-training/blob/master/Notebooks/IGV/IGV.pdf. You can find a copy of this tutorial in your manual. Unfortunately, there is not enough time to complete this tutorial now but you may find it useful to look at it after the course.",
    "crumbs": [
      "Home",
      "Read alignment",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module2/module2_exercises.html#ngs-workflows",
    "href": "course_modules/Module2/module2_exercises.html#ngs-workflows",
    "title": "Exercises",
    "section": "NGS Workflows",
    "text": "NGS Workflows\nA typical NGS experiment involves more than one sample, potential 10’s or 100’s of samples. During the experiment, a sample may be split across multiple libraries and and a library may be split across multiple sequencing runs (lanes). For example, you may have to increase the number of runs for a specific sample to increase the read-depth (sequencing volume), so you have to prepare multiple libraries.\nTherefore you need a coordinated workflow, driven by standard software to bring it reliably together.\nRead alignment is just the first part of that. Once you have a BAM file for each sequencing run you need to merge them together to produce a BAM file for the library. At this stage it is important to perform de-duplication on the merged data. The main purpose of removing duplicates is to mitigate the effects of PCR amplification bias introduced during library construction. PCR duplicates erroneously inflate the coverage and, if not removed, can give the illusion of high confidence when it is not really there which can have an effect on downstream analysis such as variant calling.\nThe figure below outlines a typical NGS workflow:\n In this part of the tutotial, we have two lanes of illumina sequencing data produced from a single library of yeast. We will use the BWA aligner to align the data to the Saccromyces cerevisiae genome (ftp://ftp.ensembl.org/pub/current_fasta/saccharomyces_cerevisiae/dna/) and produce a merged BAM file for the library.\nTo begin go to the following directory:\ncd /home/manager/course_data/read_alignment/data/Exercise2/60A_Sc_DBVPG6044/library1\n\nIndex the reference\nbwa index ../../../../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa.gz\n\n\nAlign the first sequencing run\nRecall that to align a lane of data to a reference genome we must perform the following steps:\n• Align the data\n• Convert from SAM to BAM\n• Sort the BAM file\n• Index the sorted BAM file\n\n\nFind the data\nGo to the directory that contains the data for the first sequencing run:\ncd lane1\n\n\nRun the alignment\nRemember from earlier in the tutorial that the Unix pipe command allows you to feed the output of one command into the next command. So using Unix pipes, we can combine all of the alignment steps together into one command and do all of this data processing together and avoid writingintermediate files. To do this type the command:\nbwa mem -M -R '@RG\\tID:lane1\\tSM:60A_Sc_DBVPG6044' ../../../../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa.gz s_7_1.fastq.gz s_7_2. fastq.gz | samtools view -bS - | samtools sort -T temp -O bam -o lane1.sorted.bam -\nQ1: What do the -M and -R options do?\nQ2: What does the -bS option do?\nNow index the BAM file:\nsamtools index lane1.sorted.bam\n\n\nGenerate QC stats\nNow use samtools to collect some statistics and generate QC plots from the alignment in the BAM file. Type the commands:\nsamtools stats lane1.sorted.bam &gt; lane1.stats.txt\nplot-bamstats -p plot/ lane1.stats.txt\nNow look at the output and answer the following questions:\nQ3: What is the total number of reads?\nQ4: What proportion of the reads were mapped?\nQ5: How many reads were paired correctly/properly?\nQ6: How many reads mapped to a different chromosome?\nQ7: What is the insert size mean and standard deviation?\nIn a web browser open the file called plots.html to view the QC information.\nQ8: How many reads have zero mapping quality?\nQ9: Which of the first fragments or second fragments are higher base quality on average?\n\n\nAlign the second sequencing run\nThere is a second lane of sequencing data in the library1 directory contained in the directory lane2. We want to also align this sequncing data and produce a BAM file.\nGo to the directory that contains the data for the second sequencing run:\ncd ../lane2\nNow align the data in this directory to the yeast reference genome and produce a sorted BAM file.\nNote: This time when you use the bwa mem command use the following header option to specify lane2 as the read group ID:\n@RG\\tID:lane2\\tSM:60A_Sc_DBVPG6044\nQ10: What is the size of the BAM file that is produced?\n\n\nMerge the BAM files\nGo to the directory that contains the data for the library 60A_Sc_DBVPG6044/library1 . Use ls to get a listing of the files and directories contained in this directory.\ncd ..\npwd\nls\nYou will notice that there are two directories called lane1 and lane2. There were two sequencing lanes produced from this sequencing library. In order to mark library PCR duplicates, we need to merge the two lane BAM files together to produce a single BAM file. We will use the picard tool called ‘MergeSamFiles’ (http://picard.sourceforge.net) to merge the lane BAM files.\nTo find the options for ‘MergeSamFiles’ command, type:\npicard MergeSamFiles\nNow use the I= option to specify both the input BAM files and the O= option to specify the outputfile (e.g. library1.bam). Note: Multiple input files can be specified using multiple I= options\n\n\nMark PCR duplicates\nWe will use a program called ‘MarkDuplicates’ that is part of Picard tools (http://picard.source-forge.net) to remove PCR duplicates that may have been introduced during the library construction stage. To find the options for ‘MarkDuplicates’ type:\npicard MarkDuplicates\nNow use the I= option to specify the input BAM file and the O= option to specify the output file (e.g. library1.markdup.bam). You will also need to specify the duplication metrics output file using\nM= (e.g. library1.markdup.metrics).\n**Don’t forget to index your final bam file using samtools index.\nQ11: From looking at the output metrics file - how many reads were marked as duplicates?\nQ12: What was the percent duplication?\n\n\nVisualise the alignment\nGo to the directory containing the reference genome and uncompress the file as IGV cannot read a compressed file.\ncd /home/manager/course_data/read_alignment/data/ref\ngunzip Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa.gz\nStart IGV by typing:\nigv &\n\n\nLoad the reference genome\nOn the top menu bar go to ’ Genomes –&gt; Load Genome From File… ‘, go to the “ref” directory and select the file “Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa” and click’ Open ’\n\n\n\n\nLoad the alignment\nTo load the merged BAM file, on the top menu bar go to ’File –&gt; Load from File…’ and select the library BAM file that you created in the previous step.\n\n\n\n\nExercises\n1. Go to Chromosome IV and position 764,292. (Hint: use the navigation bar across the top)\n2. What is the reference base at this position?\n3. Do the reads agree with the reference base?\n4. What about the adjacent position (IV:764,293)? What is the reference base at this position? Is it supported by the reads?\n5. Go to Chromosome IV and position 766,589.\n6. What sort of mutation are the alignments indicating might be present?\n7. Go to Chromosome IV and position 770,137 using the navigation bar across the top.\n8. What sort of mutation are the alignments indicating might be present? Is there anything in the flanking sequence of the reference genome that might make you suspicious about this mutation?\n9. Convert the BAM file to a CRAM file\nYou have reached the end of the Read Alignment tutorial.",
    "crumbs": [
      "Home",
      "Read alignment",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module2/module2_solutions.html",
    "href": "course_modules/Module2/module2_solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "Read Alignment\nThere are no questions in this section.\n\n\nPerforming Read Alignment\n1. 145441459\n2. The sam is ~ 157M and the bam is ~ 25M\n3. 67,461 (17.2%) or 359 + (33551 * 2)\n4. 392,820\n5. 391603/392820 (99.7%)\n6. 389410\n7. 0\n8. 419 (mean) 113.9 (standard deviation)\n9. 7,853 (2.0%)\n10. First/Forward read\n\n\nAlignment Visualisation\n1. This exercise is just asking you to explore the genome and become familiar with navigating in IGV.\n2. 23X-57X\n3. There is a 1bp insertion (at “T)” at position 87,483,966. This is supported by 9 reads.\nThere is a 28bp deletion at position 87,483,966. This is supported by 3 reads.\nThe third mutation is a bit harder to spot, because it’s bigger than the read length (so no single read will span it). Look first at the coverage track, and notice a sharp coverage drop between chr7:87,483,833 and chr7:87,484,169. That suggests that one of the two alleles have been deleted across that position. Notice also the soft-clipping of some reads “entering” chr7:87,483,833 from the left, and the same softclipping of reads entering chr7:87,484,169 from the right. This soft-clipping shows up as reads being partly multi-coloured. That’s happening because the physical genome between those points has been excised for one allele, causing the mis-alignment when we attempt to align some reads to the reference genome. That mis-alignment causes the bwa aligner to “give up” and mark a part of the read as soft-clipped.\n4. This mouse was bred from a zygote which was mutagenised with Crispr-Cas9, targeted at the Tyr locus. You are watching the zygote DNA-repair machinery panicking and grabbing at straws when trying to repair double-stranded DNA breaks. In the process, it makes mistakes, and those mistakes are propagated into the mouse genome: different zygote cells received different mutations, which is why some reads reflect different mutations to others. Specifically - The CRISPR-Cas9 has acted on the zygote at this locus to create Non-Homologous-End-Join-based damage around 87,483,960: that resulted in a subclonal 1bp insertion and a 28bp deletion. Also, a related DNA repair process has resulted in the a 336bp deletion across the same area.\n\n\nAlignment workflows\n1. -M marks shorter split hits as secondary and -R adds the read group to the header of the BAM file\n2. -b means create a BAM as output and -S indicates that the input files is a SAM file. The -S option is now ignored by samtools as it can now autodetect the input file type.\n3. 397506\n4. 303036/397506 (76.2%)\n5. 282478\n6. 2239\n7. 275.9 (mean) and 47.7 (standard deviation)\n8. 23,789 (7.9%)\n9. First\n10.\nbwa mem -M -R ”@RG\\tID:lane2\\tSM:60A_Sc_DBVPG6044” ../../../../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa.gz s_7_1.fastq.gz s_7_2.fastq.gz | samtools view -bS - | samtools sort -T temp -O bam -o lane1.sorted.bam -\n~22M\nMerge: ~/course_data/read_alignment/data/Exercise2/60A_Sc_DBVPG6044/library1$ picard\nMergeSamFiles -I lane1/lane1.sorted.bam -I lane2/lane1.sorted.bam -O library1.bam\nMarkdup: ~/course_data/read_alignment/data/Exercise2/60A_Sc_DBVPG6044/library1$ picard\nMarkDuplicates -I library1.bam -O library1.markdup.bam -M library1.metrics.txt\n11. 12399 or 3115 + (4642 * 2) = unpaired read dups + (paired read dups *2)\n12. 2.5%\n\n\nExercises\n1. No answer needed\n2. The reference base is C\n3. No (the reads call T)\n4. The reference base is G and all reads agree\n5. No answer\n6. An insertion\n7. No answer\n8. A deletion. This is unlikely to be a true variant and may be due to misalignment due the run of T’s in the flanking region.\n9. The following command procuces a cram file which should be ~29MB in size.\nsamtools view -C -T ../../../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa -o library1.markdup.cram library1.markdup.bam\n-C means create a CRAM file as output\n-T is the reference file to use for the compression\n-o is the name of CRAM file to create",
    "crumbs": [
      "Home",
      "Read alignment",
      "Solutions"
    ]
  },
  {
    "objectID": "course_modules/Module2/module2_instructor_notes.html",
    "href": "course_modules/Module2/module2_instructor_notes.html",
    "title": "Instructor notes",
    "section": "",
    "text": "If learners haven’t used IGV before, spend a few minutes showing how to load a BAM and navigate to a gene locus.\nMany think a higher alignment rate always means better data – remind them that quality filtering and reference completeness also play roles.\nRunning the alignment on the sample data should take ~5 minutes; ensure everyone has the reference indexed beforehand to save time.\nTrainer Tips:\nConnect Theory to Practice: Start with a high-level example – e.g., show how a short read aligns to a reference by drawing it on the board, then relate that to what BWA-MEM does at scale. This helps demystify the aligner’s “black box.”\nAddress Common Errors: Warn about pitfalls like forgetting to index the reference (a very common mistake that causes BWA to error). Also, if learners will run commands, remind them about file paths and available disk space (alignment can generate large files).\nUse Analogies: When explaining the seed-and-extend algorithm or hash tables, analogies can help. For instance, “finding a seed match is like finding the first few letters of a word in a big book index, then checking the surrounding text for a full match.”\nTime Management: Aligners on large data can take a long time; ensure any live runs use small data. Test the exercise beforehand and maybe have an output ready in case running it live fails or is slow. Also, if short on time, the actual running can be skipped and results provided for interpretation instead – the key is understanding the process and output, not just executing commands.\nAsk the students questions (“Why do you think long reads have a higher error rate? How might that affect alignment?”) to keep learners engaged and check comprehension.",
    "crumbs": [
      "Home",
      "Read alignment",
      "Instructor notes"
    ]
  },
  {
    "objectID": "course_modules/VM_guide/Software_list.html",
    "href": "course_modules/VM_guide/Software_list.html",
    "title": "Software list",
    "section": "",
    "text": "Software used during the course\n\n\n\nSoftware\nVersion (if not latest)\nModule\nNotes\n\n\n\n\nbreakdancer\n1.4.5\nngsbio\n\n\n\nsamtools\n1.15.1\nngsbio\n\n\n\nbcftools\n1.15.1\nngsbio\n\n\n\nbedtools\n2.30.0\nngsbio\n\n\n\nopenmpi\n4.1.4\nngsbio\n\n\n\nr-base\n4.0.5\nngsbio\n\n\n\nbowtie2\n2.4.5\nngsbio\n\n\n\nmacs2\n2.2.7.1\nngsbio\n\n\n\nmeme\n5.4.1\nngsbio\n\n\n\nucsc-bedgraphtobigwig\n377\nngsbio\n\n\n\nucsc-fetchchromsizes\n377\nngsbio\n\n\n\nr-sleuth\n0.30.0\nngsbio\n\n\n\nbioconductor-rhdf5\n2.34.0\nngsbio\n\n\n\nbioconductor-rhdf5filters\n1.2.0\nngsbio\n\n\n\nbioconductor-rhdf5lib\n1.12.0\nngsbio\n\n\n\nhdf5\n1.10.5\nngsbio\n\n\n\nhisat2\n2.2.1\nngsbio\n\n\n\nkallisto\n0.46.2\nngsbio\n\n\n\nbwa\n0.7.17\nngsbio\n\n\n\nassembly-stats\n1.0.1\nngsbio\n\n\n\ncanu\n2.2\nngsbio\n\n\n\nkmer-jellyfish\n2.3.0\nngsbio\n\n\n\nseqtk\n1.3\nngsbio\n\n\n\nvelvet\n1.2.10\nngsbio\n\n\n\nwtdbg\n2.5\nngsbio\n\n\n\ngenomescope2\n2\nngsbio\n\n\n\nfreebayes\n0.9.21.7\nngsbio\n\n\n\ngatk4\n4.2.6.1\nngsbio\n\n\n\npicard-slim\n2.27.4\nngsbio\n\n\n\nminimap2\n2.24\nngsbio\n\n\n\nsniffles\n2.0.7\nngsbio\n\n\n\npytz\n\nngsbio (Python Dependency)\n\n\n\nedlib\n\nngsbio (Python Dependency)\n\n\n\nthreadpoolctl\n\nngsbio (Python Dependency)\n\n\n\nsix\n\nngsbio (Python Dependency)\n\n\n\nscipy\n\nngsbio (Python Dependency)\n\n\n\nnetworkx\n\nngsbio (Python Dependency)\n\n\n\njoblib\n\nngsbio (Python Dependency)\n\n\n\ncython\n\nngsbio (Python Dependency)\n\n\n\nclick\n\nngsbio (Python Dependency)\n\n\n\nscikit-learn\n\nngsbio (Python Dependency)\n\n\n\npython-dateutil\n\nngsbio (Python Dependency)\n\n\n\npandas\n\nngsbio (Python Dependency)\n\n\n\nlightgbm\n\nngsbio (Python Dependency)\n\n\n\nsortedcontainers\n\nngsbio (Python Dependency)\n\n\n\ndysgu\n\nngsbio (Pip Installation)\n\n\n\nr-ngsplot\n\nchipseq-project\n\n\n\npython\n2.7\nchipseq-project\n\n\n\njupyter\n1.0.0\njupyter\n\n\n\npandoc\n2.12\njupyter\n\n\n\nbash_kernel\n\njupyter (Pip Installation)\n\n\n\ntexlive-base\n\njupyter (System Package)\n\n\n\ntexlive-xetex\n\njupyter (System Package)\n\n\n\ntexlive-formats-extra\n\njupyter (System Package)\n\n\n\ntexlive-fonts-extra\n\njupyter (System Package)\n\n\n\ntexlive-luatex\n\njupyter (System Package)\n\n\n\nIGV\n2.14.1\nStandalone\n\n\n\n\n\n\nIn this course run we are using Oracle VM Virtual Box (https://www.virtualbox.org/) to deliver Informatics, you can find Virtual Box Guides below.\nThe Host OS Requirements for Virtual Box:\n\nRAM requirement: 8GB (preferably 12GB)\nProcessor requirement: 4 processors (preferably 8)\nHard disk space: 200GB\nAdmin rights to the computer",
    "crumbs": [
      "Home",
      "How to setup a VM",
      "Software list"
    ]
  },
  {
    "objectID": "course_modules/VM_guide/Software_list.html#informatics-set-up",
    "href": "course_modules/VM_guide/Software_list.html#informatics-set-up",
    "title": "Software list",
    "section": "",
    "text": "In this course run we are using Oracle VM Virtual Box (https://www.virtualbox.org/) to deliver Informatics, you can find Virtual Box Guides below.\nThe Host OS Requirements for Virtual Box:\n\nRAM requirement: 8GB (preferably 12GB)\nProcessor requirement: 4 processors (preferably 8)\nHard disk space: 200GB\nAdmin rights to the computer",
    "crumbs": [
      "Home",
      "How to setup a VM",
      "Software list"
    ]
  },
  {
    "objectID": "course_modules/Module3/module3_assessment.html",
    "href": "course_modules/Module3/module3_assessment.html",
    "title": "Quiz",
    "section": "",
    "text": "Q1\nWhat is the main difference between germline mutations and somatic mutations?\nA. Germline mutations occur in non-reproductive cells, while somatic mutations occur in reproductive cells.\nB. Germline mutations are heritable as they occur in egg or sperm cells, while somatic mutations occur in non-germline tissues and are not passed to offspring.\nC. Germline mutations only occur in bacteria, while somatic mutations are found only in humans.\nD. Both types of mutations are heritable but affect different chromosomes.\n\n\nQ2\nWhich of the following is an example of a large-scale genomic variation?\nA. A point mutation in a single base.\nB. A duplication involving 2 kilobases of DNA (CNV).\nC. A small insertion or deletion of 10 base pairs.\nD. A single nucleotide polymorphism (SNP).\n\n\nQ3\nWhich statement best describes the difference between single nucleotide variants (SNVs) and indels?\nA. SNVs involve changes in multiple adjacent bases; indels involve changes in just one base.\nB. SNVs are large deletions; indels are large insertions.\nC. SNVs involve single base substitutions, while indels are small insertions or deletions typically less than 50 base pairs.\nD. SNVs occur only in coding regions; indels occur only in non-coding regions.\n\n\nQ4\nWhich of the following is NOT a practical application of variant calling?\nA. Cataloging biological diversity in population genetics.\nB. Identifying pathogenic mutations for disease diagnosis.\nC. Determining the speed of DNA replication during cell division.\nD. Tailoring drug dosages in pharmacogenomics.\n\n\nQ5\nWhat is the benefit of resolving phased haplotypes in variant calling analyses?\nA. It provides information about the total number of chromosomes in the organism.\nB. It determines which variants are inherited together on the same chromosome, thereby clarifying genotype information.\nC. It helps to identify only somatic mutations exclusively.\nD. It replaces the need for any sequencing data.\n\n\nQ6\nWhich key field is NOT typically included in a Variant Call Format (VCF) file?\nA. CHROM (chromosome)\nB. POS (position)\nC. TEMP (temperature of the sample)\nD. ALT (alternate allele)\n\n\nQ7\nWhich sequence correctly describes the process from raw sequencing data to final variant calls?\nA. Alignment, Quality Control, Generate Sequencing Data, Pileup, Variant Calling.\nB. Generate Sequencing Data, Alignment, Quality Control, Variant Calling, Pileup.\nC. Generate Sequencing Data, Quality Control, Alignment, Pileup, Variant Calling.\nD. Pileup, Generate Sequencing Data, Quality Control, Alignment, Variant Calling.\n\n\nQ8\nWhich of the following is a common source of error in variant calling?\nA. Uniform base composition across the genome.\nB. Homopolymers and repetitive regions causing sequencing and mapping errors.\nC. Excessively high mapping quality in all reads.\nD. Infrequent occurrence of strand bias in high-quality datasets.\n\n\nQ9\nThe transition/transversion (Ts/Tv) ratio is used as a quality metric in SNP datasets. What Ts/Tv ratio is typically expected in high-quality human SNP calls?\nA. Approximately 0.5–1\nB. Approximately 1–2\nC. Approximately 2–3\nD. Approximately 4–5\n\n\nQ10\nHow do population-level datasets like those from the 1000 Genomes Project and gnomAD aid researchers?\nA. They provide a comprehensive snapshot of human genetic diversity and variant frequencies across various populations.\nB. They measure the metabolic rates of individuals in diverse populations.\nC. They only include data from European populations.\nD. They offer real-time monitoring of gene expression levels."
  },
  {
    "objectID": "course_modules/Module3/module3_manual.html",
    "href": "course_modules/Module3/module3_manual.html",
    "title": "Manual",
    "section": "",
    "text": "Mutations refer to any alteration in the DNA base sequence and can be broadly classified into germline and somatic mutations.",
    "crumbs": [
      "Home",
      "Variant calling",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module3/module3_manual.html#human-population-specific-variation",
    "href": "course_modules/Module3/module3_manual.html#human-population-specific-variation",
    "title": "Manual",
    "section": "Human Population Specific Variation",
    "text": "Human Population Specific Variation\n\n1000 Genomes Project\n(http://www.internationalgenome.org/):\n\nThis global collaboration catalogs human genetic variation across diverse populations. It provides open-access data on common and rare variants, supporting studies in population genetics, ancestry, and disease. The project has greatly advanced our understanding of population-specific variation by sequencing individuals from over 25 populations.\n\n\ngnomAD\n(https://gnomad.broadinstitute.org/):\n\nThe Genome Aggregation Database (gnomAD) aggregates and harmonizes exome and genome sequencing data from over 140,000 individuals. It provides allele frequencies across populations, helping researchers filter out common variants in rare disease studies and identify population-specific variation, making it a crucial reference for clinical and research genomics.\n\n\nEnsembl Variation\n(https://grch37.ensembl.org/info/genome/variation/index.html):\n\nEnsembl Variation provides access to annotated genetic variants across species, with a focus on human data. It integrates information from projects like dbSNP and 1000 Genomes, offering functional annotations, allele frequencies, and phenotype associations, helping researchers explore the genomic context and population distribution of variants.\n\n\nReferences and Additional Resources\nOlson, N.D., et al. (2023) Nature Reviews Genetics 24:464–483.\nThe 1000 Genomes Project Consortium. (2015) Nature 526:68–74.\nCann, H.M., et al. (2002) Science 296:261–262. (HGDP-CEPH)\nEnsembl Variation documentation: https://www.ensembl.org/info/docs/variation/index.html\nBCFtools: https://github.com/samtools/bcftools\nVEP (Variant Effect Predictor): https://github.com/willmclaren/ensembl-vep\ngnomAD: https://gnomad.broadinstitute.org/about\nFor course-related inquiries, contact: qasim.ayub@monash.edu",
    "crumbs": [
      "Home",
      "Variant calling",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module3/module3_exercises.html",
    "href": "course_modules/Module3/module3_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "1 Variant Calling\n\n\n1.1 Introduction\nVariant calling is the process of identifying differences between the reference genome and the samples that have been sequenced. These differences can be single nucleotide polymorphisms (SNPs), multi-nucleotide polymorphisms (MNPs) or small insertions and deletions (indels) and examples of each of these are shown below.\n\nFigure 1: SNPs and small insertions and deletions\n\n\n1.2 Learning outcomes\nOn completion of the tutorial, you can expect to be able to:\n• Perform variant calling (SNPs and indels) using standard tools\n• Assess the quality/confidence of a variant call\n• Filter variant calls to remove low quality/confidence calls\n• Perform variant calling across multiple samples\n• Visualise variants using standard tools\n• Annotate variants with consequence calls\n\n\n1.3 Tutorial sections\nThis tutorial comprises the following sections:\n\nPerforming variant calling\nFiltering variants\nMulti-sample variant calling\nVisualising variants\n\n\n\n1.4 Authors\nThis tutorial was written by Jacqui Keane based on material from Thomas Keane and Petr Danecek.\nThere is also an additional (optional) section: 5. Variant annotation\n\n\n1.5 Running the commands from this tutorial\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal on your computer and type the command below:\ncd ~/course_data/variant_calling/data\nNow you can follow the instructions in the tutorial from here.\n\n\n1.6 Let’s get started!\nThis tutorial assumes that you have samtools, bcftools and IGV installed on your computer. These are already installed on the VM you are using. To check that these are installed, you can run the following commands:\nsamtools --help\nbcftools --help\nigv\nThis should return the help message for samtools and bcftools. The final command should launch the genome viewer IGV. You can close the IGV software, we will use it later in this tutorial to visualise variants.\nTo get started with the tutorial, go to to the first section: Performing variant calling\n\n\n2 Performing Variant Calling\nWhen performing variant calling we need the aligned sequences in SAM, BAM or CRAM format and the reference genome that we want to call varants against.\nFirst, check you are in the correct directory.\npwd\nIt should display something like: /home/manager/course_data/variant_calling/data\n\n\n2.1 Assessing the input data\nTo list the files in the current directory, type\nls -lh\nThe listing shows aligned data for two mouse strains A/J and NZO (A_J.bam and NZO.bam) and the chromosome 19 of the mouse reference genome (GRCm38_68.19.fa).\nBefore performing variant calling, it is important to check the quality of the data that you will be working with. We have already seen how to do this in the QC and Data Formats and Read Alignment sessions. The commands would look like:\nsamtools stats -r GRCm38_68.19.fa A_J.bam &gt; A_J.stats\nsamtools stats -r GRCm38_68.19.fa NZO.bam &gt; NZO.stats\nplot-bamstats -r GRCm38_68.19.fa.gc -p A_J.graphs/ A_J.stats\nplot-bamstats -r GRCm38_68.19.fa.gc -p NZO.graphs/ NZO.stats\nYou do not need to run these QC checks on this data and for this we will assume that QC has already been performed and the data is of good quality.\n\n\n2.2 Generating pileup\nThe command samtools mpileup prints the read bases that align to each position in the reference genome. Type the command:\nsamtools mpileup -f GRCm38_68.19.fa A_J.bam | less -S\nEach line corresponds to a position on the genome.\nThe columns are: chromosome, position, reference base, read depth, read bases (dot . and comma , indicate match on the forward and on the reverse strand; ACGTN and acgtn a mismatch on the forward and the reverse strand) and the final column is the base qualities encoded into characters.\nThe caret symbol ^ marks the start of a read, the dollar sign $ the end of a read, deleted bases are represented by asterisk *.\n\n\n2.3 Generating genotype likelihoods and calling variants\nThis output can be used for a simple consensus calling. One rarely needs this type of output. Instead, for a more sophisticated variant calling method, see the next section.\n\n\n2.2.1 Exercises\nLook at the output from the mpileup command above and answer the following questions.\nQ1: What is the read depth at position 10001994? (Rather than scrolling to the position, use the substring searching capabilities of less: press /, then type 10001994 followed by enter to find the position.)\nQ2: What is the reference allele and the alternate allele at position 10001994?\nQ3: How many reads call the reference allele at position 10001994 and how many reads call the alternate allele at position 10001994?\n\n\n2.3 Generating genotype likelihoods and calling variants\nThe bcftools mpileup command can be used to generate genotype likelihoods. (Beware: the command mpileup is present in both samtools and bcftools, but in both they do different things. While samtools mpileup produces the text pileup output seen in the previous exercise, bcftools mpileup generates a VCF file with genotype likelihoods.)\nRun the following command (when done, press q to quit the viewing mode):\nbcftools mpileup -f GRCm38_68.19.fa A_J.bam | less -S\nThis generates an intermediate output which contains genotype likelihoods and other raw information necessary for variant calling. This output is usually streamed directly to the caller like this\nbcftools mpileup -f GRCm38_68.19.fa A_J.bam | bcftools call -m | less -S\nThe output above contains both variant and non-variant positions. Check the input/output options section of the bcftools call usage page and see if there is an option to print out only variant sites.\nThen construct a command to print out variant sites only:\nThe INFO and FORMAT fields of each entry tells us something about the data at the position in the genome. It consists of a set of key-value pairs with the tags being explained in the header of the VCF file (see the ##INFO and ##FORMAT lines in the header).\nWe can tell mpileup to add additional ##INFO and ##FORMAT information to the output. For example, we can ask it to add the FORMAT/AD tag which informs about the number of high-quality reads that support alleles listed in REF and ALT columns. The list of all available tags can be printed with the command:\nbcftools mpileup -a ?\nNow let’s run the variant calling again, this time adding the -a AD option. We will also add the -Ou option so that it streams a binary uncompressed BCF into call. This is to avoid the unnecessary CPU overhead of formatting the internal binary format to plain text VCF only to be immediately formatted back to the internal binary format again.\nbcftools mpileup -a AD -f GRCm38_68.19.fa A_J.bam -Ou | bcftools call -mv -o out.vcf\n\n\n2.4 Exercises\nLook at the content of the VCF file produced above and answers the questions that follow.\nless -S out.vcf\nQ1: What is the reference allele and the alternate allele at position 10001994?\nQ2: What is the total raw read depth at position 10001994?\nNote: This number may be different from the values we obtained earlier, because some low quality reads or bases might have been filtered previously.\nQ3: What is the number of high-quality reads supporting the SNP call at position 10001994? How many reads support the reference allele and how many support the alternate allele?\nHint: Look up the AD tag in the FORMAT column: the first value gives the number of reads calling the reference allelle and the second gives the number of reads calling the alternate alleles.\nQ4: What sort of event is happening at position 10003649?\nCongratulations, you have sucessfully called variants from some NGS data. Now continue to the next section of the tutorial: filtering variants.\n\n\n3 Variant Filtering\nIn the next series of commands we will learn how to extract information from VCFs and how to filter the raw calls. We will use the bcftools commands again. Most of the commands accept the -i, – include and -e, –exclude options https://samtools.github.io/bcftools/bcftools.html# expressions which will be useful when filtering using fixed thresholds. We will estimate the quality of the callset by calculating the ratio of transitions and transversions https://en.wikipedia.org/wiki/Transversion.\nWhen drafting commands, it is best to build them gradually. This prevents errors and allows you to verify that they work as expected. Let’s start with printing a simple list of positions from the VCF using the bcftools query command https://samtools.github.io/bcftools/bcftools.html#query and pipe through the head command to limit the printed output to the first few lines:\nbcftools query --format 'POS=%POS\\n' out.vcf | head\nAs you can see, the command expanded the formatting expression POS=%POSin the following way: for each VCF record the string POS= was copied verbatim, the string %POS was replaced by the VCF coordinate stored in the POS column, and then the newline character ended each line. (Without the newline character, positions from the entire VCF would be printed on a single line.)\nNow add the reference and the alternate allele to the output. They are stored in the REF and ALT column in the VCF, and let’s separate them by a comma:\nbcftools query -f'%POS %REF,%ALT\\n' out.vcf | head\nIn the next step add the quality (%QUAL), genotype (%GT) and sequencing depth (%AD) to the output. Note that FORMAT tags must be enclosed within square brackets […] to iterate over all samples in the VCF. (Check the Extracting per-sample tags section in the manual https://samtools.github.io/bcftools/howtos/query.html for a more detailed explanation why the square brackets are needed.)\nbcftools query -f'%POS %QUAL [%GT %AD] %REF %ALT\\n' out.vcf | head\nNow we are able to quickly extract important information from the VCFs. Now let’s filter rows with QUAL smaller than 30 by adding the filtering expression –exclude ‘QUAL&lt;30’ or –include ‘QUAL&gt;=30’ like this:\nbcftools query -f'%POS %QUAL [%GT %AD] %REF %ALT\\n' -i'QUAL&gt;=30' out.vcf | head\nNow compare the result with the output from the previous command, were the low-quality lines removed?\nIn the next step limit the output to SNPs and ignore indels by adding the type=“snp” condition to the filtering expression. Because both conditions must be valid at the same time, we request the AND logic using the && operator:\nbcftools query -f'%POS %QUAL [%GT %AD] %REF %ALT\\n' -i'QUAL&gt;=30 && type=\"snp\"'out.vcf | head\n\n\n3.1 Exercises\nQ1: Can you print SNPs with QUAL bigger than 30 and require at least 25 alternate reads in the AD tag?\nRemember, the first value of the AD tag is the number of reference reads, the second is the number of alternate reads, therefore you will need to query the second value of the AD tag. The first value can be queried as AD[0] and the second as AD[1] (the allele indexes are zero-based). In case of FORMAT fields, also the queried sample must be selected as AD[sample:subfield] . Therefore add to the expression the condition AD[0:1] &gt;= 25 to select the first (and in our case the only one) sample or AD[*:1] &gt;= 25 to select any sample for which the condition is valid.\nNow we can filter our callset. In order to evaluate the quality, we will use bcftools stats to calculate the ratio of transitions vs transversions. We start by checking what is the ts/tv of the raw unfiltered callset. The stats command produces a text output, we extract the field of interest as follows:\nbcftools stats out.vcf | less\nbcftools stats out.vcf | grep TSTV\nbcftools stats out.vcf | grep TSTV | cut -f5\nQ2: Calculate ts/tv of the set filtered as above by adding -i ‘QUAL&gt;=30 && AD[*:1]&gt;=25’ to the bcftools stats command.\n(Here the asterisk followed by a colon tells the program to apply the filtering to all samples. At least one sample must pass in order for a site to pass.) After applying the filter, you should observe an increased ts/tv value.\nQ3: Can you do the reverse and find out the ts/tv of the removed sites? Use the -e option instead of -i. The ts/tv of the removed low-quality sites should be lower.\nQ4: The test data come from an inbred homozygous mouse, therefore any heterozygous genotypes are most likely mapping and alignment artefacts. Can you find out what is the ts/tv of the heterozyous SNPs? Do you expect higher or lower ts/tv? Use the filtering expression -i ‘GT=“het”’ to select sites with heterozygous genotypes.\nAnother useful command is bcftools filter which allows to “soft filter” the VCF: instead of removing sites, it can annotate the FILTER column to indicate sites which fail. Apply the above filters (‘QUAL&gt;=30 && AD[*:1]&gt;=25’) to produce a final callset, adding also the –SnpGap and the –IndelGap option to filter variants in close proximity to indels:\nbcftools filter -s LowQual -i'QUAL&gt;=30 && AD[*:1]&gt;=25' -g8 -G10 out.vcf -o out.flt.vcf\n\n\n3.2 Variant normalization\nThe same indel variant can be represented in different ways. For example, consider the following 2bp deletion. Although the resulting sequence does not change, the deletion can be placed at two different positions within the short repeat:\n12345\nTTCTC\nPOS=1 T–TC\nPOS=3 TTC–\nIn order to be able to compare indels between two datasets, we left-align such variants.\nQ5: Use the bcftools norm command to normalize the filtered callset. Note that you will need to provide the –fasta-ref option. Check in the output how many indels were realigned.\nNow continue to the next section of the tutorial: Multi-sample variant calling\n4 Calling Variants Across Multiple Samples\nIn many types of experiments we sequence multiple samples and compare their genetic variation across samples. The single-sample variant calling we have done so far has the disadvantage of not providing information about reference genotypes. Because only variant sites are stored, we are not able to distinguish between records missing due to reference genotypes versus records missing due to lack of coverage.\nIn this section we will call variants across two mouse samples.\nTo begin, check that there are two BAM files in the directory.\nls *.bam\nNow modify the variant calling command from the previous section to use both BAM files. Write the output to a BCF file called multi.bcf.\nNow index the file multi.bcf\nFilter the file multi.bcf using the same filters as the previous section and write the output to a BCF file called multi.filt.bcf.\nNow index the multi.filt.bcf file.\n\n\n4.1 Exercises\nQ1: What is the ts/tv of the raw calls and of the filtered set?\nQ2: What is the ts/tv of the removed sites?\nNow continue to the next section of the tutorial: Visualising variants\n\n\n5 Variant visualisation\nIt is often useful to visually inspect a SNP or indel of interest in order to assess the quality of the variant and interpret the genomic context of the variant. We can use the IGV tool to view some of the variant positions from the VCF file.\nStart IGV by typing:\nigv\n\n\n5.1 Load the reference genome\nOpen the mouse reference genome”\nGo to ’ Genomes -&gt; Load Genome From Server… ’ and select “Mouse mm10”. This is a synonym for GRCm38, which is the current mouse assembly (reference genome)\n\n\n5.2 Load the alignment\nLoad the alignment file for the sample A_J (A_J.bam).\nGo to ’ File -&gt; Load from File… ‘. Select the “A_J.bam” BAM file that you created in the previous section and click’ Open ’.\n\n\n5.3 Exercises\nUse the IGV navigation bar, go to the region chr19:10,001,874-10,002,017 and inspect the SNP at position 10001946.\nQ1: How many forward aligned reads support the SNP call?\nHint; Hover the mouse pointer over the coverage bar at the top (or click, depending on the IGV settings) to get this information.\nQ2: Was this SNP called by bcftools?\nHint Use bcftools view -H -r chr19:10001946 multi.filt.bcf to verify\nQ3: Did this SNP pass the filters?\nHint Look for this information in the BCF file\nUse the IGV navigation bar, go to the region chr19:10072443 and inspect the SNP at position 10072443.\nQ4: Was this SNP called by bcftools?\nQ5: Did the SNP pass the filters?\nQ6: Does this look like a real SNP? Please explain why.\nNow continue to the next section of the tutorial: Variant annotation\n\n\n6 Variant annotation\nVariant annotation is used to help researchers filter and prioritise functionally important variants for further study. There are several popular programs available for annotating variants. These include:\n• bcftools csq\n• Ensembl VEP (Variant Effect Predictor)\n• SnpEff\nThese tools can be used to to predict the functional consequence of the variants on the protein (e.g. whether a variant is mis-sense, stop-gain, frameshift inducing etc).\n\n\n6.1 bcftools csq\nHere we will use the lightweight bcftools csq command to annotate the variants. Type the command:\nbcftools view -i 'FILTER=\"PASS\"' multi.filt.bcf | bcftools csq -p m -f GRCm38_68.19.fa -g Mus_musculus.part.gff3.gz -Ob -o multi.filt.annot.bcf\nThe command takes VCF as input, the -f option specifies the reference file that the data was aligned\ntoo and the -g option specifies the GFF file that contains the gene models for the reference. Because our data is not phased, we provide the -p option (which does not actually phase the data, but tells the program to make an assumption about the phase). The -Ob option ensures the command produces compressed BCF as output.\nNow index the BCF file:\nbcftools index multi.filt.annot.bcf\n\n\n6.2 Exercises\nQ1 Use the bcftools query -f ‘%BCSQ’ command to extract the consequence at position 19:10088937.\nQ2 What is the functional annotation at this site?\nQ3 What is the amino acid change?\nCongratulations you have reached the end of the variant calling tutorial. For the answers to the exercises in this tutorial please visit answers.",
    "crumbs": [
      "Home",
      "Variant calling",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module3/module3_assessement_answers.html",
    "href": "course_modules/Module3/module3_assessement_answers.html",
    "title": "Quiz answers",
    "section": "",
    "text": "Q1.\nCorrect Answer: B Explanation: Germline mutations occur in egg or sperm cells, meaning they are heritable, while somatic mutations occur in non-reproductive tissues and are not passed on.\n\n\nQ2.\nCorrect Answer: B Explanation: A duplication involving 2 kb is an example of a copy number variant, which is considered a large-scale variation.\n\n\nQ3.\nCorrect Answer: C Explanation: SNVs refer to single base substitutions, and indels refer to small insertions or deletions generally less than 50 bp.\n\n\nQ4.\nCorrect Answer: C Explanation: Determining the speed of DNA replication is not a direct application of variant calling.\n\n\nQ5.\nCorrect Answer: B Explanation: Phased haplotypes allow for the determination of which variants are inherited together on the same chromosome, enhancing genotype interpretation.\n\n\nQ6.\nCorrect Answer: C Explanation: “TEMP” is not a standard field in the VCF format, while CHROM, POS, and ALT are.\n\n\nQ7.\nCorrect Answer: C Explanation: The correct order is to generate sequencing data, perform quality control, align the reads, generate a pileup, and then call variants.\n\n\nQ8.\nCorrect Answer: B Explanation: Homopolymers and repetitive regions are known to cause systematic errors in sequencing and mapping, leading to inaccuracies in variant calling.\n\n\nQ9.\nCorrect Answer: C Explanation: High-quality human SNP datasets typically have a Ts/Tv ratio of around 2 to 3.\n\n\nQ10.\nCorrect Answer: A Explanation: Population-level datasets provide valuable insights into human genetic diversity and help determine variant frequencies across various populations."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html",
    "href": "course_modules/Module4/Notebooks/index.html",
    "title": "Structural Variation Calling",
    "section": "",
    "text": "Structural variants (SVs) are large genomic alterations of at least 50 bp or larger in size. There are several types of SVs, including deletions, duplications, insertions, inversions, and translocations which describe different combinations of DNA gains, losses, or rearrangements. Copy number variations (CNVs) are a particular subtype of SV mainly represented by deletions and duplications. SVs are typically described as single events, although more complex scenarios involving combinations of SV types exist."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html#introduction",
    "href": "course_modules/Module4/Notebooks/index.html#introduction",
    "title": "Structural Variation Calling",
    "section": "",
    "text": "Structural variants (SVs) are large genomic alterations of at least 50 bp or larger in size. There are several types of SVs, including deletions, duplications, insertions, inversions, and translocations which describe different combinations of DNA gains, losses, or rearrangements. Copy number variations (CNVs) are a particular subtype of SV mainly represented by deletions and duplications. SVs are typically described as single events, although more complex scenarios involving combinations of SV types exist."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html#learning-outcomes",
    "href": "course_modules/Module4/Notebooks/index.html#learning-outcomes",
    "title": "Structural Variation Calling",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nOn completion of the tutorial, you can expect to be able to:\n\nCall structural variants using standard tools\nVisualise structural variants using standard tools\nCall structural variants from long read data\nUse bedtools to do regional comparisons over genomic co-ordinates"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html#tutorial-sections",
    "href": "course_modules/Module4/Notebooks/index.html#tutorial-sections",
    "title": "Structural Variation Calling",
    "section": "Tutorial sections",
    "text": "Tutorial sections\nThis tutorial comprises the following sections: 1. Looking at structural variants in VCF 2. Calling structural variants\n3. Structural variants from long reads\n4. Bedtools"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html#authors",
    "href": "course_modules/Module4/Notebooks/index.html#authors",
    "title": "Structural Variation Calling",
    "section": "Authors",
    "text": "Authors\nThis tutorial was written by Jacqui Keane based on material from Thomas Keane."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html#running-the-commands-in-this-tutorial",
    "href": "course_modules/Module4/Notebooks/index.html#running-the-commands-in-this-tutorial",
    "title": "Structural Variation Calling",
    "section": "Running the commands in this tutorial",
    "text": "Running the commands in this tutorial\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal window and type the command below:\n\ncd /home/manager/course_data/structural_variation/data"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html#lets-get-started",
    "href": "course_modules/Module4/Notebooks/index.html#lets-get-started",
    "title": "Structural Variation Calling",
    "section": "Let’s get started!",
    "text": "Let’s get started!\nThis tutorial requires that you have breakdancer, lumpy, minimap2, sniffles, bedtools and igv installed on your computer. These are already installed on the virtual machine you are using. To check that these are installed, run the following commands:\n\nbreakdancer-max -h\n\n\ndysgu --help\n\n\nminimap2 --help\n\n\nsniffles --help\n\n\nbedtools --help\n\nThis should return the help message for software breakdancer, dysgu, minimap2, sniffles and bedtools respectively.\nIf after this course you would like to download and install this software the instructions can be found at the links below, alternatively we recommend bioconda for the installation and management of your bioinformatics software.\n\nThe breakdancer website\nThe dysgu github page\nThe minimap2 website\nThe sniffles website\nThe bedtools website\n\nTo get started with the tutorial, go to the first section: Looking at structural variants in VCF"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/solutions.html",
    "href": "course_modules/Module4/Notebooks/solutions.html",
    "title": "Structural Variation Calling - Solutions",
    "section": "",
    "text": "No questions in this section."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/solutions.html#exercises",
    "href": "course_modules/Module4/Notebooks/solutions.html#exercises",
    "title": "Structural Variation Calling - Solutions",
    "section": "Exercises",
    "text": "Exercises\n\nWhat does the CIPOS format tag indicate? Confidence interval around POS for imprecise variants\nWhat does the PE tag indicate? Number of paired-end reads supporting the variant across all samples\nWhat tag is used to describe an inversion event? INV\nWhat tag is used to describe a duplication event? DUP\nHow many deletions were called in total? (Hint: DEL is the info field for a deletion. The -c option of the grep command can be used to return a count of matches.) 31, try\n\ngrep -c \"&lt;DEL&gt;\" ERR1015121.vcf\n\nWhat type of event is predicted at IV:437148? What is the length of the SV? How many paired-end reads and split-reads support this SV variant call? Deletion -370 20 PE 21 split\n\ngrep \"437148\" ERR1015121.vcf\n\nWhat is the total number of SV calls predicted on the IV chromosome? 10, try\n\ngrep -c \"^IV\" ERR1015121.vcf"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/solutions.html#breakdancer",
    "href": "course_modules/Module4/Notebooks/solutions.html#breakdancer",
    "title": "Structural Variation Calling - Solutions",
    "section": "Breakdancer",
    "text": "Breakdancer\n\nExercises\ngrep \"83065\" ERR1015121.breakdancer.out\n\nInversion\n-116,\n42\n\ngrep \"258766\" ERR1015121.breakdancer.out\n\nDeletion (7325, 99)\ngrep DEL | awk OFS= breakdancer.dels.bed | awk '{print $1\"\\t\"$2\"\\t\"$5\"\\t\"$7\"\\t\"$9}' &gt; breakdancer.dels.bed"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/solutions.html#inspecting-svs-with-igv",
    "href": "course_modules/Module4/Notebooks/solutions.html#inspecting-svs-with-igv",
    "title": "Structural Variation Calling - Solutions",
    "section": "Inspecting SVs with IGV",
    "text": "Inspecting SVs with IGV\n\nExercises\n\nYes, a deletion (view as paired, sort by insert size, squish).\nThere are very few reads mapping, the reads that are mapped are of low mapQ and it has a SV score = 99\nSize estimate? ~7.5k\n\nWas the deletion at II:258766 also called by the other structural variant software and was the predicted size?\n\nYes, SVTYPE=DEL, SVLEN=-7438\nDEL called by breakdancer (score=59). Not found by other caller Lumpy.\nYes, 2 reads support (red)."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/solutions.html#dysgu",
    "href": "course_modules/Module4/Notebooks/solutions.html#dysgu",
    "title": "Structural Variation Calling - Solutions",
    "section": "Dysgu",
    "text": "Dysgu\n\nExercises\n\nWhat was the total number of SVs identified? How many PASS SVs were identified by Dysgu? Why did the rest of the SVs fail? bcftools view -H ERR1015069.vcf | wc -l 30\n\nbcftools view -H -i ‘FILTER=“PASS”’ ERR1015069.vcf |wc -l 8\nlowProb: ##FILTER=&lt;ID=lowProb,Description=“Probability below threshold set with –thresholds”&gt;\n\nWhat type of SV event occurs at position IV:384221? What is the length of the SV event? What is the genotype quality?\n\nDEL = Deletion, SVLEN=328, GQ=62\n\nWhat type of SV event occurs at position XV:31115? What is the length of the SV event? What is the probability of the structural variant?\n\nINS = Insertion, SVLEN=63, PROB=0.816\n\nCalling Structural Variants from Long Reads\n\n\n\nAlign the reads with minimap and convert to bam\nminimap2 -t 2 -x map-pb -a ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa YPS128.filtered_subreads.10x.fastq.gz | samtools view -b -o YPS128.filtered_subreads.10x.bam -\n\n\nSort the bam\nsamtools sort -T temp -o YPS128.filtered_subreads.10x.sorted.bam YPS128.filtered_subreads.10x.bam\nsamtools calmd -b YPS128.filtered_subreads.10x.sorted.bam ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa &gt; YPS128.filtered_subreads.10x.sorted.calmd.bam\n\n\nIndex the sorted bam\nsamtools index YPS128.filtered_subreads.10x.sorted.calmd.bam\n\n\nCall SVs with sniffles\nsniffles --input YPS128.filtered_subreads.10x.sorted.calmd.bam --vcf YPS128.filtered_subreads.10x.vcf\n\n\nExercises\n\nWhat sort of SV was called at on chromosome ‘XV’ at position 854272? __Deletion_\nWhat is the length of the SV? 344\nHow many reads are supporting the SV? 14 (SUPPORT tag)\nWhat sort of SV was called at on chromosome ‘XI’ at position 74608? __Insertion_\nWhat is the length of the SV? 358\nHow many reads are supporting the SV? 15\nHow many inversions were called in the VCF? Note inversions are denoted by the type ‘INV’. 6 in total, 5 passed\nHow many duplications were called in the VCF? Note duplications are denoted by the type ‘DUP’. 2"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/solutions.html#exercises-5",
    "href": "course_modules/Module4/Notebooks/solutions.html#exercises-5",
    "title": "Structural Variation Calling - Solutions",
    "section": "Exercises",
    "text": "Exercises\n\nHow many SVs found in ERR1015069.dels.vcf overlap with a gene? (Hint: Use bedtools intersect command) 18, try (note the -u parameter is required to get the unique number of SVs)\n\nbedtools intersect -u -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3  | wc -l\n\nHow many SVs found in ERR1015069.dels.vcf do not overlap with a gene? (Hint: note the -v parameter to bedtools intersect) 9, try\n\nbedtools intersect -v -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3  | wc -l\n\nHow many SVs found in ERR1015069.dels.vcf overlap with a more strict definition of 50%? 14, try\n\nbedtools intersect -u -f 0.5 -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3  | wc -l\n\nHow many features does the deletion at VII:811446 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect. bedtools intersect -wb -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | grep 811446 4 features, all of them are protein coding genes (biotype=protein_coding)\nHow many features does the deletion at XII:650823 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect. bedtools intersect -wb -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | grep 811446 2 features, all of them are protein coding genes (biotype=protein_coding)\nWhat is the closest gene to the structural variant at IV:384220 in ERR1015069.dels.vcf? YDL037C, try\n\nbedtools closest -d -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3| grep IV | grep 384220\n\nHow many SVs overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf? 27, try\n\nbedtools intersect -u -a ERR1015069.dels.vcf -b ERR1015121.dels.vcf | wc -l\n\nHow many SVs have a 90% reciprocal overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf (Hint: first find the option for reciprocal overlap by typing: bedtools intersect -h) 24, try\n\nbedtools intersect -u -r -f 0.9 -a ERR1015069.dels.vcf -b ERR1015121.dels.vcf | wc -l"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/sv-calling.html",
    "href": "course_modules/Module4/Notebooks/sv-calling.html",
    "title": "Calling Structural Variants",
    "section": "",
    "text": "There are several software tools available for calling structural variants. We will use two callers in this part of the tutorial, breakdancer and lumpy"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/sv-calling.html#breakdancer",
    "href": "course_modules/Module4/Notebooks/sv-calling.html#breakdancer",
    "title": "Calling Structural Variants",
    "section": "Breakdancer",
    "text": "Breakdancer\nBreakDancer predicts five types of structural variants: insertions (INS), deletions (DEL), inversions (INV), inter-chromosomal translocations (CTX) and intra-chromosomal translocations (ITX) from next-generation short paired-end sequencing reads using read pairs that are mapped with unexpected separation distances or orientation.\nNavigate to the exercise2 directory:\n\ncd ../exercise2\n\n\nls\n\nWe will use the Breakdancer software package to call structural variants on a yeast sample that was paired-end sequenced on the illumina HiSeq 2000. Breakdancer first needs to examine the BAM file to get information on the fragment size distribution for each sequencing library contained in the BAM file.\nThe breakdancer.config file has information about the sequencing library fragment size distribution. Use the cat command to print the contents of the breakdancer.config file.\nQ What is the mean and standard deviation of the fragment size?\nRun the breakdancer SV caller using the command:\n\nbreakdancer-max breakdancer.config &gt; ERR1015121.breakdancer.out\n\nLook at the output of Breakdancer.\n\nhead ERR1015121.breakdancer.out\n\nNote that the output from Breakdancer is NOT VCF format, instead it is a simple text format with one line per SV event.\n\nExercises\n\nWhat type of SV event is predicted at position III:83065?\nWhat is the size of this SV?\nWhat is the score of this SV?\nWhat type of SV event is predicted at position II:258766?\nConvert the output of breakdancer into BED format\n\nThe BED format is explained here: https://genome.ucsc.edu/FAQ/FAQformat.html#format1\nTo complete this task, create a command that: 1. Extracts all the deletions from the breakdancer.out file (Hint: use grep) 2. Prints columns: 1, 2, 5, 7, and 9 to create a BED file with columns: chromosome, start, end, name, and score. (Hint: use awk to do this, e.g. awk '{print $1\"\\t\"$2}') 3. Print the resulting bed output into a file called: breakdancer.dels.bed"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/sv-calling.html#inspecting-svs-with-igv",
    "href": "course_modules/Module4/Notebooks/sv-calling.html#inspecting-svs-with-igv",
    "title": "Calling Structural Variants",
    "section": "Inspecting SVs with IGV",
    "text": "Inspecting SVs with IGV\nNow we will open the IGV genome browser and inspect some of the predicted structural variants.\nTo do this type:\n\nigv\n\nOpen the reference genome. Go to ’ Genomes -&gt; Load Genome From Server… ’ and select “S. cerevisiae (SacCer3)”.\nLoad the BAM file. Go to ’ File -&gt; Load from File… ‘. Select the “ERR1015121.bam” BAM file and click’ Open ’.\nLoad the BED file for the deletion calls that you created in the exercise 5 above. Go to ’ File -&gt; Load from File… ‘. Select the “breakdancer.dels.bed” BED file and click’ Open ’.\n\nExercises\nUsing the navigation bar, go to region II:258,500-266,700.\n\nCan you see the structural variant? What type of structural variant is it? (Hint: you may need to zoom out a little to see the full structural variant).\nCan you see any evidence to support this SV call?\nCan you estimate the size of the SV?\n\nThe VCF in the exercise1 directory was produced by another structural variant caller on the same sample as this exercise.\n\nLoad the exercise1/ERR1015121.vcf VCF into IGV also (File - Load from file, and select ERR1015121.vcf in the exercise 1 directory).\nWas the structural variant at II:258766 also called by the other structural variant software (lumpy)? If so, what was the predicted size?\n\nUsing the navigation bar, go to to region II:508,064-511,840.\n\nIs there a SV deletion called in this region by either SV caller?\nIs there any read support for a SV deletion in this region? If so, how many read pairs could support the deletion call (Hint: change the IGV view to squished and View as pairs to see any inconsistently aligned read pairs)."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/sv-calling.html#dysgu",
    "href": "course_modules/Module4/Notebooks/sv-calling.html#dysgu",
    "title": "Calling Structural Variants",
    "section": "Dysgu",
    "text": "Dysgu\nWe will use the Dysgu (pronounced duss-key) software package (https://github.com/kcleal/dysgu) to call structural variants on a yeast sample that was paired-end sequenced on the Illumina Hiseq 2000. Dysgu is designed to take BAM files that have been aligned with BWA-mem.\nNavigate to the exercise 3 directory:\n\ncd ../exercise3\n\n\nls\n\nCheck that there is a BAM file called ERR1015069.bam and an index file ERR1015069.bam.bai in the directory. The sequence data has already been mapped with bwa mem and the results are stored in ERR1015069.bam.\nTo call SVs, a sorted and indexed .bam/cram is needed plus an indexed reference genome (fasta format). Also a working directory must be provided to store temporary files.\n\ndysgu run ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa temp ERR1015069.bam &gt; ERR1015069.vcf\n\nDysgu is a multi-stage pipeline * The first stage of the “run” pipeline is to separate SV-associated reads - split/discordant reads, and reads with a soft-clip &gt;= clip_length (15 bp by default). * The next stage of the pipeline is to call and genotype SVs using the reads from the first stage. The run command above combines both of these stages together.\n\nExercises\n\nWhat was the total number of SVs identified? How many PASS SVs were identified by Dysgu? Why did the rest of the SVs fail?\nWhat type of SV event occurs at position IV:384221? What is the length of the SV event? What is the genotype quality?\nWhat type of SV event occurs at position XV:31115? What is the length of the SV event? What is the probability of the structural variant?\n\nCongratulations, you have sucessfully called structural variants from some NGS data. Now continue to the next section of the tutorial: Calling structural variants from long reads"
  },
  {
    "objectID": "course_modules/Module4/module4.html",
    "href": "course_modules/Module4/module4.html",
    "title": "Overview",
    "section": "",
    "text": "Module Title\nStructural Variation Calling\n\n\nAuthors\nThis tutorial was written by Jacqui Keane based on material from Thomas Keane.\n\n\nDuration\n3 hours\n\n\nKey topics\n\n\nLearning outcomes\n\nUnderstand the background and theory of structural variation\nCall structural variants using standard tools\nVisualise structural variants using standard tools\nCall structural variants from long read data\nUse bedtools to do regional comparisons over genomic coordinates\n\n\n\nActivities\n\nLecture: 1 hour\nPractical exercises: 1.5 hours\nAssessment quiz: 0.5 hours\n\n\n\nManual\nModule manual\nExercises\nSolutions\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual. Instructions on how to that will be added here.)\n\n\nLecture recording, notes and scripts\n\n\nCheck you knowledge quiz\nQuestions\n\n\nSummary of this module\n\n\nTeaching guidance\nInstructor notes\n\n\nDatasets\nPractice files for this module can be found on Github.\n\n\nPlatform Guidance\nGuide",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Overview"
    ]
  },
  {
    "objectID": "course_modules/Module4/module4_exercises.html",
    "href": "course_modules/Module4/module4_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Structural variants (SVs) are large genomic alterations of at least 50 bp or larger in size. There are several types of SVs, including deletions, duplications, insertions, inversions, and translocations which describe different combinations of DNA gains, losses, or rearrangements. Copy number variations (CNVs) are a particular subtype of SV mainly represented by deletions and duplications. SVs are typically described as single events, although more complex scenarios involving combinations of SV types exist.\n\n\n\nThis tutorial comprises the following sections:\n1. Looking at structural variants in VCF\n2. Calling structural variants\n3. Structural variants from long reads\n4. Bedtools\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files. To get started, open a new terminal window and type the command below:\ncd /home/manager/course_data/structural_variation/data\n\n\n\nThis tutorial requires that you have breakdancer, lumpy, minimap2, sniffles, bedtools and igv installed on your computer.\nThese are already installed on the virtual machine you are using. To check that these are installed, run the following commands:\nbreakdancer-max -h\ndysgu --help\nminimap2 --help\nsniffles --help\nbedtools --help\nThis should return the help message for software breakdancer, dysgu, minimap2, sniffles and bedtools respectively.\nIf after this course you would like to download and install this software the instructions can be found at the links below, alternatively we recommend bioconda for the installation and management of your bioinformatics software.\n• The breakdancer website\n• The dysgu github page\n• The minimap2 website\n• The sniffles website\n• The bedtools website\nTo get started with the tutorial, go to the first section: Looking at structural variants in VCF",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module4/module4_exercises.html#structural-variation-calling",
    "href": "course_modules/Module4/module4_exercises.html#structural-variation-calling",
    "title": "Exercises",
    "section": "",
    "text": "Structural variants (SVs) are large genomic alterations of at least 50 bp or larger in size. There are several types of SVs, including deletions, duplications, insertions, inversions, and translocations which describe different combinations of DNA gains, losses, or rearrangements. Copy number variations (CNVs) are a particular subtype of SV mainly represented by deletions and duplications. SVs are typically described as single events, although more complex scenarios involving combinations of SV types exist.\n\n\n\nThis tutorial comprises the following sections:\n1. Looking at structural variants in VCF\n2. Calling structural variants\n3. Structural variants from long reads\n4. Bedtools\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files. To get started, open a new terminal window and type the command below:\ncd /home/manager/course_data/structural_variation/data\n\n\n\nThis tutorial requires that you have breakdancer, lumpy, minimap2, sniffles, bedtools and igv installed on your computer.\nThese are already installed on the virtual machine you are using. To check that these are installed, run the following commands:\nbreakdancer-max -h\ndysgu --help\nminimap2 --help\nsniffles --help\nbedtools --help\nThis should return the help message for software breakdancer, dysgu, minimap2, sniffles and bedtools respectively.\nIf after this course you would like to download and install this software the instructions can be found at the links below, alternatively we recommend bioconda for the installation and management of your bioinformatics software.\n• The breakdancer website\n• The dysgu github page\n• The minimap2 website\n• The sniffles website\n• The bedtools website\nTo get started with the tutorial, go to the first section: Looking at structural variants in VCF",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module4/module4_exercises.html#looking-at-structural-variants-in-vcf",
    "href": "course_modules/Module4/module4_exercises.html#looking-at-structural-variants-in-vcf",
    "title": "Exercises",
    "section": "Looking at Structural Variants in VCF",
    "text": "Looking at Structural Variants in VCF\nStructural variants can be stored in VCF files. In this part of the tutorial, we will look at how these are represented in a VCF file.\nFirst, check you are in the correct directory:\npwd\nIt should display something like: /home/manager/course_data/structural_variation/data\nNavigate to the exercise1 directory:\ncd exercise1\nThere is a VCF file called ERR1015121.vcf that was produced using the Lumpy SV calling software. Look at the VCF file using the less command and answer the questions that follow:\nless ERR1015121.vcf\n\nExcercises\n1. What does the CIPOS format tag indicate?\n2. What does the PE tag indicate?\n3. What tag is used to describe an inversion event?\n4. What tag is used to describe a duplication event?\n5. How many deletions were called in total? (Hint: DEL is the info field for a deletion. The -c option of the grep command can be used to return a count of matches.)\n6. What type of event is predicted at IV:437148? What is the length of the SV? How many paired-end reads and split-reads support this SV variant call?\n7. What is the total number of SV calls predicted on the IV chromosome?\nNow continue to the next section of the tutorial: Calling structural variants",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module4/module4_exercises.html#calling-structural-variants",
    "href": "course_modules/Module4/module4_exercises.html#calling-structural-variants",
    "title": "Exercises",
    "section": "Calling Structural Variants",
    "text": "Calling Structural Variants\nThere are several software tools available for calling structural variants. We will use two callers in this part of the tutorial, breakdancer and lumpy\n\nBreakdancer\nBreakDancer predicts five types of structural variants: insertions (INS), deletions (DEL), inversions (INV), inter-chromosomal translocations (CTX) and intra-chromosomal translocations (ITX) from next-generation short paired-end sequencing reads using read pairs that are mapped with unexpected separation distances or orientation.\nNavigate to the exercise2 directory:\ncd ../exercise2\nls\nWe will use the Breakdancer software package to call structural variants on a yeast sample that was paired-end sequenced on the illumina HiSeq 2000. Breakdancer first needs to examine the BAM file to get information on the fragment size distribution for each sequencing library contained in the BAM file.\nThe breakdancer.config file has information about the sequencing library fragment size distribution. Use the cat command to print the contents of the breakdancer.config file.\nQ What is the mean and standard deviation of the fragment size?\nRun the breakdancer SV caller using the command:\nbreakdancer-max breakdancer.config &gt; ERR1015121.breakdancer.out\nLook at the output of Breakdancer.\nhead ERR1015121.breakdancer.out\nNote that the output from Breakdancer is NOT VCF format, instead it is a simple text format with one line per SV event.\n\n\nExercises\n1. What type of SV event is predicted at position III:83065?\n2. What is the size of this SV?\n3. What is the score of this SV?\n4. What type of SV event is predicted at position II:258766?\n5. Convert the output of breakdancer into BED format\n\n\nInspecting SVs with IGV\nThe BED format is explained here: https://genome.ucsc.edu/FAQ/FAQformat.html#format1\nTo complete this task, create a command that:\n1. Extracts all the deletions from the breakdancer.out file (Hint: use grep)\n2. Prints columns: 1, 2, 5, 7, and 9 to create a BED file with columns: chromosome, start, end, name, and score. (Hint: use awk to do this, e.g. awk ’{print $1”*”$2}’)***\n3. Print the resulting bed output into a file called: breakdancer.dels.bed\nNow we will open the IGV genome browser and inspect some of the predicted structural variants. To do this type:\nigv\nOpen the reference genome: Go to ’ Genomes -&gt; Load Genome From Server… ’ and select “S. cerevisiae (SacCer3)”.\nLoad the BAM file: Go to ’ File -&gt; Load from File… ‘. Select the “ERR1015121.bam” BAM file and click’ Open ’.\nLoad the BED file for the deletion calls that you created in the exercise 5 above: Go to ’ File -&gt; Load from File… ‘. Select the “breakdancer.dels.bed” BED file and click’Open’.\n\n\nExercises\nUsing the navigation bar, go to region II:258,500-266,700.\n1. Can you see the structural variant? What type of structural variant is it? (Hint: you may need to zoom out a little to see the full structural variant).\n2. Can you see any evidence to support this SV call?\n3. Can you estimate the size of the SV? The VCF in the exercise1 directory was produced by another structural variant caller on the same sample as this exercise.\n4. Load the exercise1/ERR1015121.vcf VCF into IGV also (File - Load from file, and select ERR1015121.vcf in the exercise 1 directory).\n5. Was the structural variant at II:258766 also called by the other structural variant software (lumpy)? If so, what was the predicted size? Using the navigation bar, go to to region II:508,064-511,840.\n6. Is there a SV deletion called in this region by either SV caller?\n7. Is there any read support for a SV deletion in this region? If so, how many read pairs could support the deletion call (Hint: change the IGV view to squished and View as pairs to see any inconsistently aligned read pairs).\n\n\nDysgu\nWe will use the Dysgu (pronounced duss-key) software package (https://github.com/kcleal/dysgu) to call structural variants on a yeast sample that was paired-end sequenced on the Illumina Hiseq 2000.\nDysgu is designed to take BAM files that have been aligned with BWA-mem.\nNavigate to the exercise 3 directory:\ncd ../exercise3\nls\nCheck that there is a BAM file called ERR1015069.bam and an index file ERR1015069.bam.bai in the directory. The sequence data has already been mapped with bwa mem and the results are stored in ERR1015069.bam.\nTo call SVs, a sorted and indexed .bam/cram is needed plus an indexed reference genome (fasta format). Also a working directory must be provided to store temporary files.\ndysgu run ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa temp ERR1015069.bam &gt; ERR1015069.vcf\nDysgu is a multi-stage pipeline\n\nThe first stage of the “run” pipeline is to separate SV-associated reads - split/discordant reads, and reads with a soft-clip &gt;= clip_length (15 bp by default)\nThe next stage of the pipeline is to call and genotype SVs using the reads from the first stage. The run command above combines both of these stages together.\n\n1. What was the total number of SVs identified? How many PASS SVs were identified by Dysgu? Why did the rest of the SVs fail?\n2. What type of SV event occurs at position IV:384221? What is the length of the SV event? What is the genotype quality?\n3. What type of SV event occurs at position XV:31115? What is the length of the SV event? What is the probability of the structural variant?\nCongratulations, you have sucessfully called structural variants from some NGS data. Now continue to the next section of the tutorial: Calling structural variants from long reads",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module4/module4_exercises.html#calling-structural-variants-from-long-reads",
    "href": "course_modules/Module4/module4_exercises.html#calling-structural-variants-from-long-reads",
    "title": "Exercises",
    "section": "Calling Structural Variants from Long Reads",
    "text": "Calling Structural Variants from Long Reads\nIn this part of the tutorial we will use long read data to identify structural variants using the SV caller Sniffles.\nFirst navigate to the exercise4 directory:\ncd ../exercise4\nls\n\nIntroducing the dataset\nWe will use data from a Saccharomyces cerevisiae strain (YPS128) that was sequenced at the Wellcome Sanger Institute and deposited in the ENA (Project: PRJEB7245, sample: SAMEA2757770, analysis: ERZ448241).\nThe sequencing reads are contained in a fastq file: YPS128.filtered_subreads.10x.fastq.gz\nThe reference genome is in the ../ref directory in a fasta file: Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa\n\n\nAlign the data\nBefore you can use Sniffles to call SVs, it is very important that the reads are aligned with an aligner suitable for long reads. The software minimap2 is a long-read aligner designed to align PacBio or Oxford Nanopore (standard and ultra-long) to a reference genome. You can find the usage of minimap2 by typing:\nminimap2\nUse minimap2 to align the reads and send the output to a SAM file called YPS128.filtered_sub- reads.10x.sam\nNote: use the -t option to use multiple threads in parallel (this will increase the speed of the alignment by using more than one CPU core, I suggest using 2).\nAlso look at the -x option. Convert the SAM file to BAM.\nSort the BAM file and produce a sorted BAM file called YPS128.filtered_subreads.10X.sorted.bam.\nHint: Use samtools sort.\n\n\nCall structural variants\nUse samtools calmd to calculates MD and NM tags. This enables variant calling without requiring access to the entire original reference.\nsamtools calmd -b YPS128.filtered_subreads.10x.sorted.bam ../ref/ Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa &gt; YPS128.filtered_subreads.10x.fastq.sorted.calmd.bam\nFinally, use samtools to index this BAM file.\nHint: Use samtools index\nSniffles is a structural variation (SV) caller that is designed for long reads (Pacbio or Oxford Nanopore). It detects all types of SVs (10bp+) using evidence from split-read alignments, high- mismatch regions, and coverage analysis. Sniffles takes the BAM file as input and outputs VCF.\nTo find the usage for Sniffles, type:\nsniffles\nUsing the default parameters, call SVs with Sniffles and output the results to a VCF file called YPS128.10x.vcf.\nHint: You don’t need to change any of the default parameters, but you will need to work out how to provide the input BAM file and specify the output VCF file. The documentation on sniffles is here : https://github.com/fritzsedlazeck/Sniffles/wiki/Parameter\n\n\nInspecting SVs with IGV\nOpen IGV:\nigv\nOpen the reference genome ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa and load the BAM file YPS128.filtered_subreads.10x.fastq.sorted.calmd.bam and the VCF file YPS128.filtered_subreads.10x.vcf.\nNow answer the questions that follow using either the command line or IGV.\n1. What sort of SV was called at on chromosome ‘XV’ at position 854272?\n2. What is the length of the SV?\n3. How many reads are supporting the SV?\n4. What sort of SV was called at on chromosome ‘XI’ at position 74608?\n5. What is the length of the SV?\n6. How many reads are supporting the SV?\n7. How many inversions were called in the VCF? Note inversions are denoted by the type ‘INV’.\n8. How many duplications were called in the VCF? Note duplications are denoted by the type ‘DUP’.\nNow continue to the next section of the tutorial: bedtools.",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module4/module4_exercises.html#bedtools",
    "href": "course_modules/Module4/module4_exercises.html#bedtools",
    "title": "Exercises",
    "section": "Bedtools",
    "text": "Bedtools\nBedtools is an extremely useful tool for doing regional comparisons over genomic coordinates. It has many commands for doing region based comparisons with BAM, VCF, GFF, BED file formats.\nTo see the list of commands available, on the command line type:\nbedtools\nNavigate to the exercise5 directory:\ncd ../exercise5\nls\nIn this directory, there are two VCF files and the yeast genome annotation in GFF3 format Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3.\n\nbedtools intersect\n\n\n\n(Credit to Aaron Quinlan for original source of figure: http://quinlanlab.org/tutorials/bedtool- s/bedtools.html)\n\n\nGiven two sets of genomic features, the bedtools intersect command can be used to determine whether or not any of the features in the two sets “overlap” with one another. For the intersect command, the -a and -b parameters are used to denote the input files A and B.\nFor example, to find out the overlap between the SVs in ERR1015069.dels.vcf and the annotated region of the yeast genome try\nThis command reports the variant in the file ERR1015069.dels.vcf every time it overlaps with a feature in Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3.\nTherefore if a variant overlaps more than one feature it will be reported more than once. To report the unique set of variants use:\nbedtools intersect -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\nbedtools intersect -u -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1- 1.82.genes.gff3\nThe default is to report overlaps between features in A and B so long as at least one base pair of overlap exists. However, the -f option allows you to specify what fraction of each feature in A should be overlapped by a feature in B before it is reported.\nTo specify a more strict intersect and require at least 25% of overlap of the SV with the genes use the command:\nbedtools intersect -u -f 0.25 -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\nThe bedtools intersect command can also be used to determine how many SVs overlap between two VCF files. For more information about bedtools intersect see the help:\nbedtools intersect -h\n\n\nExercises\n1. How many SVs found in ERR1015069.dels.vcf overlap with a gene? (Hint: Use bedtools intersect command)\n2. How many SVs found in ERR1015069.dels.vcf do not overlap with a gene? (Hint: note the -v parameter to bedtools intersect)\n3. How many SVs found in ERR1015069.dels.vcf overlap with a more strict definition of 50%?\n4. How many features does the deletion at VII:811446 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect.\n5. How many features does the deletion at XII:650823 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect.\n\n\nbedtools closest\nSimilar to intersect, bedtools closest searches for overlapping features in A and B. In the event that no feature in B overlaps the current feature in A, closest will report the nearest (that is, least genomic distance from the start or end of A) feature in B.\nAn example of the usage of bedtools closest is:\nbedtools closest -d -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\nThis command will list all the features in the file Saccharomyces_cerevisiae.R64-1- 1.82.genes.gff3 that are closest to each of the variants in ERR1015069.dels.vcf. The -d option means that in addition to the closest feature in Saccharomyces_cerevisiae.R64-1- 1.82.genes.gff3, the distance to the variant in ERR1015069.dels.vcf will be reported as an extra column. The reported distance for any overlapping features will be 0.\nFor example, to find the closest gene to the variant found at position 43018 on chromosome XV, try:\nbedtools closest -d -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3| grep XV | grep 43018\nFor more information about bedtools closest see the help:\nbedtools closest -h\n\n\nExercises\n6. What is the closest gene to the structural variant at IV:384220 in ERR1015069.dels.vcf?\n7. How many SVs overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf?\n8. How many SVs have a 50% reciprocal overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf (Hint: first find the option for reciprocal overlap by typing: bedtools intersect -h)\nCongratulations, you have reached the end of the tutorial",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module4/module4_solutions.html",
    "href": "course_modules/Module4/module4_solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "No questions in this section.",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Solutions"
    ]
  },
  {
    "objectID": "course_modules/Module4/module4_solutions.html#structural-variation-calling---solutions",
    "href": "course_modules/Module4/module4_solutions.html#structural-variation-calling---solutions",
    "title": "Solutions",
    "section": "",
    "text": "No questions in this section.",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Solutions"
    ]
  },
  {
    "objectID": "course_modules/Module4/module4_solutions.html#looking-at-structural-variants-in-vcf",
    "href": "course_modules/Module4/module4_solutions.html#looking-at-structural-variants-in-vcf",
    "title": "Solutions",
    "section": "2 Looking at Structural Variants in VCF",
    "text": "2 Looking at Structural Variants in VCF\n\n2.1 Exercises\n1. What does the CIPOS format tag indicate? Confidence interval around POS for imprecise variants\n2. What does the PE tag indicate? Number of paired-end reads supporting the variant across all samples\n3. What tag is used to describe an inversion event? INV\n4. What tag is used to describe a duplication event? DUP\n5. How many deletions were called in total? (Hint: DEL is the info field for a deletion. The -c option of the grep command can be used to return a count of matches.), try\ngrep -c \"&lt;DEL&gt;\" ERR1015121.vcf\n6. What type of event is predicted at IV:437148? What is the length of the SV? How many paired-end reads and split-reads support this SV variant call? Deletion -370 20 PE 21 split\ngrep \"437148\" ERR1015121.vcf\n7. What is the total number of SV calls predicted on the IV chromosome? 10, try\ngrep -c \"^IV\" ERR1015121.vcf",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Solutions"
    ]
  },
  {
    "objectID": "course_modules/Module4/module4_solutions.html#calling-structural-variants",
    "href": "course_modules/Module4/module4_solutions.html#calling-structural-variants",
    "title": "Solutions",
    "section": "3 Calling Structural Variants",
    "text": "3 Calling Structural Variants\nQ: mean=454.87 std=86.29\n\n3.1 Breakdancer\n\n\n3.1.1 Exercises\ngrep \"83065\" ERR1015121.breakdancer.out\n1. Inversion\n2. -116,\n3. 42\ngrep \"258766\" ERR1015121.breakdancer.out\n4. Deletion (7325, 99)\n5. grep DEL | awk OFS= breakdancer.dels.bed | awk '{print $1\"\\t\"$2\"\\t\"$5\"\\t\"$7\"\\t\"$9}' &gt; breakdancer.dels.bed",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Solutions"
    ]
  },
  {
    "objectID": "course_modules/Module4/module4_solutions.html#calling-structural-variants-from-long-reads",
    "href": "course_modules/Module4/module4_solutions.html#calling-structural-variants-from-long-reads",
    "title": "Solutions",
    "section": "4 Calling Structural Variants from Long Reads",
    "text": "4 Calling Structural Variants from Long Reads\n\n3.2 Inspecting SVs with IGV\n\n\n3.2.1 Exercises\n1. Yes, a deletion (view as paired, sort by insert size, squish).\n2. There are very few reads mapping, the reads that are mapped are of low mapQ and it has a SV score = 99\n3. Size estimate? ~7.5k\nWas the deletion at II:258766 also called by the other structural variant software and was the predicted size?\n5. Yes, SVTYPE=DEL, SVLEN=-7438\n6. DEL called by breakdancer (score=59). Not found by other caller Lumpy.\n7. Yes, 2 reads support (red).\n\n\n3.3 Lumpy\n\n\n3.3.1 Exercises\n1. The -F option in samtools view excludes reads matching the specified flag\n2. reads in proper pair | read unmapped | mate unmapped | not primary alignment | PCR optical duplicate\n3. Deletion -625\n4. Deletion -369\n\n\n4 Calling Structural Variants from Long Reads\n4.0.1 Align the reads with minimap and convert to bam\nminimap2 -t 2 -x map-pb -a ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa.gz YPS128.filtered_subreads.10x.fastq.gz | samtools view -b -o YPS128.filtered_subreads.10x.bam -\n4.0.2 Sort the bam\nsamtools sort -T temp -o YPS128.filtered_subreads.10x.sorted.bam YPS128.filtered_subreads.10x.bam\nsamtools calmd -b YPS128.filtered_subreads.10x.sorted.bam ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa.gz &gt; YPS128.filtered_subreads.10x.sorted.calmd.bam\n4.0.3 Index the sorted bam\nsamtools index YPS128.filtered_subreads.10x.sorted.calmd.bam\n\n\n5 Bedtools\n\n\n4.0.4 Call SVs with sniffles\nsniffles -m YPS128.filtered_subreads.10x.sorted.calmd.bam -v YPS128.filtered_subreads.10x.vcf\n\n\n4.0.5 Exercises\n1. What sort of SV was called at on chromosome ‘XV’ at position 854271? __Deletion_\n2. What is the length of the SV? 345\n3. How many reads are supporting the SV? 17 (RE tag)\n4. What sort of SV was called at on chromosome ‘XI’ at position 74608? __Insertion_\n5. What is the length of the SV? 358\n6. How many reads are supporting the SV? 15\n7. How many inversions were called in the VCF? Note inversions are denoted by the type ‘INV’.\nNone - no inversions were called\n8. How many duplications were called in the VCF? Note duplications are denoted by the type ‘DUP’. 2\n\n\n5 Bedtools\n\n\n5.1 Exercises\n1. How many SVs found in ERR1015069.dels.vcf overlap with a gene? (Hint: Use bedtools intersect command) 18, try (note the -u parameter is required to get the unique number of SVs)\nbedtools intersect -u -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | wc -l\n2. How many SVs found in ERR1015069.dels.vcf do not overlap with a gene? (Hint: note the -v parameter to bedtools intersect) 9, try\nbedtools intersect -v -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | wc -l\n3. How many SVs found in ERR1015069.dels.vcf overlap with a more strict definition of 50%?\n14, try\nbedtools intersect -u -f 0.5 -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | wc -l\n4. How many features does the deletion at VII:811446 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect.\nbedtools intersect -wb -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | grep 811446\n4 features, all of them are protein coding genes (biotype=protein_coding)\n5. How many features does the deletion at XII:650823 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect.\nbedtools intersect -wb -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | grep 811446\n2 features, all of them are protein coding genes (biotype=protein_coding)\n6. What is the closest gene to the structural variant at IV:384220 in ERR1015069.dels.vcf?\nYDL037C, try\nbedtools closest -d -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3| grep IV | grep 384220\n5. How many SVs overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf? 27, try\nbedtools intersect -u -a ERR1015069.dels.vcf -b ERR1015121.dels.vcf | wc -l\n6. How many SVs have a 90% reciprocal overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf (Hint: first find the option for reciprocal overlap by typing: bedtools intersect -h) 24, try\nbedtools intersect -u -r -f 0.9 -a ERR1015069.dels.vcf -b ERR1015121.dels.vcf | wc -l",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Solutions"
    ]
  },
  {
    "objectID": "welcome.html",
    "href": "welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to the course on Genome Sequence Analysis!\n\nThis practical course will take you through the complete workflow of genome sequence data analysis. You’ll learn key steps including quality assessment, read alignment, variant calling, annotation, genome assembly, RNA-seq data analysis and ChIP-seq data analysis. By the end, you’ll be equipped with the skills to confidently process and interpret various genomic datasets in research and clinical contexts."
  },
  {
    "objectID": "template_course.html",
    "href": "template_course.html",
    "title": "template_course",
    "section": "",
    "text": "Course Overview\n- Course Title\n- Course Description: Short and extended versions\n- Course format \n- Target Audience: (e.g., clinicians, researchers, students)\n- Prerequisites: Knowledge or skills expected\n \nLearning Outcomes\n- Clearly defined, measurable outcomes using Blooms action verbs \n- Mapped to competencies if relevant\nAssessment\n- Formative assessments (quizzes, exercises, peer review)\n- Summative assessments (final project, test, reflection)\n- Rubrics and grading templates for instructors \n- Submission methods/platform\n- Model answers or guides \n \nTeaching Platform & Tech Tools\n- Platforms guide: e.g., GitHub Classroom, Google classroom, Moodle, Zoom etc \n- Accounts/Access setup instructions\n- Tools used in communication and teaching: e.g., Zoom, Slack, BLAST, RStudio\n- Instructions for setting up any required environments (Docker, Conda, virtual machines)\n \nResources & References\n- Core readings (articles, textbooks)\n- Supplementary materials (websites, tools, videos)\n- Glossary of terms\n- FAQs and Troubleshooting guide (tech and teaching)\n \nInstructor & Facilitator Guide - important to include\n- Adult learning strategies [see T3 resource] \n- Teaching tips for each module\n- Common learner questions/issues \n- Discussion prompts\n- Inclusivity guidance [see T3 resource, UDL] \n \n \nEvaluation & Feedback Templates\n- Evaluation questions [what do we want to know] \n- Data collection: learner feedback forms, instructor reflection log/observation structured notes, ost course survey etc)\n- Plan for data analysis \n- Plan for results dissemination"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "The materials provided in this repository are FREE to use. This work is licensed under a Creative Commons Attribution 4.0 International License. Reuse is encouraged with acknowledgement.\n\nAll materials to be added in markdown (with extension .qmd), except for recorded lectures.\nThere are no slides on this website. Instead, you will find instructions on how to generate slides from the markdown content.\n\nWelcome to the course\nMeet the team\nGlossary"
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html",
    "href": "course_modules/Module4/module4_manual.html",
    "title": "Manual",
    "section": "",
    "text": "Genomic structural variation (SV) refers to any rearrangement of chromosome structure and includes alterations such as insertions, deletions, inversions, translocations, and copy number changes. These variations contribute significantly to genetic diversity, evolution, gene function, and phenotypic variation, and they can involve rare variants with large biological effects. Structural variants are also frequent causes of disease, known as genomic disorders, and are associated with both Mendelian conditions and complex traits, including behaviors. For example, an increase in gene dosage due to elevated copy number can have a direct impact on phenotype or disease. Some SVs are complex events, involving multiple types of variation in close proximity. A key concept in SV analysis is the breakpoint, defined as a pair of bases that are adjacent in a sequenced sample genome but not in the reference genome. Detecting structural variation involves a variety of experimental techniques, each with strengths in resolving different SV types and sizes.",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html#genomic-structural-variation",
    "href": "course_modules/Module4/module4_manual.html#genomic-structural-variation",
    "title": "Manual",
    "section": "",
    "text": "Genomic structural variation (SV) refers to any rearrangement of chromosome structure and includes alterations such as insertions, deletions, inversions, translocations, and copy number changes. These variations contribute significantly to genetic diversity, evolution, gene function, and phenotypic variation, and they can involve rare variants with large biological effects. Structural variants are also frequent causes of disease, known as genomic disorders, and are associated with both Mendelian conditions and complex traits, including behaviors. For example, an increase in gene dosage due to elevated copy number can have a direct impact on phenotype or disease. Some SVs are complex events, involving multiple types of variation in close proximity. A key concept in SV analysis is the breakpoint, defined as a pair of bases that are adjacent in a sequenced sample genome but not in the reference genome. Detecting structural variation involves a variety of experimental techniques, each with strengths in resolving different SV types and sizes.",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html#methods-for-detecting-structural-variants",
    "href": "course_modules/Module4/module4_manual.html#methods-for-detecting-structural-variants",
    "title": "Manual",
    "section": "Methods for detecting Structural Variants",
    "text": "Methods for detecting Structural Variants\n\nExperimental approaches\nStructural variant (SV) detection can be performed using a range of experimental approaches. Chromosome banding is a classical cytogenetic technique where stained chromosomes from dividing cells are examined under a microscope to detect large-scale abnormalities, such as deletions, duplications, or translocations, based on altered banding patterns. Fluorescence in situ hybridization (FISH) uses fluorescently labeled DNA probes to identify specific loci on chromosomes in both metaphase and interphase cells. It is particularly useful for confirming the presence, absence, or relocation of genomic regions, and can be used alongside data from microarrays or sequencing to validate findings. Microarrays, including array comparative genomic hybridisation (array CGH) and SNP arrays, detect copy number variations (CNVs) and allelic imbalances by comparing hybridization intensities between test and reference genomes. However, microarrays do not provide precise information about SV breakpoints or structural organization.\n\n\nSequencing-based approaches\nSequencing-based approaches offer higher resolution for detecting structural variants. Whole-genome sequencing (WGS) allows the identification of SVs by analyzing paired-end reads with discordant alignments, which signal breakpoints in the genome. This method is capable of detecting both copy-number and copy-neutral SVs. Third-generation sequencing technologies, such as those from PacBio or Oxford Nanopore, generate long reads (several kilobases in length), which can span complex or repetitive regions, enabling more accurate detection and mapping of SVs. Long-read sequencing is particularly effective for identifying complex rearrangements and resolving ambiguous alignments that are challenging for short-read platforms. These sequencing approaches have become essential for comprehensive SV discovery and characterization.\nFiber-FISH (fluorescence in situ hybridization on DNA fibers) is a high-resolution cytogenetic technique used to visualize the physical arrangement and organization of genomic regions along stretched DNA molecules. By using color-coded fluorescent probes that hybridize to specific sequences, Fiber-FISH enables direct visualization of structural variation, such as insertions, deletions, duplications, or gene order changes across individuals or strains.\n\n\n\nExample of fiberFISH. In the example shown, DNA from different mouse strains is analyzed across a segment of chromosome 10 (Chr10:21.1–21.7 Mb). The C57BL/6J reference genome is shown at the top with genes such as Raet1e, H60b, and Raet1d labeled. Five fosmid probes are used, each assigned a distinct color (e.g., DIG = red, Bio = blue, DNP = green), to span the region of interest. The lower panels display Fiber-FISH results for various strains. For instance, PWK/EiJ shows a markedly different signal pattern from the reference, suggesting rearrangements or copy number differences in the Raet1 region. Similarly, the A/J and NOD/ShiLtJ strains display additional segments such as Raet1b and Raet1a, and duplicated probe signals (e.g., repeated blue and red signals), indicating complex structural variation. Fiber-FISH provides a clear, direct visualization of such differences, complementing sequencing-based SV detection.\n\n\n\n\n\nStructural variation types, Schematic representation of different types of structural variation. (A) Normal chromosomes. (B) Intrachromosomal SVs include inversion, terminal deletion, interstitial deletion, and interstitial duplication. (C) Interchromosomal SVs include balanced and unbalanced translocations, where segments are exchanged or rearranged between chromosomes.Human Structural Variation: Mechanisms of Chromosome Rearrangements, Weckselblatt, Brooke et al., Trends in Genetics, Volume 31, Issue 10, 587 - 599\n\n\n\n\nSV types and NGS paired-end sequencing\nDifferent types of structural variation (SV)—deletion, insertion, inversion, and translocation—can be detected using next-generation sequencing (NGS) paired-end reads. In paired-end sequencing, each DNA fragment is sequenced from both ends, producing two reads with an expected orientation and insert size. SVs disrupt these expectations in predictable ways. For example, a deletion results in paired reads mapping farther apart than expected, while an insertion causes them to map closer together. In an inversion, read pairs may map with incorrect orientation (e.g., both pointing in the same direction), and in a translocation, each read may map to different chromosomes or distant loci. These abnormal mapping patterns serve as key indicators for identifying SVs in genome sequencing data.\n\n\n\nRetrotransposition\nRetrotransposition refers to the movement of transposable elements (TEs)—segments of DNA capable of changing their position within the genome. These elements possess a minimal “genome” that allows them to replicate and relocate, often leaving behind molecular relics of ancient viral infections. TEs are a dominant feature of mammalian genomes, comprising 38–45% of rodent and primate genomes, and contributing significantly to genome size. They are classified into Class 1 elements, which move via an RNA intermediate (e.g., LINEs and SINEs), and Class 2 elements, which move via a DNA intermediate. Retrotransposons are potent mutagens, capable of disrupting gene expression, triggering genome rearrangements, and driving evolutionary change through mechanisms such as transduction of flanking sequences. These elements are also species-specific: in humans, common families include Alu, L1, and SVA, while in mice, SINE, LINE, and ERV elements are prevalent.\n\n\n\nStructural features and activity of retrotransposons and DNA transposons. (A) Schematic representations of major human retrotransposon families—Alu, SVA, L1, and HERV—highlight their internal components and approximate sizes. (B) Diagram of the MARINER DNA transposon and a conceptual illustration of germline and somatic retrotransposition in humans. Adapted from Genomics Inform. 2012 Dec;10(4):226–233.\n\n\nIn the figure above, Panel (A) displays Class I retrotransposons, which mobilize via an RNA intermediate. Alu elements (~300 bp) are primate-specific SINEs made of two similar monomers with a poly(A) tail and target site duplications (TSDs). SVA (~2 kb) is a composite element combining features of SINEs, VNTRs, and Alu-like sequences. L1 (~6 kb) is an autonomous LINE element with two open reading frames (ORF1 and ORF2) that encode proteins for reverse transcription and integration. HERV (~10 kb) are endogenous retroviruses with coding potential for structural (GAG), enzymatic (POL), and envelope (ENV) proteins, flanked by long terminal repeats (LTRs).\nPanel (B) shows a Class II DNA transposon (MARINER, ~1 kb) with a DNA-binding domain and a DDE catalytic domain flanked by inverted repeats (IRs). The lower image illustrates retrotransposition events:\n\nGermline retrotransposition, such as Alu and L1 mobilization, contributes to genomic diversity across human populations.\nSomatic retrotransposition can occur in specific tissues, such as tumors, where elements like L1 insert into new locations, potentially disrupting gene regulation and contributing to cancer development.\n\n\n\nSources of evidence\n\nFragment size\nPaired-end sequencing provides a valuable source of evidence for detecting various types of structural variations (SVs), including large insertions, deletions, inversions, and translocations. In this approach, both ends of a DNA fragment are sequenced, generating read pairs with a known orientation and expected insert size. When read pairs deviate from these expectations—such as mapping farther apart or closer together than expected, aligning in unexpected orientations, or when one mate is missing—it can signal the presence of an SV. For example, a deletion will cause read pairs to map with increased distance, while inversions or translocations may lead to reads mapping in reversed orientations or to different chromosomes. Thus, read pair analysis is a fundamental strategy for identifying structural rearrangements in genomic data.\n\n\n\nThe ~200 bp spread reflects variation in library preparation and fragment size, and deviations from this expected range can indicate potential structural variants such as insertions or deletions.\n\n\n\n\n\nFragment size QC\n\n\nFragment size quality control (QC) is a key method for detecting potential structural variation (SV) in sequencing data. The figure above shows insert size distributions from two BAM files. The left panel displays a tight, unimodal distribution centered around ~450 bp, typical of a well-prepared library with consistent fragment size. In contrast, the right panel shows a bimodal distribution, with an unexpected secondary peak at ~150–200 bp. This deviation suggests the presence of aberrant fragment sizes, which may reflect insertions, deletions, or other SVs. Unusual peaks, broader distributions, or multiple modes in insert size profiles are often early indicators of structural rearrangements, contamination, or library preparation artifacts, highlighting the importance of insert size QC in NGS workflows.\n\n\n\nFragment size QC. The DBA/1J library shows a broader distribution (~450 bp range), centered at 176 bp, whereas DBA/2J has a tighter distribution (~250 bp range), centered at 328 bp. This difference in fragment size range impacts structural variant (SV) detection sensitivity. SV callers typically define discordant read pairs—used to detect SVs—based on deviations outside the expected insert size range. In DBA/1J, the wider distribution means fewer read pairs fall outside this threshold, especially for variants in the 300–500 bp range, resulting in lower SV detection sensitivity compared to DBA/2J, where the narrower insert size window more readily flags such events as discordant.\n\n\n\n\nSplit reads\n\n\n\nSplit reads\n\n\nSplit-read alignments provide powerful evidence for identifying structural variation (SV), particularly for pinpointing breakpoints at base-pair resolution. A split read occurs when a single sequencing read spans a structural variant junction and thus aligns in two separate parts to different positions on the reference genome. In the figure, reads at the breakpoint do not align continuously but are soft-clipped or partially aligned, indicating a disruption in the reference sequence. These reads suggest the presence of an insertion, deletion, or other rearrangement. However, due to sequencing and alignment inaccuracies, the precise breakpoint may not always be clear, resulting in slight ambiguity in breakpoint localization.\n\n\nRead depth\n\n\n\nRead depth. Plot of sequencing depth across a one-megabase region of A/J chromosome 17, highlighting both a 3-fold copy number gain between 30.6–31.1 Mb and a copy number loss at 31.3 Mb. The solid black line above the plot indicates the gain, while the line below indicates the loss. (B) A plot of heterozygous SNP rate across the same region reveals a high density of apparent heterozygous SNPs in the copy number gain region, consistent with misaligned paralogous reads. Adapted from Simpson et al. (2009), Bioinformatics, 25(12):1461–1462. https://doi.org/10.1093/bioinformatics/btp161\n\n\n\n\n\nBreakDancer\nBreakDancer is a structural variant (SV) caller that uses paired-end read information to detect a wide range of SV types, including deletions, insertions, inversions, as well as intra- and interchromosomal translocations. It takes a BAM file as input and begins by analyzing a subset of reads to estimate the mean and standard deviation of the fragment size for each sequencing library. The algorithm then systematically scans each chromosome to identify anomalous read pairs—those that deviate in orientation or insert size from expected values. These anomalies are grouped into interconnected clusters, which are then classified into SV types. The final output is a text file, listing one SV event per line, including its type, coordinates, and supporting evidence. Users can apply filters based on the minimum number of supporting reads, quality score, or SV type to refine the call set for downstream analysis.\n\n\nLUMPY\n\n\n\nLUMPY\n\n\nLUMPY is a flexible and probabilistic structural variant (SV) caller that integrates multiple sources of alignment evidence—including read pairs, split reads, read depth, and even user-supplied data—into a unified discovery process (Layer et al., 2014). It is designed with modular components, each responsible for parsing and mapping a specific type of alignment evidence to breakpoint intervals. These intervals represent genomic positions where structural rearrangements likely occur. LUMPY identifies clusters of overlapping evidence and calculates the probability of each SV event based on the combined support from all evidence types. This integrative approach allows LUMPY to detect a wide variety of SVs with high sensitivity and specificity, outperforming tools that rely on a single evidence type.\nReference: Layer RM, Chiang C, Quinlan AR, Hall IM. (2014). LUMPY: a probabilistic framework for structural variant discovery. Genome Biology, 15(6):R84. https://doi.org/10.1186/gb-2014-15-6-r84\nGithub: https://github.com/arq5x/lumpy-sv\n\n\nStructural variation visualisation\nStructural variant (SV) visualisation is inherently more complex than viewing single nucleotide polymorphisms (SNPs) or small indels due to the larger genomic span and complex read patterns involved. Visualising SVs often requires inspection of several hundred to thousands of base pairs to identify features such as discordant read pairs, split reads, and soft-clipped bases, which help in refining breakpoint positions and understanding the SV type. Interpreting these patterns is essential not only for confirming variant calls but also for identifying potential alignment artifacts or false positives. Among the available NGS visualisation tools, IGV (Integrative Genomics Viewer) from the Broad Institute is widely used due to its intuitive interface and powerful display features. IGV requires a BAM file with aligned reads and the corresponding reference genome in FASTA format. To effectively interpret different SV types, viewing settings in IGV—such as coloring by read strand, showing soft clips, or displaying insert size—should be tailored appropriately, often guided by best practice notes or visual examples.\n\n\n\nA deletion, as shown in IGV. A complex deletion likely affecting exonic or coding sequence. It is characterized by a drop in coverage, split and discordant read pairs, and mismatches at the breakpoint, indicative of precise breakpoint resolution.\n\n\n\n\n\nA repeat element deletion, as shown in IGV. A deletion of a repetitive element, which appears as a clean drop in coverage without evident mismatches or split reads. This is typical of repetitive regions where reads fail to align due to the absence of unique sequence anchors, making it harder to define precise breakpoints.\n\n\n\n\n\nInsertion, as viewed in IGV. The insertion is supported by split reads (multi-colored bars) and soft-clipped bases at the breakpoint, as well as a local region of increased coverage, suggesting additional sequence not present in the reference genome. The alignment pattern indicates that sequencing reads span across a novel insertion site, with some reads mapping partially on either side of the breakpoint and others failing to align due to sequence not present in the reference. These features are characteristic of insertions detected in short-read sequencing data.\n\n\n\n\n\nTandem duplication event, as shown in IGV. The duplicated region is marked by a dramatic increase in read coverage and a large number of discordant read pairs (red horizontal bars), which align abnormally across the duplication breakpoints. This pileup of overlapping reads and split alignments indicates the presence of a duplicated segment, typical of copy number gain events. The duplication spans approximately 4.4 kb, and the read alignment patterns suggest a complex rearrangement with possible breakpoint uncertainty.\n\n\n\n\nStructural variants and long read sequencing\nLong-read sequencing technologies, such as those from Oxford Nanopore and Pacific Biosciences, enable the sequencing of single DNA molecules spanning 10–20 kilobases or more—lengths that routinely exceed the size of most transposable elements and common repeats. This makes long-read platforms particularly powerful for structural variant (SV) detection, as individual reads can often span entire SVs, including both breakpoints, allowing for highly accurate identification and characterization of complex rearrangements that short reads often miss. However, these benefits come with new challenges: long reads typically have higher error rates (ranging from 5% to 20%), which can introduce alignment artifacts and complicate downstream variant calling. As a result, specialized alignment algorithms and error correction tools are required to fully harness the potential of long-read data for SV analysis.\n\n\nNGMLR (Next-Generation Mapping and Long Read aligner)\nNGMLR (Next-Generation Mapping and Long Read aligner) is a specialized aligner designed to accurately map long, error-prone reads—such as those from PacBio and Oxford Nanopore platforms—while preserving structural variant (SV) signals. Unlike traditional aligners, NGMLR uses a convex gap-cost scoring model, which penalizes the extension of long insertions and deletions less severely than short ones. This scoring approach is well-suited to long-read data because it reduces the likelihood of breaking large SVs into multiple small errors, thereby improving alignment accuracy across large structural events such as insertions, deletions, and inversions. NGMLR has been shown to improve the sensitivity and precision of SV detection when paired with long-read SV callers like Sniffles.\nReference: Sedlazeck FJ, Rescheneder P, Smolka M, Fang H, Nattestad M, von Haeseler A, Schatz MC. (2018). Accurate detection of complex structural variations using single-molecule sequencing. Nature Methods, 15(6), 461–468. https://doi.org/10.1038/s41592-018-0001-7\n\n\n\nBwa mem and NGMLR. In both SV types, NGMLR clearly produces cleaner and more interpretable alignments. For the deletion, BWA-MEM results in a fragmented alignment with a large unaligned gap and scattered mismatches, whereas NGMLR shows continuous reads spanning the breakpoint with characteristic split-read signals, allowing clear delineation of the event. Similarly, in the inversion panel, BWA-MEM displays heavy mismatching and alignment ambiguity, while NGMLR provides a more structured alignment with clear orientation changes that indicate the inversion boundaries. This highlights NGMLR’s superior ability to handle large indels and complex structural variants, thanks to its convex gap scoring model.\n\n\n\n\nSniffles\nSniffles is a structural variant (SV) caller specifically designed to detect SVs from long-read sequencing data generated by platforms such as PacBio and Oxford Nanopore. It leverages long-read alignments to accurately identify a wide range of SV types, including insertions, deletions, inversions, duplications, and translocations. Sniffles uses a multi-evidence approach, integrating split-read, read pair, and read depth signals to increase the accuracy of SV detection, even in repetitive or complex regions. It is widely used in long-read genomics pipelines due to its speed, scalability, and high sensitivity.\nReference: Sedlazeck FJ, Rescheneder P, Smolka M, et al. (2018). Accurate detection of complex structural variations using single-molecule sequencing. Nature Methods, 15(6), 461–468. https://doi.org/10.1038/s41592-018-0001-7\n\n\n\nComplex SVs. Long-read (NGMLR) and short-read (Illumina, aligned with BWA-MEM) sequencing alignments over a 3,078 bp inversion. In the NGMLR track, long reads span the entire inversion, allowing clear detection through split alignments and changes in orientation (shown by color changes across reads), which directly reveal the inverted sequence boundaries. The ability of long reads to map across complex regions results in clean, high-confidence SV evidence. In contrast, the Illumina reads below show increased mapping ambiguity, incomplete coverage, and clusters of mismatches at the inversion breakpoints. Due to the short length of reads and the repetitive nature of the inverted region, short-read alignments fail to span the inversion, making it difficult to resolve the event accurately.\n\n\n\n\nEvaluating structural variant (SV) calls\nEvaluating structural variant (SV) calls is essential to ensure the reliability of genomic analyses, and involves balancing sensitivity (true positive rate) and specificity (true negative rate). High-quality SV detection aims to minimize both false positives and false negatives. To assess sensitivity, researchers compare called SVs to a set of known or validated variants, often from benchmark datasets or previously characterized samples. For specificity, a subset of SV calls—typically chosen at random—can be experimentally validated using independent methods such as PCR or long-read sequencing. Receiver operating characteristic (ROC) curves are useful tools for visualizing the trade-off between sensitivity and specificity as SV calling parameters are adjusted, helping optimize performance and improve confidence in variant interpretation.",
    "crumbs": [
      "Home",
      "Structural Variation Calling",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module4/Notebooks/vcfs.html",
    "href": "course_modules/Module4/Notebooks/vcfs.html",
    "title": "Looking at Structural Variants in VCF",
    "section": "",
    "text": "Structural variants can be stored in VCF files. In this part of the tutorial, we will look at how these are represented in a VCF file.\nFirst, check you are in the correct directory:\npwd\nIt should display something like:\n/home/manager/course_data/structural_variation/data\nNavigate to the exercise1 directory:\ncd exercise1\nThere is a VCF file called ERR1015121.vcf that was produced using the Lumpy SV calling software. Look at the VCF file using the less command and answer the questions that follow:\nless ERR1015121.vcf"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/vcfs.html#excercises",
    "href": "course_modules/Module4/Notebooks/vcfs.html#excercises",
    "title": "Looking at Structural Variants in VCF",
    "section": "Excercises",
    "text": "Excercises\n\nWhat does the CIPOS format tag indicate?\nWhat does the PE tag indicate?\nWhat tag is used to describe an inversion event?\nWhat tag is used to describe a duplication event?\nHow many deletions were called in total? (Hint: DEL is the info field for a deletion. The -c option of the grep command can be used to return a count of matches.)\nWhat type of event is predicted at IV:437148? What is the length of the SV? How many paired-end reads and split-reads support this SV variant call?\nWhat is the total number of SV calls predicted on the IV chromosome?\n\nNow continue to the next section of the tutorial: Calling structural variants"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/long-reads.html",
    "href": "course_modules/Module4/Notebooks/long-reads.html",
    "title": "Calling Structural Variants from Long Reads",
    "section": "",
    "text": "In this part of the tutorial we will use long read data to identify structural variants using the SV caller Sniffles.\nFirst navigate to the exercise4 directory:\ncd ../exercise4\nls"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/long-reads.html#introducing-the-dataset",
    "href": "course_modules/Module4/Notebooks/long-reads.html#introducing-the-dataset",
    "title": "Calling Structural Variants from Long Reads",
    "section": "Introducing the dataset",
    "text": "Introducing the dataset\nWe will use data from a Saccharomyces cerevisiae strain (YPS128) that was sequenced at the Wellcome Sanger Institute and deposited in the ENA (Project: PRJEB7245, sample: SAMEA2757770, analysis: ERZ448241).\nThe sequencing reads are contained in a fastq file:\nYPS128.filtered_subreads.10x.fastq.gz\nThe reference genome is in the ../ref directory in a fasta file:\nSaccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/long-reads.html#align-the-data",
    "href": "course_modules/Module4/Notebooks/long-reads.html#align-the-data",
    "title": "Calling Structural Variants from Long Reads",
    "section": "Align the data",
    "text": "Align the data\nBefore you can use Sniffles to call SVs, it is very important that the reads are aligned with an aligner suitable for long reads.\nThe software minimap2 is a long-read aligner designed to align PacBio or Oxford Nanopore (standard and ultra-long) to a reference genome.\nYou can find the usage of minimap2 by typing:\n\nminimap2\n\nUse minimap2 to align the reads and send the output to a SAM file called YPS128.filtered_subreads.10x.sam\nNote: use the -t option to use multiple threads in parallel (this will increase the speed of the alignment by using more than one CPU core, I suggest using 2). Also look at the -x option.\nConvert the SAM file to BAM.\nSort the BAM file and produce a sorted BAM file called YPS128.filtered_subreads.10X.sorted.bam. Hint: Use samtools sort.\nUse samtools calmd to calculates MD and NM tags. This enables variant calling without requiring access to the entire original reference.\n\nsamtools calmd -b YPS128.filtered_subreads.10x.sorted.bam ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa &gt; YPS128.filtered_subreads.10x.fastq.sorted.calmd.bam\n\nFinally, use samtools to index this BAM file. Hint Use samtools index"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/long-reads.html#call-structural-variants",
    "href": "course_modules/Module4/Notebooks/long-reads.html#call-structural-variants",
    "title": "Calling Structural Variants from Long Reads",
    "section": "Call structural variants",
    "text": "Call structural variants\nSniffles is a structural variation (SV) caller that is designed for long reads (Pacbio or Oxford Nanopore). It detects all types of SVs (10bp+) using evidence from split-read alignments, high-mismatch regions, and coverage analysis. Sniffles takes the BAM file as input and outputs VCF.\nTo find the usage for Sniffles, type:\n\nsniffles\n\nUsing the default parameters, call SVs with Sniffles and output the results to a VCF file called YPS128.10x.vcf.\nHint: You don’t need to change any of the default parameters, but you will need to work out how to provide the input BAM file and specify the output VCF file. The documentation on sniffles is here : https://github.com/fritzsedlazeck/Sniffles/wiki/Parameter."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/long-reads.html#inspecting-svs-with-igv",
    "href": "course_modules/Module4/Notebooks/long-reads.html#inspecting-svs-with-igv",
    "title": "Calling Structural Variants from Long Reads",
    "section": "Inspecting SVs with IGV",
    "text": "Inspecting SVs with IGV\nOpen IGV:\n\nigv\n\nOpen the reference genome ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa and load the BAM file YPS128.filtered_subreads.10x.fastq.sorted.calmd.bam and the VCF file YPS128.filtered_subreads.10x.vcf. Now answer the questions that follow using either the command line or IGV."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/long-reads.html#exercises",
    "href": "course_modules/Module4/Notebooks/long-reads.html#exercises",
    "title": "Calling Structural Variants from Long Reads",
    "section": "Exercises",
    "text": "Exercises\n\nWhat sort of SV was called at on chromosome ‘XV’ at position 854272?\nWhat is the length of the SV?\nHow many reads are supporting the SV?\nWhat sort of SV was called at on chromosome ‘XI’ at position 74608?\nWhat is the length of the SV?\nHow many reads are supporting the SV?\nHow many inversions were called in the VCF? Note inversions are denoted by the type ‘INV’.\nHow many duplications were called in the VCF? Note duplications are denoted by the type ‘DUP’.\n\nNow continue to the next section of the tutorial: bedtools"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/bedtools.html",
    "href": "course_modules/Module4/Notebooks/bedtools.html",
    "title": "Bedtools",
    "section": "",
    "text": "Bedtools is an extremely useful tool for doing regional comparisons over genomic co-ordinates. It has many commands for doing region based comparisons with BAM, VCF, GFF, BED file formats.\nTo see the list of commands available, on the command line type:\nbedtools\nNavigate to the exercise5 directory.\ncd ../exercise5\nls\nIn this directory, there are two VCF files and the yeast genome annotation in GFF3 format Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/bedtools.html#bedtools-intersect",
    "href": "course_modules/Module4/Notebooks/bedtools.html#bedtools-intersect",
    "title": "Bedtools",
    "section": "bedtools intersect",
    "text": "bedtools intersect\nGiven two sets of genomic features, the bedtools intersect command can be used to determine whether or not any of the features in the two sets “overlap” with one another. For the intersect command, the -a and -b parameters are used to denote the input files A and B.\n\n\n\nIGV - main window\n\n\n(Credit to Aaron Quinlan for original source of figure: http://quinlanlab.org/tutorials/bedtools/bedtools.html)\nFor example, to find out the overlap between the SVs in ERR1015069.dels.vcf and the annotated region of the yeast genome try\nThis command reports the variant in the file ERR1015069.dels.vcf every time it overlaps with a feature in Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3. Therefore if a variant overlaps more than one feature it will be reported more than once. To report the unique set of variants use:\n\nbedtools intersect -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\n\n\nbedtools intersect -u -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\n\nThe default is to report overlaps between features in A and B so long as at least one base pair of overlap exists. However, the -f option allows you to specify what fraction of each feature in A should be overlapped by a feature in B before it is reported.\nTo specify a more strict intersect and require at least 25% of overlap of the SV with the genes use the command:\n\nbedtools intersect -u -f 0.25 -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\n\nThe bedtools intersect command can also be used to determine how many SVs overlap between two VCF files. For more information about bedtools intersect see the help:\n\nbedtools intersect -h\n\n\nExercises\n\nHow many SVs found in ERR1015069.dels.vcf overlap with a gene? (Hint: Use bedtools intersect command)\nHow many SVs found in ERR1015069.dels.vcf do not overlap with a gene? (Hint: note the -v parameter to bedtools intersect)\nHow many SVs found in ERR1015069.dels.vcf overlap with a more strict definition of 50%?\nHow many features does the deletion at VII:811446 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect.\nHow many features does the deletion at XII:650823 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/bedtools.html#bedtools-closest",
    "href": "course_modules/Module4/Notebooks/bedtools.html#bedtools-closest",
    "title": "Bedtools",
    "section": "bedtools closest",
    "text": "bedtools closest\nSimilar to intersect, bedtools closest searches for overlapping features in A and B. In the event that no feature in B overlaps the current feature in A, closest will report the nearest (that is, least genomic distance from the start or end of A) feature in B.\nAn example of the usage of bedtools closest is:\n\nbedtools closest -d -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\n\nThis command will list all the features in the file Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 that are closest to each of the variants in ERR1015069.dels.vcf.\nThe -d option means that in addition to the closest feature in Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3, the distance to the variant in ERR1015069.dels.vcf will be reported as an extra column. The reported distance for any overlapping features will be 0.\nFor example, to find the closest gene to the variant found at position 43018 on chromosome XV, try\n\nbedtools closest -d -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3| grep XV | grep 43018 \n\nFor more information about bedtools closest see the help:\n\nbedtools closest -h"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/bedtools.html#exercises-1",
    "href": "course_modules/Module4/Notebooks/bedtools.html#exercises-1",
    "title": "Bedtools",
    "section": "Exercises",
    "text": "Exercises\n\nWhat is the closest gene to the structural variant at IV:384220 in ERR1015069.dels.vcf?\nHow many SVs overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf?\nHow many SVs have a 50% reciprocal overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf (Hint: first find the option for reciprocal overlap by typing: bedtools intersect -h)\n\nCongratulations, you have reached the end of the tutorial."
  },
  {
    "objectID": "course_modules/Module3/module3_instructor_notes.html",
    "href": "course_modules/Module3/module3_instructor_notes.html",
    "title": "Instructor notes",
    "section": "",
    "text": "Clarify Jargon: The field has a lot of terminology (SNV vs SNP, mutation vs variant, etc.). Take time to clarify these. A tip for trainers is to gauge the audience’s background – if they seem lost in terminology, use simpler language or analogies (e.g., “think of the reference genome as a book and a variant as a typo or an extra sentence in your copy compared to the reference copy”).\nCommon Misconceptions: Address the idea that variant calling is always straightforward. For instance, explain why repetitive regions are hard for variant callers, or why a variant in only 2 out of 20 reads might be a false positive (or a low-frequency mosaic variant depending on context). Encourage questions like “what if two variant callers disagree?” to discuss reliability.\nInteractive Discussion: When covering variant annotation, rather than just lecturing, ask the group how they would prioritize variants if given thousands of results. This can lead into explaining annotation, databases, and predicting impact. It keeps learners engaged and thinking like a researcher.\nLink to Downstream Analysis: Note to instructors: highlight that variant calling is not the end – results must be filtered and interpreted. If there’s a subsequent module on interpretation or clinical genomics, mention that connection. If not, at least mention what one would do next (validation experiments, etc.),\nTrainer Tips\n\nWhen discussing mutation types, ask learners for examples of diseases or traits known to be caused by each type (e.g., a single nucleotide change causing sickle-cell anemia). This makes it more relatable.\nDuring variant calling theory, mention common misconceptions: “Learners may think all variants are found with 100% certainty. Emphasize the uncertainties and the need for quality scores/filters.",
    "crumbs": [
      "Home",
      "Variant calling",
      "Instructor notes"
    ]
  },
  {
    "objectID": "course_modules/Module3/module3_solutions.html",
    "href": "course_modules/Module3/module3_solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "Variant Calling - Solutions\nNo exercises in this section.\n\n\nPerforming variant calling\nQ1 66 reads\nQ2 The reference allele is A and the alternate allele is G. The upper/lower case letters indicate the forward/reverse orientation of the read.\nQ3 0 reads calling reference allelle and 66 reads calling the alternate allele\nQ4 Add the -v option to the command:\nbcftools mpileup -f GRCm38_68.19.fa A_J.bam | bcftools call -mv | less -S\n\n\nExercises\nQ1 The reference allele is A and the alternate allele is G.\nQ2 Look up the tag DP in the INFO column: there were 69 raw reads at the position.\nQ3 There are 0 reads calling the reference and 66 high-quality reads calling the alternate.\nQ4 An indel. Five bases TGTGG were inserted after the T at position 10003649\n\n\nVariant Filtering\nQ1 The complete command is:\nbcftools query -f'%POS %QUAL [%GT %AD] %REF %ALT\\n' -i'QUAL&gt;=30 && type=\"snp\" && AD[*:1]&gt;=25' out.vcf | head\nQ2 The complete command is:\nbcftools stats -i'QUAL&gt;=30 && AD[*:1]&gt;=25' out.vcf | grep TSTV | cut -f5\nQ3 The complete command is:\nbcftools stats -e'QUAL&gt;=30 && AD[*:1]&gt;=25' out.vcf | grep TSTV | cut -f5\nQ4 The complete command is:\nbcftools stats -i 'GT=\"het\"' out.vcf | grep TSTV | cut -f5\nQ5 The complete command is:\nbcftools norm -f GRCm38_68.19.fa out.flt.vcf -o out.flt.norm.vcf\n\n\nCalling Variants Across Multiple Samples\nQ1 Use the commands:\nbcftools mpileup -a AD -f GRCm38_68.19.fa *.bam -Ou | bcftools call -mv -Ob -o multi.bcf bcftools index multi.bcf\nQ2 Use the commands\nbcftools filter -s LowQual -i'QUAL&gt;=30 && AD[*:1]&gt;=25' -g8 -G10 multi.bcf -Ob -o multi.filt.bcf bcftools index multi.filt.bcf\nQ3 Use the commands:\nbcftools stats multi.filt.bcf | grep TSTV | cut -f5 (raw calls)\nbcftools stats -i 'FILTER=\"PASS\"' multi.filt.bcf | grep TSTV | cut -f5 (only filtered set)\nQ4 Use the commands:\nbcftools stats -e 'FILTER=\"PASS\"' multi.filt.bcf | grep TSTV | cut -f5\n\n\nVisualising Alignments\nQ1 75 in total, 39 on the forward and 36 on the reverse strand.\nQ2 Yes. Use the command:\nbcftools view -H -r 19:10001946 multi.filt.bcf\nQ3 Yes.\nQ4 Yes. Use the command:\nbcftools view -H -r 19:10072443 multi.filt.bcf\nQ5 No. It fails due to lowQual and snpGap, this means the call was removed by filtering because the quality of the call falls below the treshold set and the call is in close proximity to an indel.\nQ6 No. It is an alignment artefact, the aligner prefered two SNPs instead of a long deletion.\n\n\nVariant annotation\nQ1 Use the command:\nbcftools query -f '%BCSQ' -r 19:10088937 multi.filt.annot.bcf\nto return\nmissense|Fads2|ENSMUST00000025567|protein_coding|-|163V&gt;163I|10088937C&gt;T(base)\nQ2 A missense mutation\nQ3 The C&gt;T mutation changes the amino acid at position 163 in the protein sequence, from valine to isoleucine.",
    "crumbs": [
      "Home",
      "Variant calling",
      "Solutions"
    ]
  },
  {
    "objectID": "course_modules/Module3/module3_platform_guidance.html",
    "href": "course_modules/Module3/module3_platform_guidance.html",
    "title": "Platform guidance",
    "section": "",
    "text": "For the hands-on exercise, you will need a variant calling tool (such as bcftools or GATK) and a visualization tool (IGV) installed. These run on Linux/Mac, and Windows users can use WSL or a Docker container we provide.",
    "crumbs": [
      "Home",
      "Variant calling",
      "Platform guidance"
    ]
  },
  {
    "objectID": "course_modules/Module3/module3.html",
    "href": "course_modules/Module3/module3.html",
    "title": "Overview",
    "section": "",
    "text": "Module Title\nVariant calling\n\n\nAuthors\nContent by Qasim Ayub, qasim.ayub@monash.edu\n\n\nDuration\n3h\n\n\nKey topics\nIn this module, learners will look at\n\nDNA variation\nVariant calling basics and steps in variant detection\nCommon sources of error in variant calling\nPopulation level variant data, genomic context and clinical relevance of variants.\n\n\n\nLearning outcomes\nBy the end of this module, you should be able to:\n\nUnderstand various types of variation and how they are ascertained.\nUnderstand how variant calls are made.\nAssess variant quality and visualise variants.\nAnnotate variants and assess consequences.\n\n\n\nActivities\n\nLecture 0.5h\nPractical exercises 2h\nQuiz 0.5h\n\n\n\nManual and practical exercises\nModule manual\nExercises\nSolutions\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual)\n\n\nLecture notes or scripts\nPre-recorded lecture\n\n\nAssessment quiz\nQuestions\nAnswers\n\n\nSummary of this module\n\n\nTeaching guidance\nInstructor notes\n\n\nDatasets\nPractice files for this module can be found on Github.\n\n\nPlatform Guidance\nGuide",
    "crumbs": [
      "Home",
      "Variant calling",
      "Overview"
    ]
  },
  {
    "objectID": "course_modules/VM_guide/VM_Guide.html#introduction-to-virtual-box-and-virtual-machine",
    "href": "course_modules/VM_guide/VM_Guide.html#introduction-to-virtual-box-and-virtual-machine",
    "title": "Oracle VM VirtualBox Installation Guide",
    "section": "Introduction to Virtual Box and Virtual Machine",
    "text": "Introduction to Virtual Box and Virtual Machine\n\nConcept of VirtualBox and How It Works\nVirtualBox is a powerful open-source virtualization software that enables users to run multiple operating systems on a single physical machine. This concept of virtualization allows users to create a virtual environment, or Virtual Machine (VM), that emulates the hardware of a computer, enabling the installation of guest operating systems like Linux or Windows alongside the host operating system.\n\n\nHow VirtualBox Works\n\nVirtualization Layer: VirtualBox operates as a virtualization layer between the host operating system and the guest operating systems. It abstracts the hardware resources of the physical machine, allowing the VM to use the host’s CPU, memory, storage, and other peripherals without interference.\nGuest OS Installation: When you create a VM in VirtualBox, you allocate specific resources (such as CPU cores, RAM, and storage) to that VM. You can then install a guest operating system as you would on a physical machine. This guest OS thinks it’s running on its dedicated hardware, while in reality, it’s sharing the host’s resources.\nResource Allocation: VirtualBox allows for dynamic allocation of resources. For example, you can set the amount of RAM or number of CPU cores a VM can utilize. This flexibility helps optimize the performance of both the host and guest operating systems.\nIsolation: Each VM operates in its isolated environment. Changes made within a VM do not affect the host OS or other VMs running on the same machine. This isolation is beneficial for testing software, running applications in different environments, or experimenting with new configurations without risking the stability of the host system.\nSnapshots and Cloning: VirtualBox supports features like snapshots, which allow you to save the state of a VM at a specific point in time. This is useful for testing and development, as you can revert to a previous state if something goes wrong. Cloning allows you to create a copy of a VM for further experimentation.\nNetworking: VirtualBox provides various networking options, enabling VMs to communicate with the host, other VMs, or external networks. You can set up different network modes, such as NAT, bridged, or host-only, depending on your needs.\n\nOverall, VirtualBox offers a flexible and efficient way to run multiple operating systems on a single machine, making it an invaluable tool for developers, testers, and anyone who needs to use different environments.\nNote: The VM image file (.vdi) contains all the software installed for the course.",
    "crumbs": [
      "Home",
      "How to setup a VM",
      "Oracle VM VirtualBox Installation Guide"
    ]
  },
  {
    "objectID": "course_modules/VM_guide/VM_Guide.html#host-operating-system-os-requirements-for-virtual-box",
    "href": "course_modules/VM_guide/VM_Guide.html#host-operating-system-os-requirements-for-virtual-box",
    "title": "Oracle VM VirtualBox Installation Guide",
    "section": "Host Operating System (OS) Requirements for Virtual Box",
    "text": "Host Operating System (OS) Requirements for Virtual Box\n\nRAM requirement: 8GB (preferably 12GB)\nProcessor requirement: 4 processors (preferably 8)\nHard disk space: 200GB\nAdmin rights to the computer.\n\nNote: Before installing VirtualBox, ensure that the Microsoft Visual C++ 2019 Redistributable Package is installed on your Windows computer. Some applications, including VirtualBox, require this package to function correctly.",
    "crumbs": [
      "Home",
      "How to setup a VM",
      "Oracle VM VirtualBox Installation Guide"
    ]
  },
  {
    "objectID": "course_modules/VM_guide/VM_Guide.html#installation-of-virtual-box-for-windows-os",
    "href": "course_modules/VM_guide/VM_Guide.html#installation-of-virtual-box-for-windows-os",
    "title": "Oracle VM VirtualBox Installation Guide",
    "section": "Installation of Virtual Box for Windows OS",
    "text": "Installation of Virtual Box for Windows OS\n\nInstalling Microsoft Visual C++ 2019 Redistributable Package (if needed)\n\nDownload the Package:\n\nVisit the official Microsoft download page for the Visual C++ Redistributable: Download Visual C++ Redistributable.\nScroll down to the “Visual Studio 2019” section and choose the appropriate version for your system.\n\nChoose the Right Version:\n\nx86: For 32-bit operating systems or if you need a 32-bit application on a 64-bit OS.\nx64: For 64-bit operating systems and applications.\n\nRun the Installer:\n\nLocate the downloaded installer file (named like vc_redist.x64.exe or vc_redist.x86.exe).\nDouble-click the installer to run it.\n\nAccept the License Agreement:\n\nRead and accept the license terms, then click on “Install.”\n\nComplete the Installation:\n\nWait for the installation to finish, then click “Close.”\n\nRestart Your Computer (If Necessary):\n\nIf prompted, restart your computer to apply the changes.\n\n\n\n\nVerification\nTo verify that the Visual C++ 2019 Redistributable is installed: - Open Control Panel &gt; Programs &gt; Programs and Features. - Look for “Microsoft Visual C++ 2019 Redistributable” in the list.\nBy ensuring that the Visual C++ 2019 Redistributable Package is installed, you can help prevent compatibility issues with applications that rely on these runtime components.\n\n\nSteps on Downloading VirtualBox\n\nDownload VirtualBox: Navigate to the official VirtualBox website and go to the “Downloads” section. Choose the version that matches your Windows operating system (32-bit or 64-bit).\n\n\n\nDownload VirtualBox\n\n\nDownload Extension Pack (Optional): If needed, download the VirtualBox Extension Pack from the same “Downloads” section. This pack provides additional functionalities like USB 2.0 and 3.0 support, VirtualBox Remote Desktop Protocol (VRDP), and more.\nRun the Installer: Locate the downloaded VirtualBox installer file (.exe) and double-click to run it. Follow the on-screen instructions provided by the installer. Click “Next” to proceed through the setup wizard. You may customize installation options if desired.\nInstall Extension Pack (Optional): If you downloaded the Extension Pack, double-click on the file (e.g., Oracle_VM_VirtualBox_Extension_Pack-6.0.14-133895.vbox-extpack) to install it. This can be done after VirtualBox installation.\nComplete the Installation: Once the installation is complete, click “Finish” to exit the installer.",
    "crumbs": [
      "Home",
      "How to setup a VM",
      "Oracle VM VirtualBox Installation Guide"
    ]
  },
  {
    "objectID": "course_modules/VM_guide/VM_Guide.html#installation-of-virtual-box-for-macos",
    "href": "course_modules/VM_guide/VM_Guide.html#installation-of-virtual-box-for-macos",
    "title": "Oracle VM VirtualBox Installation Guide",
    "section": "Installation of Virtual Box for MacOS",
    "text": "Installation of Virtual Box for MacOS\n\nCheck if Your Mac is Intel or ARM-based\nTo check whether your Mac uses an Intel chip or an ARM-based (M1/M2/M3) chip:\n\nClick on the Apple logo in the top-left corner of your screen.\nSelect “About This Mac.”\nIn the “Overview” tab, look for the Processor section:\n\nIf it says Intel, your Mac uses an Intel processor.\nIf it mentions Apple Silicon, your Mac uses an ARM-based chip (M1/M2/M3).\n\n\nThis information will guide you on the compatibility of VirtualBox with your system.\n\n\nSteps on Downloading VirtualBox\n\nDownload VirtualBox: Navigate to the official VirtualBox website and go to the “Downloads” section. Choose the version compatible with macOS.\n\n\n\nDownload VirtualBox for macOS\n\n\nDownload Extension Pack (Optional): If needed, download the VirtualBox Extension Pack from the same “Downloads” section. This pack provides additional functionalities like USB 2.0 and 3.0 support, VirtualBox Remote Desktop Protocol (VRDP), and more.\nRun the Installer: Locate the downloaded VirtualBox installer file (.dmg) and double-click to open it. Follow the on-screen instructions to install VirtualBox on your macOS. Drag the VirtualBox icon to the Applications folder.\nInstall Extension Pack (Optional): If you downloaded the Extension Pack, double-click on the file (e.g., Oracle_VM_VirtualBox_Extension_Pack-6.0.14-133895.vbox-extpack) to install it. This can be done after VirtualBox installation.\nComplete the Installation: Once installed, open VirtualBox from Applications folder. The first time you run it, macOS may ask for permission to run the application. Click “Open” to proceed.",
    "crumbs": [
      "Home",
      "How to setup a VM",
      "Oracle VM VirtualBox Installation Guide"
    ]
  },
  {
    "objectID": "course_modules/VM_guide/VM_Guide.html#downloading-virtual-machine-vm-image-via-globus",
    "href": "course_modules/VM_guide/VM_Guide.html#downloading-virtual-machine-vm-image-via-globus",
    "title": "Oracle VM VirtualBox Installation Guide",
    "section": "Downloading Virtual Machine (VM) Image via Globus",
    "text": "Downloading Virtual Machine (VM) Image via Globus\nThe Virtual Machine (VM) is a large download that many users find challenging. To address this, we use Globus software to facilitate the process. Globus is a research-oriented file transfer tool that offers the advantage of resuming downloads from the same position in case of an internet connection failure, instead of starting over.\n\nGlobus Website: https://www.globus.org",
    "crumbs": [
      "Home",
      "How to setup a VM",
      "Oracle VM VirtualBox Installation Guide"
    ]
  },
  {
    "objectID": "course_modules/VM_guide/VM_Guide.html#globus-vm-downloading-steps",
    "href": "course_modules/VM_guide/VM_Guide.html#globus-vm-downloading-steps",
    "title": "Oracle VM VirtualBox Installation Guide",
    "section": "Globus VM Downloading Steps:",
    "text": "Globus VM Downloading Steps:\n\nCreate a Globus Connect Personal Account:\n\nVisit Globus Connect Personal and select the download for your operating system (Mac or Windows). This will prompt you to create an account.\nChoose to use Globus ID to sign in. (picture below)\n\n\n\n\nDownload VirtualBox\n\n\n\nIf you don’t have an ID yet, select “Need a Globus ID? Sign up.”\n\n\n\n\nDownload VirtualBox\n\n\n\nIMPORTANT: Make sure to specify that it’s for research or educational purposes and create your account. Remember your password for later steps.\n\n\n\n\nDownload VirtualBox\n\n\nDownload the Globus Client:\n\nDownload the Globus client onto your local machine (or where you intend to run the VM) and allow it to install.\nIt will ask for a collection name; give it a name you’ll remember, like “home_computer” or “local_mac.” This name refers to the local folders on your computer where we will send the VM in a later step.\n\nOpen Globus File Manager:\n\nClick on the small “g” icon on the taskbar and select “Web: Transfer Files.”\nFor Linux users: There may not be a shortcut. Start Globus Personal Connect via the command line and navigate to https://app.globus.org/file-manager to begin the file manager.\n\nFind Your Files:\n\nYour local endpoint is your computer. Click on “Collections” on the left.\n\nSearch for WCS Endpoint:\n\nSearch for the endpoint wcs_data_transfers.\nClick on the endpoint labeled wcs_data_transfers and select “Open in File Manager.”\n\n\n\n\nDownload VirtualBox\n\n\nTransfer the VM:\n\nSelect the VM file CourseName_Year.vdi.gz with the checkbox, then click on “Transfer or sync to.”\nIn the opposite panel, click on the search box.\n\n\n\nDownload VirtualBox\n\n\n\nChoose Local Endpoint:\n\nClick on “Your Collections” and select your local endpoint (the name will be what you gave it during the Globus Personal Connect installation).\nYou can also browse to choose the specific directory or folder on your local machine where you want the VM to be downloaded.\n\n\n\nDownload VirtualBox\n\n\n\nStart the Download:\n\nOnce you have chosen the local location, click on the “Start” button under the wcs_data_transfers section to download to your local endpoint.\n \n\nWait for Completion:\n\nWait for the download to complete. Globus will email you once it’s done, and you can track the transfer in the “Activity” menu.\n\nInstall VirtualBox and VM:\n\nRun the installation of VirtualBox, then install the virtual machine you just downloaded.\n\n\nNote: The course data is also uploaded on Globus and can be searched as CourseName_Year.vdi.gz.",
    "crumbs": [
      "Home",
      "How to setup a VM",
      "Oracle VM VirtualBox Installation Guide"
    ]
  },
  {
    "objectID": "course_modules/VM_guide/VM_Guide.html#additional-resources-and-troubleshooting-for-globus",
    "href": "course_modules/VM_guide/VM_Guide.html#additional-resources-and-troubleshooting-for-globus",
    "title": "Oracle VM VirtualBox Installation Guide",
    "section": "Additional Resources and Troubleshooting for Globus",
    "text": "Additional Resources and Troubleshooting for Globus\n\nGlobus Documentation: https://docs.globus.org/guides/\nVirtual Box Documentation: https://www.virtualbox.org/wiki/Documentation\nVirtual Box Manual: https://www.virtualbox.org/manual/ch01.html\nVirtual Box Forum: https://forums.virtualbox.org/index.php (helpful for finding similar problem queries and solutions)\nStack Overflow: https://stackoverflow.com/ (Public Q&A platform for debugging)\nBioinformatics (BioStars) Forum: https://www.biostars.org/t/Forum/ (General Bioinformatics queries)",
    "crumbs": [
      "Home",
      "How to setup a VM",
      "Oracle VM VirtualBox Installation Guide"
    ]
  },
  {
    "objectID": "course_modules/VM_guide/VM_Guide.html#installing-a-virtual-machine-vm-image",
    "href": "course_modules/VM_guide/VM_Guide.html#installing-a-virtual-machine-vm-image",
    "title": "Oracle VM VirtualBox Installation Guide",
    "section": "Installing a Virtual Machine (VM) Image",
    "text": "Installing a Virtual Machine (VM) Image\nA Virtual Machine image (with a .vdi extension) replicates the exact VM used during the course. Downloading and installing the VM image saves time, as it contains all the necessary software and exercise data. If you’re unable to find certain data, it should be available in the course GitHub repository.\nNote: While the VM image file contains all the software installed for the course, admin rights are still required to run some sudo commands.\n\nSteps to Install the Virtual Machine Image:\n\nStart VirtualBox:\n\nLocate the VirtualBox shortcut on your desktop or find it in the Start Menu. Double-click the icon to launch the application.\n\nCreate a Virtual Machine:\n\nIn the VirtualBox Manager, click “New” to create a new virtual machine.\nEnter a name for your virtual machine (e.g., “CourseName 2024”).\nChoose “Linux” as the type and select “Ubuntu 64-bit” as the version. Please Note: The vdi files are stored as CourseName_Year.vdi.gz. For example, we are here looking at GCM24 course.\n\n\n\n\nDownload VirtualBox\n\n\nMemory (RAM) Allocation:\n\nDetermine the amount of RAM to allocate, keeping it close to the top of the green section on a PC.\nAdjust the number of processors, ideally half of the available ones. Click Next when settings are configured.\n\n\n\n\nDownload VirtualBox\n\n\nHard Disk:\n\nIndicate the location of the virtual machine file you downloaded.\nChoose “Use an existing virtual hard disk file,” click the icon next to the menu, and add the .vdi file.\nConfirm your selection in the summary window and click Finish.\n\n\n\n\nDownload VirtualBox\n\n\nFollow Ubuntu Installation Wizard:\n\nFollow the on-screen instructions to install Ubuntu. Choose language, keyboard layout, and select “Install Ubuntu.”\nFollow the prompts for time zone, user account, and installation type.\n\nComplete Installation:\n\nAllow the installation process to complete. VirtualBox might prompt you to restart the virtual machine.\nDouble-check your choices in the confirmation window. Once satisfied, start the virtual machine by highlighting its name and clicking the “Start” icon in the manager window.\n\nAdjust Screen Size:\n\nThe screen resolution should automatically adjust to match your VirtualBox window size.\nIf it doesn’t, manually adjust the screen size in the Ubuntu VM:\n\nGo to “Settings” &gt; “Displays” in the Ubuntu system settings.\nChoose the desired resolution that fits your VirtualBox window.\n\n\n\n\n\nDownload VirtualBox\n\n\nLogin Ubuntu:\n\nThe virtual machine will go through a boot process. After a short time, a window will appear.\nNote: The user account is named manager, and the password, if required, is also manager.",
    "crumbs": [
      "Home",
      "How to setup a VM",
      "Oracle VM VirtualBox Installation Guide"
    ]
  },
  {
    "objectID": "course_modules/VM_guide/VM_Guide.html#running-and-managing-vms-optional",
    "href": "course_modules/VM_guide/VM_Guide.html#running-and-managing-vms-optional",
    "title": "Oracle VM VirtualBox Installation Guide",
    "section": "Running and Managing VMs (Optional)",
    "text": "Running and Managing VMs (Optional)\nAdjusting the screen size in an Ubuntu virtual machine (VM) within VirtualBox involves installing and configuring the VirtualBox Guest Additions. By installing VirtualBox Guest Additions, you enable features like automatic screen resizing, improved graphics performance, and seamless mouse integration between your host machine and the Ubuntu VM.\n\nSteps to Adjust Screen Size:\n\nStart Ubuntu VM:\n\nEnsure that your Ubuntu VM is running.\n\nInsert Guest Additions CD:\n\nIn the VirtualBox menu, go to “Devices” and choose “Insert Guest Additions CD image.”\nThis action virtually inserts the Guest Additions CD into your Ubuntu VM.\n\nOpen Terminal:\n\nOpen a terminal window in Ubuntu by pressing Ctrl + Alt + T or using the application launcher.\n\nNavigate to the CD Directory:\n\nChange to the directory where the Guest Additions CD is mounted, typically located in the /media directory.\nUse the following command to navigate to the directory (Note: The directory name may vary based on your VirtualBox version):\ncd /media/username/VBox_GAs_6.1.26\n\nRun Guest Additions Installer:\n\nRun the Guest Additions installer by entering the following command in the terminal:\nsudo sh VBoxLinuxAdditions.run\nNote: You may be prompted to enter your password.\n\nFollow Installation Wizard:\n\nThe Guest Additions installer will launch an installation wizard. Follow the prompts to complete the installation.\n\nReboot Ubuntu VM:\n\nAfter the installation is complete, it’s recommended to reboot your Ubuntu VM to apply the changes.\n\nAdjust Screen Size:\n\nOnce the VM has restarted, the screen resolution should automatically adjust to match your VirtualBox window size.\nIf it doesn’t, you can manually adjust the screen size in the Ubuntu VM:\n\nGo to “Settings” &gt; “Displays” in the Ubuntu system settings.\nChoose the desired resolution that fits your VirtualBox window.\n\n\n\n\nDownload VirtualBox\n\n\n\nVerify Changes:\n\nConfirm that the screen resolution has changed and suits your preferences.\n\nManage VM Settings:\n\nIn the VirtualBox Manager, manage VM settings by selecting the VM and clicking on “Settings.” Here, you can adjust parameters such as RAM allocation, processors, and storage.\n\nSnapshot and Clone:\n\nVirtualBox allows you to take snapshots of your VM at different states, providing a backup mechanism. You can also clone VMs for testing or development purposes.\n\nShut Down and Save State:\n\nProperly shut down your VM when finished, either by choosing “Shut Down” from within the guest OS or by selecting the VM in the VirtualBox Manager and clicking the “Close” button.\nOptionally, you can save the machine state to resume exactly where you left off.\n\nBeware: Saving the machine state to resume is not very robust; there is a high chance VirtualBox might crash or not resume from the same point. It is advised to save all files and then “shut down” the VM.\n\n\n\nAdditional Resources and Troubleshooting for VirtualBox\n\nVirtualBox Documentation: https://www.virtualbox.org/wiki/Documentation\nVirtualBox Manual: https://www.virtualbox.org/manual/ch01.html\nVirtualBox Forum: https://forums.virtualbox.org/index.php (helpful for finding similar problem queries and solutions)\nUbuntu Documentation: https://help.ubuntu.com/\nUbuntu Community Support: https://ubuntu.com/support/community-support\nStack Overflow: https://stackoverflow.com/ (Public Q&A platform for debugging)\nBioinformatics (BioStars) Forum: https://www.biostars.org/t/Forum/ (General Bioinformatics queries)\n\n\nWellcome Connecting Science GitHub Home Page\nFor more information or queries, feel free to contact us via the Wellcome Connecting Science website. Find us on socials Wellcome Connecting Science Linktr",
    "crumbs": [
      "Home",
      "How to setup a VM",
      "Oracle VM VirtualBox Installation Guide"
    ]
  },
  {
    "objectID": "course_modules/Module2/module2_platform_guidance.html",
    "href": "course_modules/Module2/module2_platform_guidance.html",
    "title": "Platform guidance",
    "section": "",
    "text": "This exercise can be done on any Unix-like environment (Linux, macOS, or Windows 10+ with WSL). Ensure BWA and Samtools are installed.\nAlternatively, use the provided Docker image or a cloud-based environment (e.g., a pre-configured GitHub Codespace) which has these tools pre-installed.\nIGV is a Java-based tool available on all major OS; ensure learners have it installed or use IGV-Web if installation is an issue.",
    "crumbs": [
      "Home",
      "Read alignment",
      "Platform guidance"
    ]
  },
  {
    "objectID": "course_modules/Module2/module2_manual.html",
    "href": "course_modules/Module2/module2_manual.html",
    "title": "Manual",
    "section": "",
    "text": "This module covers how raw sequencing reads are aligned to a reference genome using modern tools and algorithms\nSequence alignment in NGS is the process of determining the most likely source of the observed DNA sequencing read within the reference genome sequence.\n\n\n\nReference genome\n\n\nA reference genome is a digital version of a species’ DNA used as a standard for comparison. When scientists sequence DNA from a person or organism, they compare the pieces they get (called “reads”) to this reference to figure out where they come from and what they might mean. The reference is stored as a big file (like GRCh38.fa for humans) that contains the full DNA sequence of that species. \nFor DNA, aligning short reads to a reference allows researchers to identify variations, such as single nucleotide polymorphisms (SNPs), by comparing the sequence of an individual’s DNA to the standard genome. This helps detect mutations or inherited traits. In RNA sequencing (RNA-seq), alignment reveals which genes are being expressed and how much, by mapping RNA reads back to the genome or transcriptome. This makes it possible to measure transcript abundance and study gene activity under different conditions. Overall, the reference genome acts like a scaffold that gives meaning to raw sequencing data.\nAlthough sequencing technologies have advanced dramatically, the core principles of sequence alignment have remained largely unchanged since the 1980s. Tools like BLAST (Basic Local Alignment Search Tool) introduced the foundational “seed and extend” strategy, where short matching subsequences (typically ~10 bp) from a query are used to identify candidate regions in a larger database, which are then extended to find high-scoring alignments. In the context of next-generation sequencing (NGS), alignment is performed at the nucleotide level, often between closely related sequences (e.g., human-to-human or microbial strains), which allows algorithms to assume a limited number of mismatches and optimize performance accordingly. Alignment tools also distinguish between gapped and ungapped approaches, with gapped alignment being essential for detecting insertions and deletions (indels). While the methods are conceptually similar to early tools, NGS has vastly increased the scale and complexity of alignment, requiring highly efficient algorithms to process millions to billions of short reads rapidly.\n\n\n\n\n\nHash table-based alignment\n\n\nHash Table-Based Alignment is a common approach in next-generation sequencing for mapping short reads to a reference genome. It relies on indexing short, fixed-length nucleotide sequences called k-mers (e.g., a 31-mer is a string of 31 nucleotides). The algorithm typically begins by building a hash table index of all possible k-mers in the reference genome along with their genomic positions—a process that can generate several gigabytes of data for the human genome. During alignment, each sequencing read is broken into k-mers, and the index is used to rapidly find candidate mapping positions (the seed phase). The location with the most matching k-mers is selected, and a Smith-Waterman alignment is then performed to fine-tune the placement of the read. The final alignments are typically saved in formats like BAM. Tools differ in whether they hash the reads or the reference: MAQ, ELAND, ZOOM, and SHRiMP hash the reads (offering flexibility but variable memory usage), while SOAP, BFAST, and MOSAIK hash the reference, providing more consistent memory usage.\n\n\n\nSuffix/Prefix Tree-Based Aligners use specialized data structures to enable rapid and efficient string matching during sequence alignment. A common structure is the suffix tree (or simply tree), which stores all possible suffixes (or prefixes) of a reference genome, allowing for fast lookup of sequence patterns. Modern implementations typically rely on a more compact and memory-efficient structure known as the FM-index, which is built on the Burrows-Wheeler Transform (BWT). FM-index-based aligners significantly reduce memory usage while maintaining high alignment speed. Tools such as MUMmer, BWA, and Bowtie use these methods to quickly identify candidate mapping locations. However, they still require a final local alignment step (e.g., Smith-Waterman) to accurately map reads and account for mismatches and small insertions or deletions.\n\n\n\n\n\n\nThe Smith Waterman algorithm\n\n\nLocal Alignment using the Smith-Waterman Algorithm (developed in 1981) is a crucial step in refining read placement on a reference genome. Once a rough location for a read is identified through seeding or indexing methods, Smith-Waterman is used to compute the optimal pairwise alignment between the read and the reference in that region. This alignment helps determine the precise structure of the match, including the CIGAR string, which encodes matches, mismatches, insertions, and deletions. While highly accurate, the algorithm is computationally intensive, so it is typically applied only to a subset of reads—those that cannot be exactly matched during the initial mapping. This step is particularly important for accurately detecting small insertions and deletions (indels) that simpler matching approaches may miss.\n\n\n\nTerminology\n\n\nThe fragment length refers to the full DNA segment between the sequencing adapters, while the insert size is the distance between the start of Read 1 and the end of Read 2, excluding adapters. The inner distance is the part of the insert not covered by either read. When the insert size is larger than the combined length of both reads, Read 1 and Read 2 do not overlap (top example). When the insert size is smaller, the reads can overlap or even fully span the fragment, causing adapter sequences to appear within the reads (examples A and B). Regardless of layout, both reads in a pair share the same name in sequence files, which allows them to be associated during alignment and analysis.\n\n\n\n\nFragment Length: The total length of the DNA molecule from adapter to adapter.\nInsert Size: The distance between the start of Read 1 and the end of Read 2 (excluding adapter sequences).\nInner Distance: The portion of the insert that is not covered by either read; relevant in estimating library insert size.\nRead 1 / Read 2: The two ends of the same DNA fragment that are sequenced separately. Read 1 typically starts at the 5′ end of the fragment. Read 2 starts from the opposite end (3′).\nAdapter: Synthetic DNA sequences ligated to fragment ends to allow for sequencing; sometimes appear in reads when insert size is small.\nSame Name: Both reads in a pair receive the same identifier in FASTQ or BAM files, enabling correct pairing during alignment.\n\n\n\n\nMapping quality is a score that reflects how confident the aligner is that a sequencing read has been correctly placed on the reference genome. This is especially important because genomes contain many repetitive elements, such as transposable elements (which make up 40–50% of vertebrate genomes), low-complexity regions, and reference gaps or errors. These features can make it difficult to determine the true origin of a read. Mapping quality is usually expressed as a Phred-scaled score, where higher values indicate greater confidence: for example, Q10 means a 1 in 10 chance of incorrect alignment (90% accuracy), Q20 means 1 in 100 (99%), and Q30 means 1 in 1000 (99.9%). A score of Q0 indicates no confidence at all. Paired-end sequencing improves mapping accuracy because if one read maps to a repetitive region and its mate maps to a unique sequence, the pair can still be confidently aligned, leading to a higher overall mapping quality. For this reason, paired-end reads are generally preferred for complex genomes.\n\n\n\nAlignment Limitations arise from both the characteristics of sequencing reads and the complexity of the genome being analyzed. Short reads are particularly challenging to align with confidence, especially in low-complexity genomes where repetitive or biased sequence content—such as the AT-rich genome of Plasmodium (malaria)—creates ambiguity. Alignment around insertions and deletions (indels) is another known issue: aligners may misplace reads near true indels, falsely calling single nucleotide polymorphisms (SNPs) due to scoring biases that favor mismatches over gaps. To address this, some tools perform a local realignment step on the BAM file to correct misalignments near indels. Additionally, high-density SNP regions can challenge seed-and-extend aligners, which often limit the number of mismatches allowed in the seed region (e.g., Maq allows at most two mismatches in the first 28 bases). Burrows-Wheeler Transform (BWT)-based aligners are more efficient but perform best with low sequence divergence, limiting their effectiveness for highly variable or divergent genomes.\n\n\n\n\n\nThe images above illustrate the challenges of aligning reads in regions with high sequence divergence and structural variation. Such areas can lead to poor mapping quality and inaccurate variant detection, highlighting the limitations of standard alignment tools when dealing with complex, strain-specific genomic differences.\n\n\n\nLong-read sequencing technologies, such as those from Oxford Nanopore and Pacific Biosciences, enable the sequencing of large DNA fragments—typically 10–20 kilobases in length. These long reads are especially useful for resolving repetitive regions and structural variants, as they often span entire transposable elements that short reads cannot. However, long-read sequencing introduces new challenges: the reads are more error-prone, with error rates up to 10%, making accurate alignment more difficult and requiring specialized algorithms for effective analysis.\n\nStructural variants, such as deletions and inversions, can cause challenges for short-read aligners like BWA-MEM. In the case of a deletion (left), reads fail to align properly across the missing region, often resulting in soft-clipping, mismatches, or gaps in coverage. For an inversion (right), alignment is disrupted due to the reversed orientation of the sequence, leading to a dense cluster of mismatches and misaligned bases.\n\n\n\n\n\n\nNGMLR\n\n\nNGMLR (Next-Generation Mapping and Long Read) is an aligner specifically designed for handling the unique challenges of long-read sequencing data. It uses a convex gap-cost scoring model, which means that the penalty for extending an insertion or deletion (indel) decreases as the indel gets longer. This approach is well-suited for long reads, which often contain large structural variants. Unlike traditional aligners that over-penalize long gaps, NGMLR is optimized to more accurately align reads containing large insertions, deletions, and complex rearrangements commonly found in long-read datasets.\n\n\n\n\n\n\nData production workflow\n\n\nThe Data production workflow begins with sequencing instruments generating raw FASTQ files from individual lanes or plexes. These reads are then aligned to a reference genome using tools like BWA or SMALT, producing BAM files that represent aligned read data. Next, the BAM improvement step enhances data quality through operations such as sorting, duplicate marking, and base quality recalibration. Improved BAMs are merged at the library level, combining data from multiple lanes of the same library, and then at the sample/platform level, integrating multiple libraries or sequencing runs from the same sample.\n\n\n\nLibrary duplicates are identical DNA fragments that arise during the PCR amplification step of library preparation in second-generation sequencing platforms, which are not single-molecule technologies. These duplicates can inflate read depth and potentially lead to false SNP calls, as they appear to support variants more strongly than they should. In high-quality libraries, duplicate rates are typically low—less than 5%. To address this, reads are first aligned to a reference genome, and duplicate read-pairs—those that map to the same outer coordinates—are identified and either removed or marked using tools such as Samtools (rmdup) or GATK’s MarkDuplicates. While PCR-free protocols can minimize duplicates, they require more input DNA, making them less feasible in some experimental contexts. Proper handling of duplicates is crucial for ensuring accurate variant calling and data interpretation.\n\n\n\n\n\n\nDuplicates and false SNPs\n\n\nIdentical or nearly identical sequences stack at the same genomic position, artificially inflating read depth. In the image above, the red box marks a site where duplicates introduce an apparent variant, potentially misinterpreted as a true SNP. Proper duplicate removal is essential to avoid such false-positive variant calls in downstream analysis.\n\n\n\nNext-generation sequencing (NGS) platforms like NextSeq can generate up to ~800 million reads or 160 Gbp per run, producing massive data volumes that are computationally intensive to process. Aligning a single lane of reads on one computer can take a long time, so parallel computing is used to speed up the workflow. One approach is to split the FASTQ files into smaller chunks (e.g., 1 Gbp), align them independently—each taking ~8 hours with BWA—and then merge the resulting BAM files using tools like samtools merge. Alternatively, users can leverage multiple CPU cores on a single machine by simply specifying the number of threads (e.g., bwa mem -t). Estimating IT requirements involves two main components: compute, which relates to CPU time needed for analysis (estimated at 20–40 CPU hours per Gbp), and storage, which can amount to 4–5 bytes per base pair sequenced when accounting for raw, processed, and backup data. For example, an experiment with 10 HiSeq lanes would require approximately 3 TB of storage and up to 24,000 CPU hours.",
    "crumbs": [
      "Home",
      "Read alignment",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module2/module2_manual.html#module-2-reads-alignment",
    "href": "course_modules/Module2/module2_manual.html#module-2-reads-alignment",
    "title": "Manual",
    "section": "",
    "text": "This module covers how raw sequencing reads are aligned to a reference genome using modern tools and algorithms\nSequence alignment in NGS is the process of determining the most likely source of the observed DNA sequencing read within the reference genome sequence.\n\n\n\nReference genome\n\n\nA reference genome is a digital version of a species’ DNA used as a standard for comparison. When scientists sequence DNA from a person or organism, they compare the pieces they get (called “reads”) to this reference to figure out where they come from and what they might mean. The reference is stored as a big file (like GRCh38.fa for humans) that contains the full DNA sequence of that species. \nFor DNA, aligning short reads to a reference allows researchers to identify variations, such as single nucleotide polymorphisms (SNPs), by comparing the sequence of an individual’s DNA to the standard genome. This helps detect mutations or inherited traits. In RNA sequencing (RNA-seq), alignment reveals which genes are being expressed and how much, by mapping RNA reads back to the genome or transcriptome. This makes it possible to measure transcript abundance and study gene activity under different conditions. Overall, the reference genome acts like a scaffold that gives meaning to raw sequencing data.\nAlthough sequencing technologies have advanced dramatically, the core principles of sequence alignment have remained largely unchanged since the 1980s. Tools like BLAST (Basic Local Alignment Search Tool) introduced the foundational “seed and extend” strategy, where short matching subsequences (typically ~10 bp) from a query are used to identify candidate regions in a larger database, which are then extended to find high-scoring alignments. In the context of next-generation sequencing (NGS), alignment is performed at the nucleotide level, often between closely related sequences (e.g., human-to-human or microbial strains), which allows algorithms to assume a limited number of mismatches and optimize performance accordingly. Alignment tools also distinguish between gapped and ungapped approaches, with gapped alignment being essential for detecting insertions and deletions (indels). While the methods are conceptually similar to early tools, NGS has vastly increased the scale and complexity of alignment, requiring highly efficient algorithms to process millions to billions of short reads rapidly.\n\n\n\n\n\nHash table-based alignment\n\n\nHash Table-Based Alignment is a common approach in next-generation sequencing for mapping short reads to a reference genome. It relies on indexing short, fixed-length nucleotide sequences called k-mers (e.g., a 31-mer is a string of 31 nucleotides). The algorithm typically begins by building a hash table index of all possible k-mers in the reference genome along with their genomic positions—a process that can generate several gigabytes of data for the human genome. During alignment, each sequencing read is broken into k-mers, and the index is used to rapidly find candidate mapping positions (the seed phase). The location with the most matching k-mers is selected, and a Smith-Waterman alignment is then performed to fine-tune the placement of the read. The final alignments are typically saved in formats like BAM. Tools differ in whether they hash the reads or the reference: MAQ, ELAND, ZOOM, and SHRiMP hash the reads (offering flexibility but variable memory usage), while SOAP, BFAST, and MOSAIK hash the reference, providing more consistent memory usage.\n\n\n\nSuffix/Prefix Tree-Based Aligners use specialized data structures to enable rapid and efficient string matching during sequence alignment. A common structure is the suffix tree (or simply tree), which stores all possible suffixes (or prefixes) of a reference genome, allowing for fast lookup of sequence patterns. Modern implementations typically rely on a more compact and memory-efficient structure known as the FM-index, which is built on the Burrows-Wheeler Transform (BWT). FM-index-based aligners significantly reduce memory usage while maintaining high alignment speed. Tools such as MUMmer, BWA, and Bowtie use these methods to quickly identify candidate mapping locations. However, they still require a final local alignment step (e.g., Smith-Waterman) to accurately map reads and account for mismatches and small insertions or deletions.\n\n\n\n\n\n\nThe Smith Waterman algorithm\n\n\nLocal Alignment using the Smith-Waterman Algorithm (developed in 1981) is a crucial step in refining read placement on a reference genome. Once a rough location for a read is identified through seeding or indexing methods, Smith-Waterman is used to compute the optimal pairwise alignment between the read and the reference in that region. This alignment helps determine the precise structure of the match, including the CIGAR string, which encodes matches, mismatches, insertions, and deletions. While highly accurate, the algorithm is computationally intensive, so it is typically applied only to a subset of reads—those that cannot be exactly matched during the initial mapping. This step is particularly important for accurately detecting small insertions and deletions (indels) that simpler matching approaches may miss.\n\n\n\nTerminology\n\n\nThe fragment length refers to the full DNA segment between the sequencing adapters, while the insert size is the distance between the start of Read 1 and the end of Read 2, excluding adapters. The inner distance is the part of the insert not covered by either read. When the insert size is larger than the combined length of both reads, Read 1 and Read 2 do not overlap (top example). When the insert size is smaller, the reads can overlap or even fully span the fragment, causing adapter sequences to appear within the reads (examples A and B). Regardless of layout, both reads in a pair share the same name in sequence files, which allows them to be associated during alignment and analysis.\n\n\n\n\nFragment Length: The total length of the DNA molecule from adapter to adapter.\nInsert Size: The distance between the start of Read 1 and the end of Read 2 (excluding adapter sequences).\nInner Distance: The portion of the insert that is not covered by either read; relevant in estimating library insert size.\nRead 1 / Read 2: The two ends of the same DNA fragment that are sequenced separately. Read 1 typically starts at the 5′ end of the fragment. Read 2 starts from the opposite end (3′).\nAdapter: Synthetic DNA sequences ligated to fragment ends to allow for sequencing; sometimes appear in reads when insert size is small.\nSame Name: Both reads in a pair receive the same identifier in FASTQ or BAM files, enabling correct pairing during alignment.\n\n\n\n\nMapping quality is a score that reflects how confident the aligner is that a sequencing read has been correctly placed on the reference genome. This is especially important because genomes contain many repetitive elements, such as transposable elements (which make up 40–50% of vertebrate genomes), low-complexity regions, and reference gaps or errors. These features can make it difficult to determine the true origin of a read. Mapping quality is usually expressed as a Phred-scaled score, where higher values indicate greater confidence: for example, Q10 means a 1 in 10 chance of incorrect alignment (90% accuracy), Q20 means 1 in 100 (99%), and Q30 means 1 in 1000 (99.9%). A score of Q0 indicates no confidence at all. Paired-end sequencing improves mapping accuracy because if one read maps to a repetitive region and its mate maps to a unique sequence, the pair can still be confidently aligned, leading to a higher overall mapping quality. For this reason, paired-end reads are generally preferred for complex genomes.\n\n\n\nAlignment Limitations arise from both the characteristics of sequencing reads and the complexity of the genome being analyzed. Short reads are particularly challenging to align with confidence, especially in low-complexity genomes where repetitive or biased sequence content—such as the AT-rich genome of Plasmodium (malaria)—creates ambiguity. Alignment around insertions and deletions (indels) is another known issue: aligners may misplace reads near true indels, falsely calling single nucleotide polymorphisms (SNPs) due to scoring biases that favor mismatches over gaps. To address this, some tools perform a local realignment step on the BAM file to correct misalignments near indels. Additionally, high-density SNP regions can challenge seed-and-extend aligners, which often limit the number of mismatches allowed in the seed region (e.g., Maq allows at most two mismatches in the first 28 bases). Burrows-Wheeler Transform (BWT)-based aligners are more efficient but perform best with low sequence divergence, limiting their effectiveness for highly variable or divergent genomes.\n\n\n\n\n\nThe images above illustrate the challenges of aligning reads in regions with high sequence divergence and structural variation. Such areas can lead to poor mapping quality and inaccurate variant detection, highlighting the limitations of standard alignment tools when dealing with complex, strain-specific genomic differences.\n\n\n\nLong-read sequencing technologies, such as those from Oxford Nanopore and Pacific Biosciences, enable the sequencing of large DNA fragments—typically 10–20 kilobases in length. These long reads are especially useful for resolving repetitive regions and structural variants, as they often span entire transposable elements that short reads cannot. However, long-read sequencing introduces new challenges: the reads are more error-prone, with error rates up to 10%, making accurate alignment more difficult and requiring specialized algorithms for effective analysis.\n\nStructural variants, such as deletions and inversions, can cause challenges for short-read aligners like BWA-MEM. In the case of a deletion (left), reads fail to align properly across the missing region, often resulting in soft-clipping, mismatches, or gaps in coverage. For an inversion (right), alignment is disrupted due to the reversed orientation of the sequence, leading to a dense cluster of mismatches and misaligned bases.\n\n\n\n\n\n\nNGMLR\n\n\nNGMLR (Next-Generation Mapping and Long Read) is an aligner specifically designed for handling the unique challenges of long-read sequencing data. It uses a convex gap-cost scoring model, which means that the penalty for extending an insertion or deletion (indel) decreases as the indel gets longer. This approach is well-suited for long reads, which often contain large structural variants. Unlike traditional aligners that over-penalize long gaps, NGMLR is optimized to more accurately align reads containing large insertions, deletions, and complex rearrangements commonly found in long-read datasets.\n\n\n\n\n\n\nData production workflow\n\n\nThe Data production workflow begins with sequencing instruments generating raw FASTQ files from individual lanes or plexes. These reads are then aligned to a reference genome using tools like BWA or SMALT, producing BAM files that represent aligned read data. Next, the BAM improvement step enhances data quality through operations such as sorting, duplicate marking, and base quality recalibration. Improved BAMs are merged at the library level, combining data from multiple lanes of the same library, and then at the sample/platform level, integrating multiple libraries or sequencing runs from the same sample.\n\n\n\nLibrary duplicates are identical DNA fragments that arise during the PCR amplification step of library preparation in second-generation sequencing platforms, which are not single-molecule technologies. These duplicates can inflate read depth and potentially lead to false SNP calls, as they appear to support variants more strongly than they should. In high-quality libraries, duplicate rates are typically low—less than 5%. To address this, reads are first aligned to a reference genome, and duplicate read-pairs—those that map to the same outer coordinates—are identified and either removed or marked using tools such as Samtools (rmdup) or GATK’s MarkDuplicates. While PCR-free protocols can minimize duplicates, they require more input DNA, making them less feasible in some experimental contexts. Proper handling of duplicates is crucial for ensuring accurate variant calling and data interpretation.\n\n\n\n\n\n\nDuplicates and false SNPs\n\n\nIdentical or nearly identical sequences stack at the same genomic position, artificially inflating read depth. In the image above, the red box marks a site where duplicates introduce an apparent variant, potentially misinterpreted as a true SNP. Proper duplicate removal is essential to avoid such false-positive variant calls in downstream analysis.\n\n\n\nNext-generation sequencing (NGS) platforms like NextSeq can generate up to ~800 million reads or 160 Gbp per run, producing massive data volumes that are computationally intensive to process. Aligning a single lane of reads on one computer can take a long time, so parallel computing is used to speed up the workflow. One approach is to split the FASTQ files into smaller chunks (e.g., 1 Gbp), align them independently—each taking ~8 hours with BWA—and then merge the resulting BAM files using tools like samtools merge. Alternatively, users can leverage multiple CPU cores on a single machine by simply specifying the number of threads (e.g., bwa mem -t). Estimating IT requirements involves two main components: compute, which relates to CPU time needed for analysis (estimated at 20–40 CPU hours per Gbp), and storage, which can amount to 4–5 bytes per base pair sequenced when accounting for raw, processed, and backup data. For example, an experiment with 10 HiSeq lanes would require approximately 3 TB of storage and up to 24,000 CPU hours.",
    "crumbs": [
      "Home",
      "Read alignment",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module2/module2.html",
    "href": "course_modules/Module2/module2.html",
    "title": "Overview",
    "section": "",
    "text": "Module Title\nRead alignment\n\n\nAuthors\nThis tutorial was written by Jacqui Keane based on material from Thomas Keane, Vivek Iyer and Victoria Offord.\n\n\nDuration\n4 hours\n\n\nKey topics\nIn this module, learners will look at:\n\nRead alignment theory and tools\nChallenges in alignment and quality assessment\nData processing and scalability\n\n\n\nLearning outcomes\nOn completion of the tutorial, you can expect to be able to:\n\nUnderstand the background and theory of read alignment\nPerform read alignment using standard tools (BWA-MEM)\nPerform the following task and understand their effect on analysis results (e.g. Mark Duplicates)\nVisualise read alignments using IGV (Integrated Visualisation Tool)\nMerge the results from multiple alignments and understand when it is appropriate to perform a merge\n\n\n\nActivities\n\nLecture: 1h\nPractical exercises: 2.5h\nQuiz: 0.5h\n\n\n\nManual and practical execises\nModule manual\nExercises\nSolutions\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual)\n\n\nLecture notes or scripts\nPre-recorded lecture\n\n\nCheck you knowledge quiz\nQuestions\n\n\nSummary of this module\n\n\nTeaching guidance\nInstructor notes\n\n\nDatasets\nPractice files for this module can be found on Github.\n\n\nPlatform Guidance\nGuide",
    "crumbs": [
      "Home",
      "Read alignment",
      "Overview"
    ]
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html",
    "href": "course_modules/Module5/module5_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Genome assembly is the process of taking a large number of fragments of DNA and putting them back together to create a representation of the original DNA sequence from which they originated.\n\n\n\nMany genomes contain large numbers of repeat sequences. Often these repeats are thousands of nucleotides long, and some occur in many different locations in the genome. This makes genome assembly a very difficult computational problem to solve. However, there are many genome assembly tools that exist that can produce long contiguous sequences (contigs) from sequencing reads. The assembly tool that you use will be determined by different factors, largely this will be the length of the sequencing reads and the sequencing technology used to produce the reads.\nIn this practical we will assemble one chromosome of a malaria parasite: Plasmodium falciparum, the IT clone. We have sequenced the genome with both PacBio and Illumina and pre-filtered the reads to select only those reads from a single chromosome.\n\n\n\nThis tutorial comprises the following sections:\n1. PacBio genome assembly\n2. Assembly algorithms\n3. Illumina genome assembly\n4. Genome assembly estimation\n5. PacBio genome assembly again\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal window and type the command below:\ncd ~/course_data/assembly/data\n\n\n\nThis tutorial requires that you have canu, jellyfish, velvet, assembly-stats and wtdbg installed on your computer. These are already installed on the virtual machine you are using. To check that these are installed, run the following commands:\ncanu\njellyfish\nvelvetg\nvelveth\nassembly-stats\nwtdbg2\nThis should return the help message for software canu, jellyfish, velvet, assembly-stats and wtdbg2 respectively.\nIf after this course you would like to download and install this software the instructions can be found at the links below, alternatively we recommend bioconda for the installation and management of your bioinformatics software.\n• The canu website\n•The jellyfish github page\n• The velvet website\n• The wtdbg2 github page\nTo get started with the tutorial, go to the first section: Pacbio genome assembly.",
    "crumbs": [
      "Home",
      "Genome Assembly",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html#genome-assembly",
    "href": "course_modules/Module5/module5_exercises.html#genome-assembly",
    "title": "Exercises",
    "section": "",
    "text": "Genome assembly is the process of taking a large number of fragments of DNA and putting them back together to create a representation of the original DNA sequence from which they originated.\n\n\n\nMany genomes contain large numbers of repeat sequences. Often these repeats are thousands of nucleotides long, and some occur in many different locations in the genome. This makes genome assembly a very difficult computational problem to solve. However, there are many genome assembly tools that exist that can produce long contiguous sequences (contigs) from sequencing reads. The assembly tool that you use will be determined by different factors, largely this will be the length of the sequencing reads and the sequencing technology used to produce the reads.\nIn this practical we will assemble one chromosome of a malaria parasite: Plasmodium falciparum, the IT clone. We have sequenced the genome with both PacBio and Illumina and pre-filtered the reads to select only those reads from a single chromosome.\n\n\n\nThis tutorial comprises the following sections:\n1. PacBio genome assembly\n2. Assembly algorithms\n3. Illumina genome assembly\n4. Genome assembly estimation\n5. PacBio genome assembly again\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal window and type the command below:\ncd ~/course_data/assembly/data\n\n\n\nThis tutorial requires that you have canu, jellyfish, velvet, assembly-stats and wtdbg installed on your computer. These are already installed on the virtual machine you are using. To check that these are installed, run the following commands:\ncanu\njellyfish\nvelvetg\nvelveth\nassembly-stats\nwtdbg2\nThis should return the help message for software canu, jellyfish, velvet, assembly-stats and wtdbg2 respectively.\nIf after this course you would like to download and install this software the instructions can be found at the links below, alternatively we recommend bioconda for the installation and management of your bioinformatics software.\n• The canu website\n•The jellyfish github page\n• The velvet website\n• The wtdbg2 github page\nTo get started with the tutorial, go to the first section: Pacbio genome assembly.",
    "crumbs": [
      "Home",
      "Genome Assembly",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html#pacbio-genome-assembly",
    "href": "course_modules/Module5/module5_exercises.html#pacbio-genome-assembly",
    "title": "Exercises",
    "section": "PacBio Genome Assembly",
    "text": "PacBio Genome Assembly\nFirst, check you are in the correct directory.\npwd\nIt should display something like:\n/home/manager/course_data/assembly/data\nNow we are going to start the PacBio assembly using the canu program. It first corrects the reads and then uses the Celera assembler to merge the long reads into contigs. To see the contents of the directory, type:\nls\nThe pre-filtered PacBio reads are called PBReads.fastq.gz - take a look at the contents of this file.\nzless -S PBReads.fastq.gz\nWhat do you notice compared to the Illumina fastq files you have seen previously?\nNow we will start the assembly with canu (https://canu.readthedocs.io/).\nNOTE: This will take some time, so we will start it running now in the background and hopefully it will complete while we work on the other exercises.\ncanu -p PB -d canu-assembly -s file.specs -pacbio-raw PBReads.fastq.gz &&gt;canu_log.txt &\nThe -p option sets the prefix of output files to PB, while the -d option sets the output directory to canu-assembly/. The & at the end will set this command running in the background while you work through the next sections of this practical.\nBefore we move on, let’s just make sure the program is running. Use the top or better htop command to show you all processes running on your machine (type q to exit top). You should hopefully see processes associated with canu running (or maybe something called meryl). We can also check the canu_log.txt file where the canu logs will be written. If we see error messages in the file, then something has gone wrong.\nhtop\nless canu_log.txt\nWhile the canu assembly is running, move on to the next section: Assembly algorithms.",
    "crumbs": [
      "Home",
      "Genome Assembly",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html#assembly-algorithms",
    "href": "course_modules/Module5/module5_exercises.html#assembly-algorithms",
    "title": "Exercises",
    "section": "Assembly algorithms",
    "text": "Assembly algorithms\nThere are several approaches (algorithms) that can be used to generate a set of contiguous sequences (contigs) from a set of DNA fragments (reads). Two of the main approaches are:\n• Overlap Layout Consensus (OLC): This approach looks at all pairs x,y of all reads and determines if there is a sufficient overlap of the two reads. If there is, then it bundles the stretches of overlap graphs into contigs. Examples of assembly software that use this approach are Falcon (PacBio), Canu(Pacbio, ONT), mnimap/miniasm.\n• de Brujin graph (DBG): This approach builds a graph of all subsequences of lenght k (k-mers).\nExamples of assembly software that use this approach are velvet, ABySS, SPAdes, wtdbg2.\n\nExercise\nHere we are going to build a de Brujin graph by hand from a set of short reads! Using the 6bp reads listed below, manually create the de Brujin graph and find the contig(s).\nUsing a k-mer value of k=5 produces the following k-mers of length 5 from the reads above. To finish the graph, join k-mers that overlap by 4 bases.\nQuestions:\n1. What is the contig sequence?\n2. What was difficult here?\nNow move on to the next section: Illumina genome assembly",
    "crumbs": [
      "Home",
      "Genome Assembly",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html#illumina-genome-assembly",
    "href": "course_modules/Module5/module5_exercises.html#illumina-genome-assembly",
    "title": "Exercises",
    "section": "Illumina Genome Assembly",
    "text": "Illumina Genome Assembly\nWe are now going to use the assembler velvet https://www.ebi.ac.uk/~zerbino/velvet/ to assemble the Illumina reads. Our Illumina reads are from the same sample we used to generate the PacBio data.\n\nGenerating an Illumina assembly with velvet\nCreating a genome assembly using velvet is a two stage process:\n• First, the command velveth is used to generate the k-mers from the input data\n• Second, the command velvetg is used to build the de Bruijn graph and find the optimal path through the graph\nTo assemble the data, start with the command:\nvelveth k.assembly.49 49 -shortPaired -fastq IT.Chr5_1.fastq IT.Chr5_2.fastq\nThe input option k.assembly.49 is the name of the directory where the results are to be written.\nThe vallue 49 is the k-mer size. The other options specify the type of the input data (-shortPaired) and -fastq is used to specify the fastq files that contain the sequencing reads.\nTo see all possible options for velveth use:\nvelveth\nNow use velvetg to build the graph and find the path through the graph (similar to what we did manually in the previous section):\nvelvetg k.assembly.49 -exp_cov auto -ins_length 350\nThe first parameter k.assembly.49 specifies the working directory as created with the velveth command. The second -exp_cov auto instructs velvet to find the median read coverage automatically rather than specifying it yourself. Finally, -ins_length specifies the insert size of the sequencing library used. There is a lot of output printed to the screen, but the most important information is the last line:\nFinal graph has 1455 nodes and n50 of 7527, max 38045, total 1364551, using 700301/770774 reads. (Your exact result might differ depending on the velvet version used - don’t worry).\nThis gives you a quick idea of the result.\n• 1455 nodes are in the final graph.\n• An n50 of 7527 means that 50% of the assembly is in contigs of at least 7527 bases, it is the median contig size. This number is most commonly used as an indicator of assembly quality.\nThe higher, the better! (but not always!)\n• Max is the length of the longest contig.\n• Total is the size of the assembly, here 1346kb.\n• The last two numbers tell us how many reads were used from the 7.7 million pairs input data.\nTo see all possible options for velvetg use: velvetg\nNow let’s try to improve the quality of the assembly by varying some of the input parameters to velvet. Two parameters that can play a role in improving the assembly are -cov_cutoff and -min_contig_lgth.\nUsing the -cov_cutoff parameter means that nodes with less than a specific k-mer count are removed from the graph.\nUsing the -min_contig_lgth parameter means that contigs with less than a specific size are removed from the assembly.\nTry re-running the assembly with a kmer of 49 and using a -cov_cutoff of 5 and -min_contig_lgth of 200.\nvelvetg k.assembly.49 -exp_cov auto -ins_length 350 -min_contig_lgth 200 -cov_cutoff 5\nNote as we are not changing the k-mer size, we do not need run the velveth command again.\nGenerally, the k-mer size has the biggest impact on assembly results. Let us make a few other assemblies for different k-mer sizes i.e. 55, 41. Here is the example for k-mer length of 55.\nvelveth k.assembly.55 55 -shortPaired -fastq IT.Chr5_1.fastq IT.Chr5_2.fastq\nvelvetg k.assembly.55 -exp_cov auto -ins_length 350 -min_contig_lgth 200 -cov_cutoff 5\nNote: If you find that you are having trouble running the velvet assemblies or if it is running for longer than 10-15 mins then quit the command (Ctrc-C). A pre-generated set of Illumina assemblies can be found at:\nls ~/course_data/assembly/data/backup/illumina_assemblies",
    "crumbs": [
      "Home",
      "Genome Assembly",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html#assembly-metrics",
    "href": "course_modules/Module5/module5_exercises.html#assembly-metrics",
    "title": "Exercises",
    "section": "Assembly metrics",
    "text": "Assembly metrics\nAll the assembly results are written into the directory you specified with the velvet commands, e.g. k.assembly.41, k.assembly.49, k.assembly.55. The final contigs are written to a file called contigs.fa. The stats.txt file holds some information about each contig, its length, the coverage, etc. The other files contain information for the assembler.\nAnother way to get more assembly statistics is to use a program called assembly-stats. It displays the number of contigs, the mean size and a lot of other useful statistics about the assembly. These numbers can be used to assess the quality of your assemblies and help you pick the “best” one.\nType:\nassembly-stats k.assembly*/*.fa\nWrite down the results for each assembly made using different k-mer sizes. Which one looks the best?\nQuestion: What is the best choice for k?\nWe want to choose the set of parameters that produce the assembly where the n50, average contig size and the largest contigs have the highest values, while contig number is the lowest.\nYou will notice another statistic produced by assembly-stats is N_count, what does the N_count mean?\nAs we know, DNA templates can be sequenced from both ends, resulting in mate pairs. Their outer distance is the insert size. Imagine mapping the reads back onto the assembled contigs. In some cases the two mates don’t map onto the same contig. We can use those mates to scaffold the two contigs e.g. orientate them to each other and put N’s between them, so that the insert size is correct, if enough mate pairs suggest that join. Velvet does this automatically (although you can turn it off).\nThe number of mates you need to join two contigs is defined by the parameter -min_pair_count.\nHere is the description:\n-min_pair_count &lt;integer&gt;: minimum number of paired end connections to justify the scaffolding of two long contigs (default: 5)\nHere is a schema:\nIt might be worth mentioning, that incorrect scaffolding is the most common source of error in assembly (so called mis-assemblies). If you lower the min_pair_count too much, the likelihood of generating errors increases.\nOther errors are due to repeats. In a normal assembly one would expect that the repeats are all collapsed, if they are smaller than the read length. If the repeat unit is smaller than the insert size, than it is possible to scaffold over it, leaving the space for the repeats with N’s.\nTo get the statistic for the contigs, rather than scaffolds (supercontigs), you can use seqtk to break the scaffold at any stretch of N’s with the following commands:\nseqtk cutN -n1 k.assembly.41/contigs.fa &gt; assembly.41.contigs.fasta\nassembly-stats assembly.41.contigs.fasta\nseqtk cutN -n1 k.assembly.49/contigs.fa &gt; assembly.49.contigs.fasta\nassembly-stats assembly.49.contigs.fasta\nseqtk cutN -n1 k.assembly.55/contigs.fa &gt; assembly.55.contigs.fasta\nassembly-stats assembly.55.contigs.fasta\nQuestion: How does the contig N50 compare to the scaffold N50 for each of your assemblies?\nCongratulations you have sucessfully created a genome assembly using Illumina sequence data. Now move on to the next section: Assembly estimation",
    "crumbs": [
      "Home",
      "Genome Assembly",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html#assembly-estimation",
    "href": "course_modules/Module5/module5_exercises.html#assembly-estimation",
    "title": "Exercises",
    "section": "Assembly estimation",
    "text": "Assembly estimation\nFortunately with this dataset, we have a known reference genome and therefore some expectations about the size and composition of the Plasmodium falciparum genome.\nBut what if we are working with a new genome, one which has not been sequenced before? One approach is to look at k-mer distributions from the reads. This can be done using two pieces of software, jellyfish (https://github.com/gmarcais/Jellyfish) and Genomesope (http://qb.cshl.edu/genomescope/). Jellyfish is used to determine the distribution of k-mers in the dataset and then Genomescope is used to model the single copy k-mers as heterozygotes, while double copy k-mers will be the homozygous portions of the genome. It will also estimate the haploid genome size.\nLet’s check with our Plasmodium falciparum Illumina data that the k-mer distribution gives us what we expect. To get a distribution of 21-mers:\njellyfish count -C -m 21 -s 1G -t 2 -o IT.jf &lt;(cat IT.Chr5_1.fastq IT.Chr5_2.fastq)\nThis command will count canonical (-C) 21-mers (-m 21), using a hash with 1G elements (-s 1G) and 2 threads (-t 2). The output is written to IT.jf.\nTo compute the histogram of the k-mer occurences, use\njellyfish histo IT.jf &gt; IT.histo\nLook at the output:\nless IT.histo\nNow analyse the output with genomescope:\nRscript genomescope.R IT.histo 21 76 IT.jf21\nWhere 21 is the k-mer size, 76 is the read length of the input Illumina data and IT.jf21 is the directory to write the output to. To look at the output use:\nless IT.jf21/summary.txt\nfirefox IT.jf21/plot.png &\nYou should see an image similar to the ones shown below. Notice the bump to right of the main peak. These are the repeated sequences.\n\nExercises\n1. What is the predicted heterozygosity?\n2. What is the predicted genome size?\n3. Does this seem reasonable?\nWe have used jellyfish to pre-generate a set of k-mer histograms for a handful of other species. These histo files can be found in the data directory.\nls *.histo\nTry running genomscope on these. The read length for all of the datasets is 150bp.\nfAnaTes1.jf21.histo: What is the bulge to the left of the main peak here?\nRscript genomescope.R fAnaTes1.jf21.histo 21 150 fAnaTes1.jf21\nf DreSAT1.jf21.histo: What is the striking feature of this genome?\nfMasArm1.jf21.histo: You should see a nice tight diploid peak for this sample. It has very low heterozygosity - similar to human data.\nfSalTru1.jf21.histo: This genome was actually haploid. How do we interpret the features in the genomescope profile?\nNow let us go back to our PacBio assembly: PacBio assembly",
    "crumbs": [
      "Home",
      "Genome Assembly",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html#more-on-pacbio-genome-assembly",
    "href": "course_modules/Module5/module5_exercises.html#more-on-pacbio-genome-assembly",
    "title": "Exercises",
    "section": "More on PacBio Genome Assembly",
    "text": "More on PacBio Genome Assembly\nWe have generated an assembly for the Plasmodium falciparum chromosome from our Illumina data.\nNow, let’s have a look at the PacBio assembly.\n\nGenerating PacBio assemblies\nThe canu pacbio assembly should have hopefully finished by now (check the log less canu_log.txt).\nIf it has not finished, you can find a pre-generated canu assembly at:\nls ~/course_data/assembly/data/backup/pacbio_assemblies\nNow use the assembly-stats script to look at the assembly statistics.\nassembly-stats canu-assembly/PB.contigs.fasta\nHow does it compare to the Illumina assembly?\nAnother long read assembler based on de Bruijn graphs is wtdbg2 https://github.com/ruanjue/wtdbg2. Let’s try to build a second assembly using this assembler and compare it to the assembly produced with canu.\nwtdbg2 -t2 -i PBReads.fastq.gz -o wtdbg\nwtpoa-cns -t2 -i wtdbg.ctg.lay.gz -fo wtdbg.ctg.lay.fasta\nassembly-stats wtdbg.ctg.lay.fasta\n\n\nComparing PacBio assemblies\nHow does the wtdbg2 assembly compare to the canu assembly? Both assemblies may be similar in contig number and N50, but are they really similar? Let’s map the Illumina reads to each assembly, call variants and compare.\nbwa index canu-assembly/PB.contigs.fasta\nsamtools faidx canu-assembly/PB.contigs.fasta\nbwa mem -t2 canu-assembly/PB.contigs.fasta IT.Chr5_1.fastq IT.Chr5_2.fastq |samtools sort -@2 - | samtools mpileup -f canu-assembly/PB.contigs.fasta -ug- | bcftools call -mv &gt; canu.vcf\nDo the same for wtdbg.ctg.lay.fasta and then compare some basic statistics.\nbwa index wtdbg.ctg.lay.fasta\nsamtools faidx wtdbg.ctg.lay.fasta\n\n\nPolishing PacBio assembly\nbwa mem -t2 wtdbg.ctg.lay.fasta IT.Chr5_1.fastq IT.Chr5_2.fastq | samtools sort-@2 - | samtools mpileup -f wtdbg.ctg.lay.fasta -ug - | bcftools call -mv &gt;wtdbg.vcf\nbcftools stats canu.vcf | grep ^SN\nbcftools stats wtdbg.vcf | grep ^SN\nQuestion: What do you notice in terms of the number of SNP and indel calls?\nThe wtdbg2 assembly has more variants due to having more errors. This is mainly due to a lack of polishing or error correction - something that the canu assembler performs, but the wtdbg2 assembler does not.\nCorrecting errors is an important step in making an assembly, especially from noisy long ready data. Not polishing an assembly can lead to genes not being identified due to insertion and deletion errors in the assembly sequence. To polish a genome assembly with Illumina data we use bcftools consensus to change homozygous differences between the assembly and the Illumina data to match the Illumina data\nRun the following steps to polish the canu assembly\nbgzip -c canu.vcf &gt; canu.vcf.gz\ntabix canu.vcf.gz\nbcftools consensus -i'QUAL&gt;1 && (GT=\"AA\" || GT=\"Aa\")' -Hla -f canu-assembly/PB.contigs.fasta canu.vcf.gz &gt; canu-assembly/PB.contigs.polished.fasta\nRun the following steps to polish the `wtdbg2` assembly\nbgzip -c wtdbg.vcf &gt; wtdbg.vcf.gz\ntabix wtdbg.vcf.gz\nbcftools consensus -i'QUAL&gt;1 && (GT=\"AA\" || GT=\"Aa\")' -Hla -f wtdbg.ctg.lay.fasta wtdbg.vcf.gz &gt; wtdbg.contigs.polished.fasta\nFinally, align and call variants like before (bwa index/bwa mem/samtools-sort/mpileup/bcftools call) using the polished assemblies as the reference this time.\nWhen running this analysis on these polished genomes, do we still get variants? More or less than with the raw canu and wtdbg2 assemblies? Why?\nCongratulations, you have reached the end of the Genome Assembly tutorial.",
    "crumbs": [
      "Home",
      "Genome Assembly",
      "Exercises"
    ]
  },
  {
    "objectID": "course_modules/Module5/module5.html",
    "href": "course_modules/Module5/module5.html",
    "title": "Overview",
    "section": "",
    "text": "Module Title\nGenome Assembly\n\n\nAuthors\nThis tutorial was written by Jacqui Keane based on material from Shane McCarthy and Thomas Otto.\n\n\nDuration\n3 hours\n\n\nKey topics\n\n\nLearning outcomes\nOn completion of the tutorial, you can expect to be able to:\n• Describe the different approaches to genome assembly\n• Generate a genome assembly from Illumina data\n• Generate a genome assembly from PacBio data\n• Generate statistics to evaluate the quality of a genome assembly\n• Estimate the size of a genome assembly\n\n\nActivities\n\nLecture: 1 hour\nPractical exercises: 1.5 hours\nAssessment quiz: 0.5 hours\n\n\n\nManual\nModule manual\nExercises\nSolutions\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual. Instructions on how to that will be added here.)\n\n\nLecture recording, notes and scripts\n\n\nCheck you knowledge quiz\nQuestions\n\n\nSummary of this module\n\n\nTeaching guidance\nInstructor notes\n\n\nDatasets\nPractice files for this module can be found on Github.\n\n\nPlatform Guidance\nGuide",
    "crumbs": [
      "Home",
      "Genome Assembly",
      "Overview"
    ]
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html",
    "title": "Identifying contamination",
    "section": "",
    "text": "It is always a good idea to check that your data is from the species you expect it to be. A very useful tool for this is Kraken. In this tutorial we will go through how you can use Kraken to check your samples for contamination.\nNote if using the Sanger cluster: Kraken is run as part of the automatic qc pipeline and you can retreive the results using the pf qc script. For more information, run pf man qc."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#setting-up-a-database",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#setting-up-a-database",
    "title": "Identifying contamination",
    "section": "Setting up a database",
    "text": "Setting up a database\nTo run Kraken you need to either build a database or download an existing one. The standard database is very large (33 GB), but thankfully there are some smaller, pre-built databased available. To download the smallest of them, the 4 GB MiniKraken. If you don’t already have a kraken database set up, run:\n\nwget https://ccb.jhu.edu/software/kra\\\n    ken/dl/minikraken_20171019_4GB.tgz\n\nThen all you need to do is un-tar it:\n\ntar -zxvf minikraken_20171019_4GB.tgz\n\nThis version of the database is constructed from complete bacterial, archaeal, and viral genomes in RefSeq, however it contains only around 3 percent of the kmers from the original kraken database (more information here). If the pre-packaged databases are not quite what you are looking for, you can create your own customized database instead. Details about this can be found here.\nNote if using the Sanger cluster: There are several pre-built databases available centrally on the Sanger cluster. For more information, please contact the Pathogen Informatics team."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#running-kraken",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#running-kraken",
    "title": "Identifying contamination",
    "section": "Running Kraken",
    "text": "Running Kraken\nTo run Kraken, you need to provide the path to the database you just created. By default, the input files are assumed to be in FASTA format, so in this case we also need to tell Kraken that our input files are in FASTQ format, gzipped, and that they are paired end reads:\n\nkraken --db ./minikraken_20171013_4GB --output kraken_results \\\n    --fastq-input --gzip-compressed --paired \\\n    data/13681_1#18_1.fastq.gz data/13681_1#18_2.fastq.gz\n\nThe five columns in the file that’s generated are:\n\n“C”/“U”: one letter code indicating that the sequence was either classified or unclassified.\nThe sequence ID, obtained from the FASTA/FASTQ header.\nThe taxonomy ID Kraken used to label the sequence; this is 0 if the sequence is unclassified.\nThe length of the sequence in bp.\nA space-delimited list indicating the LCA mapping of each k-mer in the sequence.\n\nTo get a better overview you can create a kraken report:\n\nkraken-report --db ./minikraken_20171013_4GB \\\n    kraken_results &gt; kraken-report"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#looking-at-the-results",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#looking-at-the-results",
    "title": "Identifying contamination",
    "section": "Looking at the results",
    "text": "Looking at the results\nLet’s have a closer look at the kraken_report for the sample. If for some reason your kraken-run failed there is a prebaked kraken-report at data/kraken-report\n\nhead -n 20 kraken-report\n\nThe six columns in this file are:\n\nPercentage of reads covered by the clade rooted at this taxon\nNumber of reads covered by the clade rooted at this taxon\nNumber of reads assigned directly to this taxon\nA rank code, indicating (U)nclassified, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. All other ranks are simply ‘-’.\nNCBI taxonomy ID\nScientific name"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#exercises",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#exercises",
    "title": "Identifying contamination",
    "section": "Exercises",
    "text": "Exercises\nQ1: What is the most prevalent species in this sample?\nQ2: Are there clear signs of contamination?\nQ3: What percentage of reads could not be classified?"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#heterozygous-snps",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#heterozygous-snps",
    "title": "Identifying contamination",
    "section": "Heterozygous SNPs",
    "text": "Heterozygous SNPs\nFor bacteria, another thing that you can look at to detect contamination is if there are heterozygous SNPs in your samples. Simply put, if you align your reads to a reference, you would expect any snps to be homozygous, i.e. if one read differs from the reference genome, then the rest of the reads that map to that same location will also do so:\nHomozygous SNP\nRef:       CTTGAGACGAAATCACTAAAAAACGTGACGACTTG\nRead1:  CTTGAGtCG\nRead2:  CTTGAGtCGAAA\nRead3:         GAGtCGAAATCACTAAAA\nRead4:               GtCGAAATCA\nBut if there is contamination, this may not be the case. In the example below, half of the mapped reads have the T allele and half have the A.\nHeterozygous SNP\nRef:       CTTGAGACGAAATCACTAAAAAACGTGACGACTTG\nRead1:  CTTGAGtCG\nRead2:  CTTGAGaCGAAA\nRead3:         GAGaCGAAATCACTAAAA\nRead4:               GtCGAAATCA\nNote if using the Sanger cluster: Heterozygous SNPs are calculated as part of the automated QC pipeline. The result for each lane is available in the file heterozygous_snps_report.txt.\nCongratulations! You have reached the end of this tutorial. You can find the answers to all the questions of the tutorial here.\nTo revisit the previous section click here. Alternatively you can head back to the index page"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html",
    "title": "Data formats for NGS data",
    "section": "",
    "text": "Here we will take a closer look at some of the most common NGS data formats. First, check you are in the correct directory.\npwd\nIt should display something like:\n/home/manager/course_data/data_formats/"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#fasta",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#fasta",
    "title": "Data formats for NGS data",
    "section": "FASTA",
    "text": "FASTA\nThe FASTA format is used to store both nucleotide data and protein sequences. Each sequence in a FASTA file is represented by two parts, a header line and the actual sequence. The header always starts with the symbol “&gt;” and is followed by information about the sequence, such as a unique identifier. The following lines show two sequences represented in FASTA format:\n&gt;Sequence_1\nCTTGACGACTTGAAAAATGACGAAATCACTAAAAAACGTGAAAAATGAGAAATG\nAAAATGACGAAATCACTAAAAAACGTGACGACTTGAAAAATGACCAC\n&gt;Sequence_2\nCTTGAGACGAAATCACTAAAAAACGTGACGACTTGAAGTGAAAAATGAGAAATG\nAAATCATGACGACTTGAAGTGAAAAAGTGAAAAATGAGAAATGAACGTGACGAC\nAAAATGACGAAATCATGACGACTTGAAGTGAAAAATAAATGACC\n\nExercises\nQ1: How many sequences are there in the fasta file data/example.fasta? (Hint: is there a grep option you can use?)"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#fastq",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#fastq",
    "title": "Data formats for NGS data",
    "section": "FASTQ",
    "text": "FASTQ\nFASTQ is a data format for sequencing reads. It is an extension to the FASTA file format, and includes a quality score for each base. Have a look at the example below, containing two reads:\n@ERR007731.739 IL16_2979:6:1:9:1684/1\nCTTGACGACTTGAAAAATGACGAAATCACTAAAAAACGTGAAAAATGAGAAATG\n+\nBBCBCBBBBBBBABBABBBBBBBABBBBBBBBBBBBBBABAAAABBBBB=@&gt;B\n@ERR007731.740 IL16_2979:6:1:9:1419/1\nAAAAAAAAAGATGTCATCAGCACATCAGAAAAGAAGGCAACTTTAAAACTTTTC\n+\nBBABBBABABAABABABBABBBAAA&gt;@B@BBAA@4AAA&gt;.&gt;BAA@779:AAA@A\nWe can see that for each read we get four lines:\n\nThe read metadata, such as the read ID. Starts with @ and, for paired-end Illumina reads, is terminated with /1 or /2 to show that the read is the member of a pair.\nThe read\nStarts with + and optionally contains the ID again\nThe per base Phred quality score\n\nThe quality scores range (in theory) from 1 to 94 and are encoded as ASCII characters. The first 32 ASCII codes are reserved for control characters which are not printable, and the 33rd is reserved for space. Neither of these can be used in the quality string, so we need to subtract 33 from whatever the value of the quality character is. For example, the ASCII code of “A” is 65, so the corresponding quality is:\nQ = 65 - 33 = 32\nThe Phred quality score Q relates to the base-calling error probability P as\n       P = 10-Q/10\nThe Phred quality score is a measure of the quality of base calls. For example, a base assigned with a Phred quality score of 30 tells us that there is a 1 in 1000 chance that this base was called incorrectly.\n\n\n\n\n\n\n\n\nPhred Quality Score\nProbability of incorrect base call\nBase call accuracy\n\n\n\n\n10\n1 in 10\n90%\n\n\n20\n1 in 100\n99%\n\n\n30\n1 in 1000\n99.9%\n\n\n40\n1 in 10,000\n99.99%\n\n\n50\n1 in 100,000\n99.999%\n\n\n60\n1 in 1,000,000\n99.9999%\n\n\n\n\nExercises\nQ2: How many reads are there in the file example.fastq? (Hint: remember that @ is a possible quality score. Is there something else in the header that is unique?)"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#sam",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#sam",
    "title": "Data formats for NGS data",
    "section": "SAM",
    "text": "SAM\nSAM (Sequence Alignment/Map) format is a unified format for storing read alignments to a reference genome. It is a standard format for storing NGS sequencing reads, base qualities, associated meta-data and alignments of the data to a reference genome. If no reference genome is available, the data can also be stored unaligned.\nThe files consist of a header section (optional) and an alignment section. The alignment section contains one record (a single DNA fragment alignment) per line describing the alignment between fragment and reference. Each record has 11 fixed columns and optional key:type:value tuples. Open the SAM/BAM file specification document https://samtools.github.io/hts-specs/SAMv1.pdf either in a web browser or you can find a copy in the QC directory as you may need to refer to it throughout this tutorial.\nNow let us have a closer look at the different parts of the SAM/BAM format.\n\nHeader Section\nThe header section of a SAM file looks like:\n@HD VN:1.0  SO:coordinate\n@SQ SN:test_ref LN:17637\n@RG ID:ERR003612 PL:ILLUMINA LB:g1k-sc-NA20538-TOS-1 PI:2000 DS:SRP000540 SM:NA20538 CN:SC\nEach line in the SAM header begins with an @, followed by a two-letter header record type code as defined in the SAM/BAM format specification document. Each record type can contain meta-data captured as a series of key-value pairs in the format of ‘TAG:VALUE’.\n\nRead groups\nOne useful record type is RG which can be used to describe each lane of sequencing. The RG code can be used to capture extra meta-data for the sequencing lane. Some common RG TAGs are:\n\nID: SRR/ERR number\nPL: Sequencing platform\nPU: Run name\nLB: Library name\nPI: Insert fragment size\nSM: Individual/Sample\nCN: Sequencing centre\n\nWhile most of these are self explanitory, insert fragment size may occasionally be negative. This simply indicates that the reads found are overlapping while its size is less than 2 x read length.\n\n\n\nExercises\nLook at the following line from the header of a SAM file and answering the questions that follow:\n@RG ID:ERR003612 PL:ILLUMINA LB:g1k-sc-NA20538-TOS-1 PI:2000 DS:SRP000540 SM:NA20538 CN:SC\nQ3: What does RG stand for?\nQ4: What is the sequencing platform?\nQ5: What is the sequencing centre?\nQ6: What is the lane identifier?\nQ7: What is the expected fragment insert size?\n\n\nAlignment Section\nThe alignment section of SAM files contains one line per read alignment, an example is\nERR005816.1408831 163 Chr1    19999970    23  40M5D30M2I28M   =   20000147    213 GGTGGGTGGATCACCTGAGATCGGGAGTTTGAGACTAGGTGG...    &lt;=@A@??@=@A@A&gt;@BAA@ABA:&gt;@&lt;&gt;=BBB9@@2B3&lt;=@A@...\nEach of the lines are composed of multiple columns listed below. The first 11 columns are mandatory.\n\nQNAME: Query NAME of the read or the read pair i.e. DNA sequence\nFLAG: Bitwise FLAG (pairing, strand, mate strand, etc.)\nRNAME: Reference sequence NAME\nPOS: 1-Based leftmost POSition of clipped alignment\nMAPQ: MAPping Quality (Phred-scaled)\nCIGAR: Extended CIGAR string (operations: MIDNSHPX=)\nMRNM: Mate Reference NaMe (’=’ if same as RNAME)\nMPOS: 1-Based leftmost Mate POSition\nISIZE: Inferred Insert SIZE\nSEQ: Query SEQuence on the same strand as the reference\nQUAL: Query QUALity (ASCII-33=Phred base quality)\nOTHER: Optional fields\n\nThe image below provides a visual guide to some of the columns of the SAM format.\n\n\n\nSAM format\n\n\n\n\nExercises\nLet’s have a look at example.sam. Notice that we can use the standard UNIX operations like cat on this file.\n\ncat data/example.sam\n\nQ8: What is the mapping quality of ERR003762.5016205? (Hint: can you use grep and awk to find this?)\nQ9: What is the CIGAR string for ERR003814.6979522? (Hint: we will go through the meaning of CIGAR strings in the next section)\nQ10: What is the inferred insert size of ERR003814.1408899?\n\n\nCIGAR string\nColumn 6 of the alignment is the CIGAR string for that alignment. The CIGAR string provides a compact representation of sequence alignment. Have a look at the table below. It contains the meaning of all different symbols of a CIGAR string:\n\n\n\nSymbol\nMeaning\n\n\n\n\nM\nalignment match or mismatch\n\n\n=\nsequence match\n\n\nX\nsequence mismatch\n\n\nI\ninsertion into the read (sample sequenced)\n\n\nD\ndeletion from the read (sample sequenced)\n\n\nS\nsoft clipping (clipped sequences present in SEQ)\n\n\nH\nhard clipping (clipped sequences NOT present in SEQ)\n\n\nN\nskipped region from the reference\n\n\nP\npadding (silent deletion from padded reference)\n\n\n\nBelow are two examples describing the CIGAR string in more detail.\nExample 1:\nRef:     ACGTACGTACGTACGT\nRead:  ACGT- - - - ACGTACGA\nCigar: 4M 4D 8M\nThe first four bases in the read are the same as in the reference, so we can represent these as 4M in the CIGAR string. Next comes 4 deletions, represented by 4D, followed by 7 alignment matches and one alignment mismatch, represented by 8M. Note that the mismatch at position 16 is included in 8M. This is because it still aligns to the reference.\nExample 2:\nRef:     ACTCAGTG- - GT\nRead:  ACGCA- TGCAGTtagacgt\nCigar: 5M 1D 2M 2I 2M 7S\nHere we start off with 5 alignment matches and mismatches, followed by one deletion. Then we have two more alignment matches, two insertions and two more matches. At the end, we have seven soft clippings, 7S. These are clipped sequences that are present in the SEQ (Query SEQuence on the same strand as the reference).\n\n\nExercises\nQ11: What does the CIGAR from Q9 mean?\nQ12: How would you represent the following alignment with a CIGAR string?\nRef:     ACGT- - - - ACGTACGT\nRead:  ACGTACGTACGTACGT\n\n\nFlags\nColumn 2 of the alignment contains a combination of bitwise FLAGs describing the alignment. The following table contains the information you can get from the bitwise FLAGs:\n\n\n\n\n\n\n\n\n\nHex\nDec\nFlag\nDescription\n\n\n\n\n0x1\n1\nPAIRED\npaired-end (or multiple-segment) sequencing technology\n\n\n0x2\n2\nPROPER_PAIR\neach segment properly aligned according to the aligner\n\n\n0x4\n4\nUNMAP\nsegment unmapped\n\n\n0x8\n8\nMUNMAP\nnext segment in the template unmapped\n\n\n0x10\n16\nREVERSE\nSEQ is reverse complemented\n\n\n0x20\n32\nMREVERSE\nSEQ of the next segment in the template is reversed\n\n\n0x40\n64\nREAD1\nthe first segment in the template\n\n\n0x80\n128\nREAD2\nthe last segment in the template\n\n\n0x100\n256\nSECONDARY\nsecondary alignment\n\n\n0x200\n512\nQCFAIL\nnot passing quality controls\n\n\n0x400\n1024\nDUP\nPCR or optical duplicate\n\n\n0x800\n2048\nSUPPLEMENTARY\nsupplementary alignment\n\n\n\nFor example, if you have an alignment with FLAG set to 113, this can only be represented by decimal codes 64 + 32 + 16 + 1, so we know that these four flags apply to the alignment and the alignment is paired-end, reverse complemented, sequence of the next template/mate of the read is reversed and the read aligned is the first segment in the template.\n\nPrimary, secondary and supplementary alignments\nA read that aligns to a single reference sequence (including insertions, deletions, skips and clipping but not direction changes), is a linear alignment. If a read cannot be represented as a linear alignment, but instead is represented as a group of linear alignments without large overlaps, it is called a chimeric alignment. These can for instance be caused by structural variations. Usually, one of the linear alignments in a chimeric alignment is considered to be the representative alignment, and the others are called supplementary.\nSometimes a read maps equally well to more than one spot. In these cases, one of the possible alignments is marked as the primary alignment and the rest are marked as secondary alignments."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#bam",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#bam",
    "title": "Data formats for NGS data",
    "section": "BAM",
    "text": "BAM\nBAM (Binary Alignment/Map) format, is a compressed binary version of SAM. This means that, while SAM is human readable, BAM is only readable for computers. BAM files can be viewed using samtools, and will then have the same format as a SAM file. The key features of BAM are:\n\nCan store alignments from most mappers\nSupports multiple sequencing technologies\nSupports indexing for quick retrieval/viewing\nCompact size (e.g. 112Gbp Illumina = 116GB disk space)\nReads can be grouped into logical groups e.g. lanes, libraries, samples\nWidely supported by variant calling packages and viewers\n\nSince BAM is a binary format, we can’t use the standard UNIX operations directly on this file format. Samtools is a set of programs for interacting with SAM and BAM files. Using the samtools view command, print the header of the BAM file:\n\nsamtools view -H data/NA20538.bam\n\n\nExercises\nQ13: What version of the human assembly was used to perform the alignments? (Hint: Can you spot this somewhere in the @SQ records?)\nQ14: How many lanes are in this BAM file? (Hint: Do you recall what RG represents?)\nQ15: What programs were used to create this BAM file? (Hint: have a look for the program record, @PG)\nQ16: What version of bwa was used to align the reads? (Hint: is there anything in the @PG record that looks like it could be a version tag?)\nThe output from running samtools view on a BAM file without any options is a headerless SAM file. This gets printed to STDOUT in the terminal, so we will want to pipe it to something. Let’s have a look at the first read of the BAM file:\n\nsamtools view data/NA20538.bam | head -n 1\n\nQ17: What is the name of the first read? (Hint: have a look at the alignment section if you can’t recall the different fields)\nQ18: What position does the alignment of the read start at?"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#cram",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#cram",
    "title": "Data formats for NGS data",
    "section": "CRAM",
    "text": "CRAM\nEven though BAM files are compressed, they are still very large. Typically they use 1.5-2 bytes for each base pair of sequencing data that they contain, and while disk capacity is ever improving, increases in disk capacity are being far outstripped by sequencing technologies.\nBAM stores all of the data, this includes every read base, every base quality, and it uses a single conventional compression technique for all types of data. CRAM was designed for better compression of genomic data than SAM/BAM. CRAM uses three important concepts:\n\nReference based compression\nControlled loss of quality information\nDifferent compression methods to suit the type of data, e.g. base qualities vs. metadata vs. extra tags\n\nThe figure below displays how reference-based compression works. Instead of saving all the bases of all the reads, only the nucleotides that differ from the reference, and their positions, are kept.\n\n\nIn lossless (no information is lost) mode a CRAM file is 60% of the size of a BAM file, so archives and sequencing centres have moved from BAM to CRAM.\nSince samtools 1.3, CRAM files can be read in the same way that BAM files can. We will look closer at how you can convert between SAM, BAM and CRAM formats in the next section."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#indexing",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#indexing",
    "title": "Data formats for NGS data",
    "section": "Indexing",
    "text": "Indexing\nTo allow for fast random access of regions in BAM and CRAM files, they can be indexed. The files must first be coordinate-sorted rather that sorted by read name. This can be done using samtools sort. If no options are supplied, it will by default sort by the left-most position of the reference.\n\nsamtools sort -o data/NA20538_sorted.bam data/NA20538.bam\n\nNow we can use samtools index to create an index file (.bai) for our sorted BAM file:\n\nsamtools index data/NA20538_sorted.bam\n\nTo look for reads mapped to a specific region, we can use samtools view and specify the region we are interested in as: RNAME[:STARTPOS[-ENDPOS]]. For example, to look at all the reads mapped to a region called chr4, we could use:\nsamtools view alignment.bam chr4\nTo look at the region on chr4 beginning at position 1,000,000 and ending at the end of the chromosome, we can do:\nsamtools view alignment.bam chr4:1000000\nAnd to explore the 1001bp long region on chr4 beginning at position 1,000 and ending at position 2,000, we can use:\nsamtools view alignment.bam chr4:1000-2000\n\nExercises\nQ19: How many reads are mapped to region 20025000-20030000 on chromosome 1?"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#vcf",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#vcf",
    "title": "Data formats for NGS data",
    "section": "VCF",
    "text": "VCF\nThe VCF file format was introduced to store variation data. VCF consists of tab-delimited text and is parsable by standard UNIX commands which makes it flexible and user-extensible. The figure below provides an overview of the different components of a VCF file:\n\n\n\nVCF format\n\n\n\nVCF header\nThe VCF header consists of meta-information lines (starting with ##) and a header line (starting with #). All meta-information lines are optional and can be put in any order, except for fileformat. This holds the information about which version of VCF is used and must come first.\nThe meta-information lines consist of key=value pairs. Examples of meta-information lines that can be included are ##INFO, ##FORMAT and ##reference. The values can consist of multiple fields enclosed by &lt;&gt;. More information about these fields is available in the VCF specification http://samtools.github.io/hts-specs/VCFv4.3.pdf. This can be accessed using a web browser and there is a copy in the QC directory.\n\nHeader line\nThe header line starts with # and consists of 8 required fields:\n\nCHROM: an identifier from the reference genome\nPOS: the reference position\nID: a list of unique identifiers (where available)\nREF: the reference base(s)\nALT: the alternate base(s)\nQUAL: a phred-scaled quality score\nFILTER: filter status\nINFO: additional information\n\nIf the file contains genotype data, the required fields are also followed by a FORMAT column header, and then a number of sample IDs. The FORMAT field specifies the data types and order. Some examples of these data types are:\n\nGT: Genotype, encoded as allele values separated by either / or |\nDP: Read depth at this position for this sample\nGQ: Conditional genotype quality, encoded as a phred quality\n\n\n\n\nBody\nIn the body of the VCF, each row contains information about a position in the genome along with genotype information on samples for each position, all according to the fields in the header line."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#bcf",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#bcf",
    "title": "Data formats for NGS data",
    "section": "BCF",
    "text": "BCF\nBCF is a compressed binary representation of VCF.\nVCF can be compressed with BGZF (bgzip) and indexed with TBI or CSI (tabix), but even compressed it can still be very big. For example, a compressed VCF with 3781 samples of human data will be 54 GB for chromosome 1, and 680 GB for the whole genome. VCFs can also be slow to parse, as text conversion is slow. The main bottleneck is the “FORMAT” fields. For this reason the BCF format was developed.\nIn BCF files the fields are rearranged for fast access. The following images show the process of converting a VCF file into a BCF file.\n\n\nBcftools comprises a set of programs for interacting with VCF and BCF files. It can be used to convert between VCF and BCF and to view or extract records from a region.\n\nbcftools view\nLet’s have a look at the header of the file 1kg.bcf in the data directory. Note that bcftools uses -h to print only the header, while samtools uses -H for this.\n\nbcftools view -h data/1kg.bcf\n\nSimilarly to BAM, BCF supports random access, that is, fast retrieval from a given region. For this, the file must be indexed:\n\nbcftools index data/1kg.bcf\n\nNow we can extract all records from the region 20:24042765-24043073, using the -r option. The -H option will make sure we don’t include the header in the output:\n\nbcftools view -H -r 20:24042765-24043073 data/1kg.bcf\n\n\n\nbcftools query\nThe versatile bcftools query command can be used to extract any VCF field. Combined with standard UNIX commands, this gives a powerful tool for quick querying of VCFs. Have a look at the usage options:\n\nbcftools query -h\n\nLet’s try out some useful options. As you can see from the usage, -l will print a list of all the samples in the file. Give this a go:\n\nbcftools query -l data/1kg.bcf\n\nAnother very useful option is -s which allows you to extract all the data relating to a particular sample. This is a common option meaning it can be used for many bcftools commands, like bcftools view. Try this for sample HG00131:\n\nbcftools view -s HG00131 data/1kg.bcf | head -n 50\n\nThe format option, -f can be used to select what gets printed from your query command. For example, the following will print the position, reference base and alternate base for sample HG00131, separated by tabs:\n\nbcftools query -f'%POS\\t%REF\\t%ALT\\n' -s HG00131 data/1kg.bcf | head\n\nFinally, let’s look at the -i option. With this option we can select only sites for which a particular expression is true. For instance, if we only want to look at sites that have at least 2 alternate alleles across all samples, we can use the following expression (piped to head to only show a subset of the output):\n\nbcftools query -f'%CHROM\\t%POS\\n' -i 'AC[0]&gt;2' data/1kg.bcf | head\n\nWe use -i with the expression AC[0]&gt;2. AC is an info field that holds the __a__llele __c__ount. Some fields can hold multiple values, so we use AC[0]&gt;2 to indicate that we are looking for the first value (this is zero indexed, and hence starts at 0 instead of 1), and that this value should be &gt; 2. To format our output, we use -f to specify that we want to print the chromosome name and position.\nThere is more information about expressions on the bcftools manual page http://samtools.github.io/bcftools/bcftools.html#expressions\n\n\nExercises\nNow, try and answer the following questions about the file 1kg.bcf in the data directory. For more information about the different usage options you can open the bcftools query manual page http://samtools.github.io/bcftools/bcftools.html#query in a web browser.\nQ20: What version of the human assembly do the coordinates refer to?\nQ21: How many samples are there in the BCF?\nQ22: What is the genotype of the sample HG00107 at the position 20:24019472? (Hint: use the combination of -r, -s, and -f options)\nQ23: How many positions are there with more than 10 alternate alleles? (Hint: use the -i filtering option)\nQ24: In how many positions does HG00107 have a non-reference genotype and a read depth bigger than 10? (Hint: you can use pipes to combine bcftools queries)"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#gvcf",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#gvcf",
    "title": "Data formats for NGS data",
    "section": "gVCF",
    "text": "gVCF\nOften it is not enough to know variant sites only. For instance, we don’t know if a site was dropped because it matches the reference or because the data is missing. We sometimes need evidence for both variant and non-variant positions in the genome. In gVCF format, blocks of reference-only sites can be represented in a single record using the “INFO/END” tag. Symbolic alleles (&lt;*&gt;) are used for incremental calling:\n\n\n\ngVCF\n\n\n\nExercises\nQ25: In the above example, what is the size of the reference-only block starting at position 9923?\nQ26: For the same block, what is the first base?\nQ27: How many reference reads does the block have?\nNow continue to the next section of the tutorial: QC assessment of NGS data."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html",
    "title": "QC assessment of NGS data",
    "section": "",
    "text": "QC is an important part of any analysis. In this section we are going to look at some of the metrics and graphs that can be used to assess the QC of NGS data."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#base-quality",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#base-quality",
    "title": "QC assessment of NGS data",
    "section": "Base quality",
    "text": "Base quality\nIllumina sequencing technology relies on sequencing by synthesis. One of the most common problems with this is dephasing. For each sequencing cycle, there is a possibility that the replication machinery slips and either incorporates more than one nucleotide or perhaps misses to incorporate one at all. The more cycles that are run (i.e. the longer the read length gets), the greater the accumulation of these types of errors gets. This leads to a heterogeneous population in the cluster, and a decreased signal purity, which in turn reduces the precision of the base calling. The figure below shows an example of this.\n\n\n\nMean Base Quality\n\n\nBecause of dephasing, it is possible to have high-quality data at the beginning of the read but really low-quality data towards the end of the read. In those cases you can decide to trim off the low-quality reads, for example using a tool called Trimmomatic.\nThe figures below shows an example of a good sequencing run (left) and a poor sequencing run (right).\n\n\n\nBase quality"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#other-base-calling-errors",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#other-base-calling-errors",
    "title": "QC assessment of NGS data",
    "section": "Other base calling errors",
    "text": "Other base calling errors\nThere are several different reasons for a base to be called incorrectly, as shown in the figure below. Phasing noise and signal decay is a result of the dephasing issue described above. During library preparation, mixed clusters can occur if multiple templates get co-located. These clusters should be removed from the downstream analysis. Boundary effects occur due to optical effects when the intensity is uneven across each tile, resulting in higher intensity found toward the center. Cross-talk occurs because the emission frequency spectra for each of the four base dyes partly overlap, creating uncertainty. Finally, for previous sequencing cycle methods T fluorophore accumulation was an issue, where incomplete removal of the dye coupled to thymine lead to an ambient accumulation the nucleotides, causing a false high Thymine trend.\n\n\n\nBase Calling Errors\n\n\nBase-calling for next-generation sequencing platforms, doi: 10.1093/bib/bbq077"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#mismatches-per-cycle",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#mismatches-per-cycle",
    "title": "QC assessment of NGS data",
    "section": "Mismatches per cycle",
    "text": "Mismatches per cycle\nAligning reads to a high-quality reference genome can provide insight to the quality of a sequencing run by showing you the mismatches to the reference sequence. This can help you detect cycle-specific errors. Mismatches can occur due to two main causes, sequencing errors and differences between your sample and the reference genome, which is important to bear in mind when interpreting mismatch graphs. The figure below shows an example of a good run (left) and a bad one (right). In the graph on the left, the distribution of the number of mismatches is even between the cycles, which is what we would expect from a good run. However, in the graph on the right, two cycles stand out with a lot of mismatches compared to the other cycles.\n\n\n\nMismatches per cycle"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#gc-content",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#gc-content",
    "title": "QC assessment of NGS data",
    "section": "GC content",
    "text": "GC content\nIt is a good idea to compare the GC content of the reads against the expected distribution in a reference sequence. The GC content varies between species, so a shift in GC content like the one seen below could be an indication of sample contamination. In the left graph below, we can see that the GC content of the sample is about the same as for the reference, at ~38%. However, in the right graph, the GC content of the sample is closer to 55%, indicating that there is an issue with this sample.\n\n\n\nGC Content"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#gc-content-by-cycle",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#gc-content-by-cycle",
    "title": "QC assessment of NGS data",
    "section": "GC content by cycle",
    "text": "GC content by cycle\nLooking at the GC content per cycle can help detect if the adapter sequence was trimmed. For a random library, it is expected to be little to no difference between the different bases of a sequence run, so the lines in this plot should be parallel with each other like in the graph on the left below. In the graph on the right, the initial spikes are likely due to adapter sequences that have not been removed.\n\n\n\nGC content by cycle"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#fragment-size",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#fragment-size",
    "title": "QC assessment of NGS data",
    "section": "Fragment size",
    "text": "Fragment size\nFor paired-end sequencing the size of DNA fragments also matters. In the first of the examples below, the fragment size peaks around 440 bp. In the second however, there is also a peak at around 200 bp. This indicates that there was an issue with the fragment size selection during library prep.\n\n\n\nFragment size distribution\n\n\n\nExercises\nQ1: The figure below is from a 100bp paired-end sequencing. Can you spot any problems?\n\n\n\nQ1 Insert size distribution"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#insertionsdeletions-per-cycle",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#insertionsdeletions-per-cycle",
    "title": "QC assessment of NGS data",
    "section": "Insertions/Deletions per cycle",
    "text": "Insertions/Deletions per cycle\nSometimes, air bubbles occur in the flow cell, which can manifest as false indels. The spike in the right image provides an example of how this can look.\n\n\n\nIndels per cycle"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#generating-qc-stats",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#generating-qc-stats",
    "title": "QC assessment of NGS data",
    "section": "Generating QC stats",
    "text": "Generating QC stats\nNow let’s try this out! We will generate QC stats for two lanes of Illumina paired-end sequencing data from yeast. The reads have already been aligned to the Saccromyces cerevisiae reference genome to produce the BAM file lane1.sorted.bam.\nNow we will use samtools stats to generate the stats for the primary alignments. The option -f can be used to filter reads with specific tags, while -F can be used to filter out reads with specific tags. The following command will include only primary alignments:\n\nsamtools stats -F SECONDARY data/lane1.sorted.bam &gt; data/lane1.sorted.bam.bchk\n\nHave a look at the first 47 lines of the statistics file that was generated:\n\nhead -n 47 data/lane1.sorted.bam.bchk\n\nThis file contains a number of useful stats that we can use to get a better picture of our data, and it can even be plotted with plot-bamstats, as you will see soon. First let’s have a closer look at some of the different stats. Each part of the file starts with a # followed by a description of the section and how to extract it from the file. Let’s have a look at all the sections in the file:\n\ngrep ^'#' data/lane1.sorted.bam.bchk | grep 'Use'\n\n\nSummary Numbers (SN)\nThis initial section contains a summary of the alignment and includes some general statistics. In particular, you can see how many bases mapped, and how much of the genome that was covered.\n\n\nExercises\nNow look at the output and try to answer the questions below.\nQ2: What is the total number of reads?\nQ3: What proportion of the reads were mapped?\nQ4: How many pairs were mapped to a different chromosome?\nQ5: What is the insert size mean and standard deviation?\nQ6: How many reads were paired properly?\n\n\nGenerating QC plots\nFinally, we will create some QC plots from the output of the stats command using the command plot-bamstats which is included in the samtools package:\n\nplot-bamstats -p data/lane1-plots/ data/lane1.sorted.bam.bchk\n\nNow in your web browser open the file lane1-plots/index.html to view the QC information.\nQ7: How many reads have zero mapping quality?\nQ8: Which read (forward/reverse) of the first fragments and second fragments are higher base quality on average?\nNow continue to the next section of the tutorial: File conversion."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html",
    "title": "NGS Data formats and QC",
    "section": "",
    "text": "There are several file formats for storing Next Generation Sequencing (NGS) data. In this tutorial we will look at some of the most common formats for storing NGS reads and variant data. We will cover the following formats:\nFASTQ - This format stores unaligned read sequences with base qualities\nSAM/BAM - This format stores unaligned or aligned reads (text and binary formats)\nCRAM - This format is similar to BAM but has better compression than BAM\nVCF/BCF - Flexible variant call format for storing SNPs, indels, structural variations (text and binary formats)\nFollowing this, we will work through some examples of converting between the different formats.\nFurther to understanding the different file formats, it is important to remember that all sequencing platforms have technical limitations that can introduce biases in your sequencing data. Because of this it is very important to check the quality of the data before starting any analysis, whether you are planning to use something you have sequenced yourself or publicly available data. In the latter part of this tutorial we will describe how to perform a QC assessment for your NGS data."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#introduction",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#introduction",
    "title": "NGS Data formats and QC",
    "section": "",
    "text": "There are several file formats for storing Next Generation Sequencing (NGS) data. In this tutorial we will look at some of the most common formats for storing NGS reads and variant data. We will cover the following formats:\nFASTQ - This format stores unaligned read sequences with base qualities\nSAM/BAM - This format stores unaligned or aligned reads (text and binary formats)\nCRAM - This format is similar to BAM but has better compression than BAM\nVCF/BCF - Flexible variant call format for storing SNPs, indels, structural variations (text and binary formats)\nFollowing this, we will work through some examples of converting between the different formats.\nFurther to understanding the different file formats, it is important to remember that all sequencing platforms have technical limitations that can introduce biases in your sequencing data. Because of this it is very important to check the quality of the data before starting any analysis, whether you are planning to use something you have sequenced yourself or publicly available data. In the latter part of this tutorial we will describe how to perform a QC assessment for your NGS data."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#learning-outcomes",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#learning-outcomes",
    "title": "NGS Data formats and QC",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nOn completion of the tutorial, you can expect to be able to:\n\nDescribe the different NGS data formats available (FASTQ, SAM/BAM, CRAM, VCF/BCF)\nPerform a QC assessment of high throughput sequence data\nPerform conversions between the different data formats"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#tutorial-sections",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#tutorial-sections",
    "title": "NGS Data formats and QC",
    "section": "Tutorial sections",
    "text": "Tutorial sections\nThis tutorial comprises the following sections:\n1. Data formats\n2. QC assessment\nIf you have time you can also complete:\n\nFile conversion"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#authors",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#authors",
    "title": "NGS Data formats and QC",
    "section": "Authors",
    "text": "Authors\nThis tutorial was written by Jacqui Keane and Sara Sjunnebo based on material from Petr Danecek and Thomas Keane."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#running-the-commands-from-this-tutorial",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#running-the-commands-from-this-tutorial",
    "title": "NGS Data formats and QC",
    "section": "Running the commands from this tutorial",
    "text": "Running the commands from this tutorial\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal on your computer and type the command below:\n\ncd ~/course_data/data_formats/\n\nNow you can follow the instructions in the tutorial from here."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#lets-get-started",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#lets-get-started",
    "title": "NGS Data formats and QC",
    "section": "Let’s get started!",
    "text": "Let’s get started!\nThis tutorial assumes that you have samtools, bcftools and Picard tools installed on your computer. These are already installed on the VM you are using. To check that these are installed, you can run the following commands:\n\nsamtools --help\n\n\nbcftools --help\n\n\npicard -h\n\nThis should return the help message for samtools, bcftools and picard tools respectively.\nTo get started with the tutorial, go to the first section: Data formats"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html",
    "title": "File conversion",
    "section": "",
    "text": "In this section we are going to look at how to convert from one file format to another. There are many tools available for converting between file formats, and we will use some of the most common ones: samtools, bcftools and Picard."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#sam-to-bam",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#sam-to-bam",
    "title": "File conversion",
    "section": "SAM to BAM",
    "text": "SAM to BAM\nTo convert from SAM to BAM format we are going to use the samtools view command. In this instance, we would like to include the SAM header, so we use the -h option:\n\nsamtools view -h data/NA20538.bam &gt; data/NA20538.sam\n\nNow, have a look at the first ten lines of the SAM file. They should look like they did in the previous section when you viewed the BAM file header.\n\nhead data/NA20538.sam\n\nWell that was easy! And converting SAM to BAM is just as straightforward. This time there is no need for the -h option, however we have to tell samtools that we want the output in BAM format. We do so by adding the -b option:\n\nsamtools view -b data/NA20538.sam &gt; data/NA20538_2.bam\n\nSamtools is very well documented, so for more usage options and functions, have a look at the samtools manual http://www.htslib.org/doc/samtools-1.0.html."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#bam-to-cram",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#bam-to-cram",
    "title": "File conversion",
    "section": "BAM to CRAM",
    "text": "BAM to CRAM\nThe samtools view command can be used to convert a BAM file to CRAM format. In the data directory there is a BAM file called yeast.bam that was created from S. cerevisiae Illumina sequencing data. There is also a reference genome in the directory, called Saccharomyces_cerevisiae.EF4.68.dna.toplevel.fa. For the conversion, an index file (.fai) for the reference must be created. This can be done using samtools faidx. However, as we will see, samtools will generate this file on the fly when we specify a reference file using the -F option.\nTo convert to CRAM, we use the -C option to tell samtools we want the output as CRAM, and the -T option to specify what reference file to use for the conversion. We also use the -o option to specify the name of the output file. Give this a try:\n\nsamtools view -C -T data/Saccharomyces_cerevisiae.EF4.68.dna.toplevel.fa -o data/yeast.cram data/yeast.bam\n\nHave a look at what files were created:\n\nls -l data\n\nAs you can see, this has created an index file for the reference genome called Saccharomyces_cerevisiae.EF4.68.dna.toplevel.fa.fai and the CRAM file yeast.cram.\n\nExercises\nQ1: Since CRAM files use reference-based compression, we expect the CRAM file to be smaller than the BAM file. What is the size of the CRAM file?\nQ2: Is your CRAM file smaller than the original BAM file?\nTo convert CRAM back to BAM, simply change -C to -b and change places for the input and output CRAM/BAM:\nsamtools view -b -T data/Saccharomyces_cerevisiae.EF4.68.dna.toplevel.fa -o data/yeast.bam data/yeast.cram"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#fastq-to-sam",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#fastq-to-sam",
    "title": "File conversion",
    "section": "FASTQ to SAM",
    "text": "FASTQ to SAM\nSAM format is mainly used to store alignment data, however in some cases we may want to store the unaligned data in SAM format and for this we can use the picard tools FastqToSam application. Picard tools is a Java application that comes with a number of useful options for manipulating high-throughput sequencing data. .\nTo convert the FASTQ files of lane 13681_1#18 to unaligned SAM format, run:\n\npicard FastqToSam F1=data/13681_1#18_1.fastq.gz F2=data/13681_1#18_2.fastq.gz O=data/13681_1#18.sam SM=13681_1#18\n\nFrom here you can go on and convert the SAM file to BAM and CRAM, as described previously. There are also multiple options for specifying what metadata to include in the SAM header. To see all available options, run:\n\npicard FastqToSam -h"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#cram-to-fastq",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#cram-to-fastq",
    "title": "File conversion",
    "section": "CRAM to FASTQ",
    "text": "CRAM to FASTQ\nIt is possible to convert CRAM to FASTQ directly using the samtools fastq command. However, for many applications we need the fastq files to be ordered so that the order of the reads in the first file match the order of the reads in the mate file. For this reason, we first use samtools collate to produce a collated BAM file.\n\nsamtools collate data/yeast.cram data/yeast.collated\n\nThe newly produced BAM file will be called yeast.collated.bam. Let’s use this to create two FASTQ files, one for the forward reads and one for the reverse reads:\n\nsamtools fastq -1 data/yeast.collated_1.fastq -2 data/yeast.collated_2.fastq data/yeast.collated.bam\n\nFor further information and usage options, have a look at the samtools manual page (http://www.htslib.org/doc/samtools.html)."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#vcf-to-bcf",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#vcf-to-bcf",
    "title": "File conversion",
    "section": "VCF to BCF",
    "text": "VCF to BCF\nIn a similar way that samtools view can be used to convert between SAM, BAM and CRAM, bcftools view can be used to convert between VCF and BCF. To convert the file called 1kg.bcf to a compressed VCF file called 1kg.vcf.gz, run:\n\nbcftools view -O z -o data/1kg.vcf.gz data/1kg.bcf\n\nThe -O option allows us to specify in what format we want the output, compressed BCF (b), uncompressed BCF (u), compressed VCF (z) or uncompressed VCF (v). With the -o option we can select the name of the output file.\nHave a look at what files were generated (the options -lrt will list the files in reverse chronological order):\n\nls -lrt data\n\nThis also generated an index file, 1kg.bcf.csi.\nTo convert a VCF file to BCF, we can run a similar command. If we want to keep the original BCF, we need to give the new one a different name so that the old one is not overwritten:\n\nbcftools view -O b -o data/1kg_2.bcf data/1kg.vcf.gz\n\nCongratulations you have reached the end of the Data formats and QC tutorial!"
  },
  {
    "objectID": "course_modules/Glossary/glossary.html",
    "href": "course_modules/Glossary/glossary.html",
    "title": "Glossary",
    "section": "",
    "text": "Here is a glossary of key terms based on the information in the provided sources:\n\n1000 Genomes Project: A global collaboration that catalogs human genetic variation across diverse populations. It provides open-access data on common and rare variants, supporting studies in population genetics, ancestry, and disease, and has sequenced individuals from over 25 populations.\n10X Genomics: A technology that generates barcoded libraries where multiple short reads are derived from the same long (~100 kb) DNA fragment, allowing reads from the same molecule to be grouped. This provides long-range sequencing information using standard short-read platforms. The resulting “read clouds” improve resolution of complex repeats, structural variants, and haplotypes, and facilitate scaffolding genome assemblies. Also referred to as linked-reads.\nAlignment and Mapping Formats: File formats that store sequence alignments against a reference genome, such as SAM and BAM.\nAncestral allele: The original state of a DNA sequence before any mutation occurred, describing the evolutionary history of a genetic variant.\nAssembly Metrics: Key quantitative measures used to assess the quality and completeness of a genome assembly, including total length, number of sequences, average length, and N50/NG50.\nBAM (Binary Alignment/Map): A binary version of the SAM format. It is compact, designed to save disk space. BAM files are indexed, allowing for rapid retrieval and visualization of specific genomic regions. They are widely compatible with tools for genomic data analysis, variant calling, and visualization tools like IGV, Ensembl, and UCSC Genome Browser. It is a file format standard maintained by GA4GH.\nBase Quality Filtering: The process of filtering out low-quality base calls (typically below a Phred score of Q20) before genotype calling to ensure accurate variant detection.\nBCF (Binary Call Format): A binary version of VCF. It rearranges fields for faster access and compresses data more effectively, making it ideal for large-scale studies. It is a file format standard maintained by GA4GH.\nBenign variant: A variant classification indicating strong evidence that the variant is not associated with disease.\nBreakDancer: A structural variant (SV) caller that uses paired-end read information to detect SV types including deletions, insertions, inversions, and translocations.\nBurrows-Wheeler Transform (BWT): A data transformation used in the construction of the FM-index, a memory-efficient structure used by suffix/prefix tree-based aligners like BWA and Bowtie.\nCallable genome: Refers to regions of the genome that can be confidently assessed for variants.\nChIP-seq: Chromatin Immunoprecipitation followed by sequencing. A powerful method to analyze protein-DNA interactions on a genome-wide scale, allowing mapping of transcription factor binding sites and histone modifications.\nChromatin shearing (ChIP-seq): Fragmenting chromatin into small pieces (typically 100-500 bp) via sonication or enzymatic digestion, a critical step in the ChIP-seq workflow.\nCIGAR string (Compact Idiosyncratic Gapped Alignment Report): A sequence of operations used in SAM/BAM files to describe how a read is aligned to a reference genome. It specifies matches (M,=,X), insertions (I), deletions (D), soft/hard clipping (S,H), skipped regions (N), and padding (P).\nCircular consensus sequencing (CCS): A PacBio sequencing mode where the same DNA molecule is read multiple times to generate high-accuracy consensus reads (HiFi reads).\nCompute (IT Cost): Refers to the CPU time needed for sequence data analysis, estimated in CPU hours per gigabase of data.\nContig: A contiguous stretch of assembled sequence formed by overlapping reads with no gaps. It represents the first level of genome assembly.\nContig generation: The initial step in genome assembly where overlapping sequencing reads are stitched together into continuous sequences.\nCRAM: A highly efficient file format for storing aligned sequencing data, offering significant compression improvements over BAM. It incorporates reference-based compression, controlled loss of quality information, and optimized compression methods. It is a file format standard maintained by GA4GH.\nDESeq2: A statistical tool for differential gene expression (DGE) analysis in RNA-Seq that uses size factors to account for sequencing depth and advanced statistical modeling to estimate gene-specific variance.\nDifferential gene expression (DGE): The step in RNA-Seq analysis that assesses whether gene or transcript expression levels differ between sample groups using statistical tools.\nDiploid assembly: Genome assembly that aims to reconstruct both sets of chromosomes separately, generating two haplotype-phased genomes that preserve allelic variation.\nDNA replication errors: Changes introduced into the genome during the process of cell division.\nDNase I hypersensitivity: Regions of open chromatin that are more readily cleaved by the DNase I enzyme due to reduced nucleosome occupancy, often corresponding to functional regulatory elements.\nFASTA: A file format that stores raw or processed nucleotide and protein sequences. It consists of a header line (starting with “&gt;”) followed by sequence lines.\nFASTQ: A simple file format for raw unaligned sequencing reads. It is an extension of FASTA, composed of the sequence and an associated per base quality score encoded in ASCII characters.\nFiltering step (Variant Calling): An essential step in variant calling to eliminate false positives that can arise from contamination, PCR errors, sequencing artifacts, or mapping errors.\nFiber-FISH: A high-resolution cytogenetic technique used to visualize the physical arrangement and organization of genomic regions along stretched DNA molecules using fluorescent probes.\nFixed allele thresholds: An approach in variant calling that applies predefined allele frequency cutoffs (e.g., 0.0, 0.5, 1.0) to assign genotypes.\nFM-index: A more compact and memory-efficient data structure built on the Burrows-Wheeler Transform (BWT), commonly used in suffix/prefix tree-based aligners.\nFPKM (Fragments Per Kilobase per Million): A normalization method for RNA-Seq data, a refinement of RPKM designed for paired-end sequencing that accounts for two reads mapping to a single fragment.\nFragment size quality control (QC) (SV): A method for detecting potential structural variation by examining the insert size distribution from paired-end reads in BAM files.\nFRiP score (Fragments In Peaks): A metric for ChIP-seq data quality that quantifies the proportion of total sequencing fragments that fall within confidently called peak regions. A score &gt;1% is generally acceptable.\nFunctional annotation (Variants): A critical step in variant interpretation where additional biological context (e.g., location in coding region, predicted consequence) is added to variants in a VCF file.\nFunctional enrichment: Interpreting lists of differentially expressed genes by using tools for Gene Ontology (GO) term enrichment or pathway analysis.\nGap (N): A placeholder for missing or unresolvable sequence in scaffolds, represented by the letter “N” in assembled genomes.\nGA4GH (Global Alliance for Genomics and Health): An international coalition dedicated to creating frameworks and standards for sharing genomic and clinical data. It maintains file format standards like CRAM, SAM/BAM, and VCF/BCF.\nGenome assembly: The process of reconstructing an organism’s genome from millions or billions of short DNA sequences generated by sequencing.\nGenome browsers: Tools such as IGV, Ensembl, or UCSC Genome Browser that allow visualization of aligned reads and signal tracks from ChIP-seq data.\nGenomic structural variation (SV): Any rearrangement of chromosome structure, including insertions, deletions, inversions, translocations, and copy number changes. SVs are often defined as being &gt;50 bp.\nGenotype-phenotype association studies: Applications using genetic data to uncover the genetic basis of traits by linking genotype information to observable characteristics.\nGERMQ (Germline Quality): In cancer genomics VCF annotations, a Phred-scaled quality score indicating whether alternate alleles are unlikely to be germline variants.\nGermline mutation: Mutations that occur in egg or sperm cells and are heritable, meaning they can be passed on to subsequent generations.\ngnomAD (Genome Aggregation Database): Aggregates and harmonizes exome and genome sequencing data to provide allele frequencies across populations, helping researchers filter out common variants.\nHaplotype: A group of linked variants (alleles) inherited together on the same chromosome segment.\nHash Table-Based Alignment: A common NGS alignment approach that relies on indexing short, fixed-length sequences (k-mers) from the reference or reads to rapidly find candidate mapping positions.\nHi-C: A sequencing technique that captures the three-dimensional organization of chromatin through proximity ligation, used for scaffolding genome assemblies into chromosome-scale sequences.\nHISAT2: A widely used splice-aware genome aligner for RNA-Seq reads, known for balancing speed, accuracy, and low memory usage, and good at discovering novel splice junctions.\nHistone Code: The concept that specific combinations of histone modifications are associated with distinct functional states of chromatin and can be used to define functional elements across the genome.\nHistone modifications: Post-translational modifications (methylation, acetylation, etc.) on histone tails that play a central role in regulating chromatin structure and gene expression.\nHomopolymers: Stretches of the same base in a DNA sequence, which are prone to systematic sequencing errors and mapping errors.\nIGV (Integrative Genomics Viewer): A popular genome browser and visualization tool compatible with BAM files and used for visualizing structural variants and RNA-Seq/ChIP-seq data.\nInsert Size: The distance between the start of Read 1 and the end of Read 2, excluding adapter sequences.\nInsertion: A structural variant where a new DNA segment is added into a genomic location.\nIntergenic variant: A sequence variant located in the region between genes.\nIntron variant: A sequence variant occurring within an intron.\nInversion: A structural variant where a segment of DNA is reversed in orientation. Can cause paired reads to map with incorrect orientation.\nKallisto: A tool that maps RNA-Seq reads to a reference transcriptome using pseudoalignment, providing a much faster alternative to genome mapping and including quantification as part of the process.\nk-mers: Short, fixed-length nucleotide sequences (e.g., a 31-mer is 31 nucleotides) used in hash table-based alignment approaches.\nLinear reference genome: A standard single reference sequence representation of a genome.\nLocal de novo assembly-based variant callers: Emerging variant calling approaches that reconstruct short regions of the genome directly from reads to detect variants simultaneously.\nLocal realignment: A step sometimes performed on BAM files to correct misalignments that occur near insertions and deletions (indels).\nLow-quality base calls: Sequencing errors are more likely to occur at these positions, which can lead to incorrect genotype assignments if not filtered.\nLUMPY: A flexible and probabilistic structural variant (SV) caller that integrates multiple sources of alignment evidence (read pairs, split reads, read depth) into a unified discovery process.\nMapping-based variant calling: An approach to variant calling where raw sequencing reads are aligned directly to a reference genome.\nMapping Quality: A score that reflects how confident the aligner is that a sequencing read has been correctly placed on the reference genome. Expressed as a Phred-scaled score (higher values = greater confidence).\nMapping RNA-Seq Reads: The process of determining which genes or transcripts sequencing reads belong to, done through mapping to a reference genome or transcriptome\nMetagenomic assembly: Reconstructing genomic sequences from DNA extracted from complex samples often containing multiple uncultured microbial species.\nMicrosatellites / Short Tandem Repeats (STRs): Short, repetitive DNA sequences (2–6 bp repeats) where the number of repeats varies between individuals, useful for genetic fingerprinting. Prone to sequencing errors.\nMissense variant: A sequence variant that changes one or more amino acids in a protein.\nMotif Analysis (ChIP-seq analysis): A step in ChIP-seq data interpretation to identify short, recurring DNA sequences (motifs) that are recognized by transcription factors.\nMutation / Variation: Any change in the DNA base sequence.\nNaive variant calling: A straightforward interpretation of observed sequencing data, often assuming ideal conditions and using limited filtering or statistical modeling.\nNanopore sequencing: A technology offering real-time, portable DNA/RNA sequencing with ultra-long reads (up to 1 megabase) and detection of base modifications. It has a high raw error rate, particularly in homopolymers.\nNG50: A variant of the N50 assembly metric that is calculated using the expected genome size rather than the total assembly length.\nNGMLR (Next-Generation Mapping and Long Read aligner): An aligner specifically designed for long-read sequencing data. It uses a convex gap-cost scoring model to better handle large insertions, deletions, and complex rearrangements commonly found in long reads.\nNoise (ChIP-seq): Background signal in ChIP-seq experiments arising from non-specific DNA–protein interactions, incomplete washing, or DNA fragments not associated with the protein of interest.\nNon-Redundant Fraction (NRF): A metric for library complexity in ChIP-seq, calculated as the number of unique fragment positions divided by the total number of mapped fragments. An NRF &gt; 0.8 is recommended.\nNormalization (RNA-Seq): A crucial step in RNA-Seq analysis to correct for technical biases like sequencing depth and gene length. Common methods include RPKM, FPKM, and TPM.\nNovel Insertion: A new DNA segment not present in the reference genome that is inserted into a genomic location.\nN50: An assembly metric representing the length X such that 50% of the total assembly is contained in contigs or scaffolds of at least that length.\nOpen chromatin: A relaxed state of chromatin that allows regulatory proteins to access the DNA.\nOptical mapping: A genome analysis technique that uses high molecular weight DNA fragments to generate a physical map of the genome based on the location of specific sequence motifs, useful for scaffolding and validating SVs.\nPacBio (Pacific Biosciences) sequencing: Sequencing technology based on single-molecule real-time (SMRT) sequencing that generates long reads and can detect methylated bases.\nPaired-end sequencing: Sequencing both ends of a DNA fragment. Improves mapping accuracy, especially in repetitive regions, and provides information for detecting SVs.\nParallel computing: Using multiple CPU cores or splitting data across computers to speed up computational workflows.\nPathogenic variant: A variant classification indicating strong evidence supports a disease-causing role.\nPCA (Principal Component Analysis): A dimensionality reduction technique used to transform high-dimensional data and spot outlying samples by visualizing principal components.\nPeak Calling (ChIP-seq analysis): The computational process to identify genomic regions enriched in sequencing reads (peaks), representing potential protein–DNA interaction sites or histone modifications. Uses statistical algorithms to compare read counts to background.\nPeak summits (Motif Analysis): The highest enrichment points within peaks in ChIP-seq data, which are likely to contain the core transcription factor binding site for motif analysis.\nPhred quality score: A score used to encode the quality of sequenced nucleotides in FASTQ files, translated from ASCII characters. Also used for base quality filtering (e.g., Q20) and mapping quality. Higher values indicate greater confidence in accuracy.\nPON (Panel of Normals): In cancer genomics VCF annotations, marks sites that are found in a reference panel of normal samples, helping to identify potential artifacts.\n(Pseudo)chromosome assignment: The process of assembling scaffolds into chromosome-scale sequences (pseudomolecules) using external evidence like a reference genome or genetic maps.\nPseudoalignment (Kallisto): An efficient method used by Kallisto to determine which transcripts reads belong to without calculating exact alignment positions.\nQ-values: Adjusted p-values reported by RNA-Seq differential expression tools to control false discovery rates when performing multiple testing.\nRead 1 / Read 2: The two ends of the same DNA fragment that are sequenced separately in paired-end sequencing.\nRead depth: The number of reads that align to a specific genomic position.\nReference allele: The original allele at a given position in the reference genome. In VCF format, it is always denoted as 0.\nReference Genome: A digital version of a species’ DNA used as a standard for comparison.\nReference-Based Compression: A method used in formats like CRAM that leverages the reference genome to reduce storage redundancy. Instead of storing full sequences, only deviations or differences from the reference are stored.\nRepeat Insertion: Insertion of a repetitive DNA element, such as a transposable element or tandem repeat, into a new genomic location.\nRepetitive elements: Regions of the genome, such as transposable elements, that are repeated and can make it difficult to determine the true origin of a read during alignment.\nRetrotransposition: The movement of transposable elements (TEs) within the genome via an RNA intermediate.\nRibosomal RNA (rRNA): A major component of total RNA in a cell, often constituting over 90%, which needs to be depleted during library preparation for mRNA sequencing.\nRPKM (Reads Per Kilobase per Million): A normalization method for RNA-Seq data that adjusts for sequencing depth and gene length.\nSAM (Sequence Alignment/Map): A text-based format for storing biological sequences aligned to a reference genome. It consists of a header section (starting with @) and an alignment section (one line per read). It is a file format standard maintained by GA4GH.\nScaffold: An ordered set of contigs that are connected by gaps (usually filled with Ns) representing unknown or unsequenced regions. Also called a supercontig.\nScaffolding: A crucial step in genome assembly that aims to order and orient contigs into larger, more complete sequences called scaffolds using long-range data like paired-end reads, linked reads, Hi-C, or optical maps.\nSeed and Extend: A foundational alignment strategy (e.g., used in BLAST) where short matching subsequences (seeds) identify candidate regions in a database, which are then extended to find high-scoring alignments.\nSequence logo (Motif Analysis): A visual representation of a consensus motif identified in ChIP-seq data, where letter height indicates relative frequency and stack height represents information content at each position.\nSequencing coverage (Assembly): Critical in genome assembly; higher coverage improves assembly reliability and helps resolve ambiguities.\nSequencing depth (RNA-Seq): Determines how well transcripts can be detected in RNA-Seq experiments.\nSheared Fragment: A random piece of genomic DNA produced by physically or enzymatically breaking the genome into smaller segments for sequencing library preparation.\nShort reads: Reads generated by sequencing technologies that are short in length, making them challenging to align confidently, especially in low-complexity or repetitive genomes.\nSignal (ChIP-seq): Represents DNA fragments that are specifically bound by the protein of interest and are enriched in sequencing data, typically appearing as peaks.\nSignal tracks (ChIP-seq visualization): Files (e.g., WIG or bedGraph) that convert mapped reads into a format reflecting read density, used by genome browsers to display ChIP-seq enrichment.\nSingle Nucleotide Variants (SNVs): Base substitutions (e.g., A→C).\nSize Fractionation: A method to select DNA fragments of a particular length range to improve consistency of read lengths for sequencing.\nSize factors (DGE tools): Used by RNA-Seq differential expression tools (e.g., DESeq2) to account for differences in sequencing depth between samples.\nSleuth: A tool for DGE and QC analysis specifically designed to work with data from pseudoalignment tools like Kallisto.\nSmith-Waterman Algorithm: A local alignment algorithm used to compute the optimal pairwise alignment between a read and a reference in a specific region, helping determine the precise match structure and CIGAR string. It is computationally intensive.\nSniffles: A structural variant (SV) caller specifically designed to detect SVs from long-read sequencing data using a multi-evidence approach.\nSNP co-occurrence: Patterns of single nucleotide polymorphisms (SNPs) inherited together, captured by haplotypes.\nSNPs: Single nucleotide polymorphisms, variants involving a single base change.\nSoft clipping (S): A CIGAR operation indicating query sequence bases that are not aligned but are present in the sequence. Often seen at structural variant breakpoints.\nSomatic mutation: Mutations that arise in non-germline tissues and do not get inherited by offspring.\nSplice acceptor variant: A splice variant that changes the 2 base region at the 3’ end of an intron.\nSplice donor variant: A splice variant that changes the 2 base region at the 5’ end of an intron.\nSplice region variant: A sequence variant in which a change has occurred within the region of the splice site.\nSplice-aware aligners (RNA-Seq): Alignment tools required for RNA-Seq reads mapping to a genome (especially eukaryotic) to handle introns that are spliced out of mature mRNA.\nSplit-read alignments (SV): Occur when a single sequencing read spans a structural variant junction and aligns in two separate parts to different positions on the reference genome, providing evidence for SVs and breakpoints.\nStart lost: A sequence variant whereby the start codon is changed.\nStop gained: A sequence variant resulting in a premature stop codon.\nStop lost: A sequence variant whereby the stop codon is changed, resulting in the extension of the translated product.\nStranded library (RNA-Seq): A library preparation protocol that preserves the information about which DNA strand the RNA was transcribed from, valuable for distinguishing overlapping or antisense transcripts.\nSubclonal (somatic): In somatic variant calling (e.g., in cancer), mutations present in a subset of cells within the sample, leading to variable allele frequencies.\nSuffix/Prefix Tree-Based Aligners: Use specialized data structures like the suffix tree or FM-index to enable rapid and efficient string matching during sequence alignment.\nSV visualisation: Visualizing structural variants in genome browsers (like IGV) by inspecting discordant read pairs, split reads, and soft-clipped bases.\nSynonymous variant: A sequence variant where there is no resulting change to the encoded amino acid.\nTechnical replicates: Repeated measurements of the same sample, used to assess variation introduced by equipment or protocols.\nThird-generation sequencing (SV): Technologies (e.g., PacBio, Oxford Nanopore) that generate long reads (several kilobases), which can span complex or repetitive regions, enabling more accurate detection and mapping of structural variants.\nTotal Length (Assembly): The combined length of all contigs or scaffolds in a genome assembly.\nTotal RNA: A mixture of RNA types present in a cell, including mRNA, rRNA, tRNA, and regulatory RNAs.\nTranscript abundance: A measure of how much of a particular RNA molecule is present, revealed by aligning RNA-Seq reads to the genome or transcriptome.\nTranscript amplification: A feature amplification of a region containing a transcript.\nTranscript ablation: A feature ablation whereby the deleted region includes a transcript feature.\nTranscriptome assembly: Reconstructing expressed RNA sequences (cDNA) from a biological sample, complicated by varying transcript abundance and alternative splicing.\nTranscriptome mapping (RNA-Seq): Mapping reads to a reference transcriptome rather than the genome, often much faster.\nTranscriptome: The complete set of transcripts present in a cell, along with their quantities, at a specific developmental stage or under specific conditions.\nTranscription factors (TFs): Specific regulatory proteins that recognize and bind to specific DNA sequences (often in accessible chromatin regions) to regulate gene expression.\nTranscriptional regulation: A dynamic process governed by the interaction between DNA and specific regulatory proteins, most notably transcription factors, and influenced by chromatin state.\nTransitions (Ts): Base substitutions between purines (A↔︎G) or between pyrimidines (C↔︎T).\nTransitions-to-Transversions (Ts/Tv) ratio: A common method for estimating the quality of called SNPs; a high ratio (typically 2-3:1 in the genome) is generally indicative of a high-quality SNP dataset.\nTranslocation: A type of structural variation where a segment of a chromosome is moved to a different location, either within the same chromosome or to a different chromosome.\nTransposable elements (TEs): Segments of DNA capable of changing their position within the genome. They are a dominant feature of mammalian genomes. Also referred to as repetitive elements.\nTransversions (Tv): Base substitutions between a purine and a pyrimidine (e.g., A↔︎C, G↔︎T).\nTPM (Transcripts Per Million): A normalization method for RNA-Seq data.\nUndercall heterozygotes: A problem associated with traditional variant calling methods, especially in low-coverage data, where it is difficult to detect both alleles.\nUnstranded library (RNA-Seq): A library preparation protocol that does not preserve information about the DNA strand the RNA was transcribed from.\nUpstream gene variant: A sequence variant located upstream of a gene.\nUTR variant (3’/5’): A sequence variant in the 3’ or 5’ untranslated region of a gene.\nVariant Call Format (VCF): A widely-used, standardized text file format for storing genetic variation data, including SNPs, indels, and structural variants. It is a tab-delimited format and integral to genomic workflows. It is compressed using BGZF and indexed with TBI/CSI. It is a file format standard maintained by GA4GH.\nVariant calling: The process of identifying differences in DNA sequences by comparing a sample’s sequence to a reference genome to determine the genotype at each position.\nVariant depth: Typically indicates the average read coverage at a variant position, helping assess the confidence of the identified variant.\nVariant pathogenicity: The classification of genetic variants based on their potential to cause disease, typically categorized into pathogenic, likely pathogenic, uncertain significance (VUS), likely benign, and benign.\nVariant calling workflows: Approaches to variant calling, including mapping-based and assembly-based, using either a linear reference genome or a pangenome.\nVariants of Uncertain Significance (VUS): Variant classification where there is limited or conflicting evidence; cannot be classified as pathogenic or benign.\nVariation graphs: Emerging approaches in variant calling that allow reads to align to a graph representation of multiple reference paths (a pangenome) rather than a single linear genome, promising improved sensitivity.\nVisualization (ChIP-seq analysis): Inspecting mapped reads in genome browsers (e.g., IGV, UCSC) to assess enrichment patterns and signal quality.\nVNTR (Variable Number Tandem Repeat): Short sequences of DNA repeated in a tandem array, where the number of repeats varies among individuals, useful for DNA fingerprinting.\nVolcano plots: Visualization tools useful for identifying differentially expressed genes by plotting effect size (e.g., log fold-change) against significance (e.g., p-value).\nWhole-genome shotgun assembly: A genome assembly strategy involving randomly fragmenting the entire genome, sequencing all fragments, and using computational methods to assemble them."
  },
  {
    "objectID": "course_modules/Train-the-trainer/strategies.html",
    "href": "course_modules/Train-the-trainer/strategies.html",
    "title": "Strategies for training professionals",
    "section": "",
    "text": "What is Pedagogy?\nPedagogy refers to the art of learning and teaching, based on how we learn and how we accommodate this learning. It describes the theory of learning and teaching and the methods and activities of teaching.\nWhile pedagogy was coined to refer to teaching and learning of children, andragogy is the term for adult learning theory. Even though this course deals with teaching of adults, pedagogy and andragogy will be used interchangeably, as the term pedagogy is often used as a synonym for teaching theories in general.\nWhat is Andragogy?\nAndragogy was proposed by M.S. Knowles in 1968. Knowles recognized that there are differences in the ways that adults learn as opposed to children. Many researchers agree that the self-directed approach to learning discussed by Knowles is applicable in a number of settings. Therefore, Knowles’ assumptions of andragogy should be considered when designing and facilitating training programs for adult learners as these assumptions aid adult learners in applying new knowledge and skills to their professional environments. \n\n\n\nAdults are self-directed learners, as they are more mature and have more secure self-concept, this allows them to take part in directing their own learning\nAdults have a vast array of experiences to draw on as they learn\nAdults are ready/interested to learn: adults have reached a point in which they see/accept the value of education and are ready to focus on learning\nAdults are often looking for practical, problem-centered approaches to learning, that they can apply in their own contexts. Many adults return to continuing education for specific practical reasons, such as entering a new field.\nAdults have strong internal motivation to learn\n\nA later sixth assumption posits that Adults are relevancy-oriented. Adult learners need to understand why learning new knowledge or skills is important. \n**What do you think about these principles of adult learning?\nDo you have an example from your experience as a learner or trainer to illustrate your views?**",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Strategies for training professionals"
    ]
  },
  {
    "objectID": "course_modules/Train-the-trainer/strategies.html#learning-theory---pedagogy-and-andragogy",
    "href": "course_modules/Train-the-trainer/strategies.html#learning-theory---pedagogy-and-andragogy",
    "title": "Strategies for training professionals",
    "section": "",
    "text": "What is Pedagogy?\nPedagogy refers to the art of learning and teaching, based on how we learn and how we accommodate this learning. It describes the theory of learning and teaching and the methods and activities of teaching.\nWhile pedagogy was coined to refer to teaching and learning of children, andragogy is the term for adult learning theory. Even though this course deals with teaching of adults, pedagogy and andragogy will be used interchangeably, as the term pedagogy is often used as a synonym for teaching theories in general.\nWhat is Andragogy?\nAndragogy was proposed by M.S. Knowles in 1968. Knowles recognized that there are differences in the ways that adults learn as opposed to children. Many researchers agree that the self-directed approach to learning discussed by Knowles is applicable in a number of settings. Therefore, Knowles’ assumptions of andragogy should be considered when designing and facilitating training programs for adult learners as these assumptions aid adult learners in applying new knowledge and skills to their professional environments. \n\n\n\nAdults are self-directed learners, as they are more mature and have more secure self-concept, this allows them to take part in directing their own learning\nAdults have a vast array of experiences to draw on as they learn\nAdults are ready/interested to learn: adults have reached a point in which they see/accept the value of education and are ready to focus on learning\nAdults are often looking for practical, problem-centered approaches to learning, that they can apply in their own contexts. Many adults return to continuing education for specific practical reasons, such as entering a new field.\nAdults have strong internal motivation to learn\n\nA later sixth assumption posits that Adults are relevancy-oriented. Adult learners need to understand why learning new knowledge or skills is important. \n**What do you think about these principles of adult learning?\nDo you have an example from your experience as a learner or trainer to illustrate your views?**",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Strategies for training professionals"
    ]
  },
  {
    "objectID": "course_modules/Train-the-trainer/strategies.html#application-of-adult-learning-principles-in-approaches-to-trainingteaching",
    "href": "course_modules/Train-the-trainer/strategies.html#application-of-adult-learning-principles-in-approaches-to-trainingteaching",
    "title": "Strategies for training professionals",
    "section": "Application of adult learning principles in approaches to training/teaching",
    "text": "Application of adult learning principles in approaches to training/teaching\nLearning and teaching is an active process where knowledge and authority is shared between teachers and learners. The teacher’s/trainer’s role becomes more one of a facilitator or a guide. Knowles summarised four key principles and practice considerations for adult learning:\n\nSince adults are self-directed, they should have a say in the content and process of their learning.\nBecause adults have so much experience to draw from, their learning should focus on adding to what they have already learned in the past.\nSince adults are looking for practical learning, content should focus on issues related to their work or personal life/career.\nAdditionally, learning should be centred on achieving higher cognitive levels beyond memorising content.\n\nThe assumptions of andragogy shape the design of training. According to Knowles et al. (2015), in the traditional context model the instructor decides in advance what knowledge or skills need to be transmitted, develops the content into logical units, selects the most appropriate delivery strategies (lectures, readings, laboratory exercises etc.), and then develops a delivery plan.\nFollowing an andragogical approach, ideally, the instructor prepares in advance a set of procedures for involving the learners and other relevant parties in a process involving these elements:\n\npreparing the learner;\nestablishing a climate conducive to learning;\ncreating a mechanism for mutual planning;\ndiagnosing the needs for learning;\nformulating program objectives (content) that will satisfy these needs;\ndesigning a range of learning experiences;\nconducting these learning experiences with suitable techniques and materials;\nevaluating the learning outcomes and diagnosing needs for improvement \n\nFurther reading:\nOther educators have based their teaching practice on variations of these principles. Vandenberg outlines five basic principles and assumptions for adult learners and learner actions or conditions which ensures that learners really do learn: Facilitating Adult Learning.\nCombining Knowles’ and Vandenberg’s assumptions, this Table outlines what trainers/facilitators could do for effective adult learning.",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Strategies for training professionals"
    ]
  },
  {
    "objectID": "course_modules/Train-the-trainer/strategies.html#from-teacher-to-facilitator-of-learning-and-trainer-of-professionals-or-experts",
    "href": "course_modules/Train-the-trainer/strategies.html#from-teacher-to-facilitator-of-learning-and-trainer-of-professionals-or-experts",
    "title": "Strategies for training professionals",
    "section": "From teacher to facilitator of learning and trainer of professionals or experts",
    "text": "From teacher to facilitator of learning and trainer of professionals or experts\nTraditional role of a teacher  is that of being responsible for what learners should learn, how, when and if they have learned. They transmit prescribed content, controlling the way learners receive it and use it, and at the end test their knowledge. This is the model many of us know and have been through. \nFacilitator of learning on the other hand, uses existing knowledge, experience and motivation of learners to shape the learning experience. From content transmitter, the role shifts to learning process management. While the content planning and transmission in traditional model requires primarily presentation skills, facilitator has to be able to perform the function of process designer and manager, and that requires relationship building, needs assessment, involvement of students in planning, linking students to resources, encouraging students’ initiative. \nTraining professionals apply adult learning principles bycreating and preparing the learning environment conducive to learning. They consider the characteristics of their target audience, main described by adult learning principles we talked about in the previous steps. They also constantly evaluate their teaching looking to make improvements.\nHere we will present how the adult learning principles can be translated into more concrete teaching/training strategies, so that as trainers we can use methods and tools to better accommodate the learning process of our adult learners, as part of the facilitation process.",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Strategies for training professionals"
    ]
  },
  {
    "objectID": "course_modules/Train-the-trainer/strategies.html#social-learning",
    "href": "course_modules/Train-the-trainer/strategies.html#social-learning",
    "title": "Strategies for training professionals",
    "section": "Social learning",
    "text": "Social learning\n\nWhen designing training, we should always take into account the social context where learning takes place, as learning is an active, contextualized process of constructing knowledge, rather than pure acquisition of information.\nLearning occurs through social interactions, and others in the community play important role in the process of learning.\nThis concept is based on social constructivism, a theory of how people learn. The theory positions social interactions, such as conversations between teacher and learner, as important in articulating what is already known so that new ideas can be introduced in the context of current understanding.\nL Vygotksy, the founder of social constructivism, proposed the idea of a Zone of Proximal Development (ZPD) to explain how learners master new ideas and skills under guidance from an expert.\nBuilding on this, E. Wenger showed that learning and value creation in networked Communities of Practice happen through engagement and social interaction of its members. \nIn the context of this course, social learning will be encouraged through opportunities for knowledge and experience sharing.\nFind below some examples and definitions of ways social learning can take place.\nKnowledge Sharing: Directly passing on ideas and information to each other\nDirect learning from others: Asking questions, getting responses\nShared knowledge building: Development of concept through dialogue and interaction\nGroup learning: Orchestrated collaboration, exploration and creation\nConversational learning: Discussion, social interaction\nVicarious learning: Observation of other people’s learning\nImplicit learning: Incidental learning",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Strategies for training professionals"
    ]
  },
  {
    "objectID": "course_modules/Train-the-trainer/strategies.html#active-learning-and-shared-responsibility",
    "href": "course_modules/Train-the-trainer/strategies.html#active-learning-and-shared-responsibility",
    "title": "Strategies for training professionals",
    "section": "Active learning and shared responsibility",
    "text": "Active learning and shared responsibility\n\nActive learning\nThis course involves the course participants as well as the course instructors, in an active process of learning and training. This will mean maximising hands-on, problem solving activities, use of different tools and technology, case studies, group work and collaboration, making of learning plans and reflecting on learning. The instructors on this course will prepare the activities which will encourage active learners involvement, motivate and support learners and provide opportunities for formative assessment throughout the course. \nActive learners and trainers have their responsibilities. Based on what you learned in previous sections, can you decide on where the following responsibilities belong:\n\n\n\nObstacles to learning/training genomics and bioinformatics\nCreate a new discussion topic mentioning a major challenge that affects your ability to train effectively in genomics or bioinformatics. Respond to your own and other’s challenges with suggestions of how these barriers might be overcome.\nAreas you might consider: availability of time and resources, trainer knowledge and skills, reaching the right target audience, lack of institutional support, etc\n\n\nForum: Discuss learning outcomes for this course\n\n\nHere are the top level Learning Outcomes for this course:\n\nExplain the flow of pathogen genomic data/information from generation to interpretation in healthcare and public health settings.\nDesign training and communication formats by applying evidence-based learning science methodologies.\nIdentify appropriate training resources for use in training pathogen genomics and surveillance tools.\nDeliver pathogen genomic data science training to professionals working in genomic epidemiology, surveillance and outbreak investigation.\nEvaluate the self-developed training and knowledge sharing of pathogen genomic data science.\n\nPick one of the outcomes and start a thread in this forum with that name (if someone else has started it already, please use that thread). For each learning outcome, discuss the Bloom’s levels, structure of the sentence (verb, object and qualifying phrase) and comment on whether that particular outcome is written in a SMART manner.",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Strategies for training professionals"
    ]
  },
  {
    "objectID": "course_modules/Train-the-trainer/strategies.html#scope-of-training-and-teaching",
    "href": "course_modules/Train-the-trainer/strategies.html#scope-of-training-and-teaching",
    "title": "Strategies for training professionals",
    "section": "Scope of training and teaching",
    "text": "Scope of training and teaching\nMost of what we taught in this module can be applied to different scopes and formats of teaching/training. The trainer’s or teacher’s knowledge, skills and attitudes we talked about are needed to apply an active, learner centred approach to teaching/training . However, we paid most attention to training of adult professionals that often takes place via short courses, workshops, seminars, on job training (or in-person, virtual or hybrid equivalents).  You might be working or planning to work on a university course or even the programme consisting of more than one course. You will still be able to use the principles of learning and training described in this module and further in Module 2, in other contexts where your work might be situated.",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Strategies for training professionals"
    ]
  },
  {
    "objectID": "course_modules/Train-the-trainer/introduction.html",
    "href": "course_modules/Train-the-trainer/introduction.html",
    "title": "Introduction",
    "section": "",
    "text": "What this module covers:\nThis introductory module gives you a chance to explore your own memories and habits regarding learning and training. \nIt also explains educational theory that underpins adult teaching and training.\nIntended learning outcomes for this module:\n\nList main principles underpinning adult learning \nDescribe practical application of adult learning principles \nExplain outcome driven design approach for use in training and teaching \nDefine learning outcomes in a SMART way using Bloom’s taxonomy\nDiscuss purpose, scope and types of evaluation in education\n\nThis module takes approximately 2 hours to go through.\n\n\nRecall\n\nYour happiest memory of learning.\nWhat experience they remember as being most fun, most effective, and perhaps most influential?\nYou happiest memory of teaching or training.\nWhat experience do you remember as being most fun, most effective, and perhaps most influential ?\nDo you teach or train the way you like to learn? Why? or Why not?",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Introduction"
    ]
  },
  {
    "objectID": "course_modules/Train-the-trainer/introduction.html#overview-of-adult-learning-principles-and-training-design-techniques",
    "href": "course_modules/Train-the-trainer/introduction.html#overview-of-adult-learning-principles-and-training-design-techniques",
    "title": "Introduction",
    "section": "",
    "text": "What this module covers:\nThis introductory module gives you a chance to explore your own memories and habits regarding learning and training. \nIt also explains educational theory that underpins adult teaching and training.\nIntended learning outcomes for this module:\n\nList main principles underpinning adult learning \nDescribe practical application of adult learning principles \nExplain outcome driven design approach for use in training and teaching \nDefine learning outcomes in a SMART way using Bloom’s taxonomy\nDiscuss purpose, scope and types of evaluation in education\n\nThis module takes approximately 2 hours to go through.\n\n\nRecall\n\nYour happiest memory of learning.\nWhat experience they remember as being most fun, most effective, and perhaps most influential?\nYou happiest memory of teaching or training.\nWhat experience do you remember as being most fun, most effective, and perhaps most influential ?\nDo you teach or train the way you like to learn? Why? or Why not?",
    "crumbs": [
      "Home",
      "Train-the-trainer",
      "Introduction"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7_manual.html",
    "href": "course_modules/Module7/module7_manual.html",
    "title": "Manual",
    "section": "",
    "text": "Transcriptional regulation in eukaryotic cells is a dynamic process governed by the interaction between DNA and specific regulatory proteins, most notably transcription factors (TFs). These proteins recognize and bind to specific DNA sequences, typically located in accessible regions of chromatin. Chromatin can exist in a compact (heterochromatin) or relaxed (euchromatin) state, with the latter allowing regulatory proteins to access the DNA.\nRegions of open chromatin are often characterized by DNase I hypersensitivity, meaning they are more readily cleaved by the DNase I enzyme due to reduced nucleosome occupancy. These hypersensitive sites frequently correspond to functionally important regulatory elements, such as promoters, enhancers, silencers, and insulators (Gross & Garrard, 1988).\nTo study these protein–DNA interactions in a cellular context, researchers use Chromatin Immunoprecipitation (ChIP). This technique involves crosslinking DNA and associated proteins, fragmenting the chromatin, and using antibodies specific to the protein of interest to isolate the bound DNA fragments. These fragments can then be analyzed by quantitative PCR, microarray (ChIP-chip), or sequencing (ChIP-seq), allowing for genome-wide mapping of protein-binding sites (Orlando, 2000; Barski et al., 2007; Johnson et al., 2007).\nReferences:\nGross, D. S., & Garrard, W. T. (1988). Nuclease hypersensitive sites in chromatin. Annual Review of Biochemistry, 57(1), 159–197.\nOrlando, V. (2000). Mapping chromosomal proteins in vivo by formaldehyde-crosslinked-chromatin immunoprecipitation. Trends in Biochemical Sciences, 25(3), 99–104.\nBarski, A., et al. (2007). High-resolution profiling of histone methylations in the human genome. Cell, 129(4), 823–837.\nJohnson, D. S., et al. (2007). Genome-wide mapping of in vivo protein-DNA interactions. Science, 316(5830), 1497–1502.\nHere is an instructional text suitable for a teaching module, explaining how ChIP-seq works, based on the concepts illustrated in your figure:",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7_manual.html#section",
    "href": "course_modules/Module7/module7_manual.html#section",
    "title": "Manual",
    "section": "",
    "text": "Overview of ChIP-seq: Chromatin Immunoprecipitation Followed by Sequencing\n\n\n\nChiP-seq\n\n\nChromatin Immunoprecipitation followed by sequencing (ChIP-seq) is a powerful method used to analyze protein-DNA interactions on a genome-wide scale. It allows researchers to determine the binding sites of DNA-associated proteins, such as transcription factors and histone modifications, across the entire genome (Johnson et al., 2007; Barski et al., 2007).\nThe ChIP-seq workflow consists of five main steps:\nCrosslinking and Chromatin Shearing: Cells are first treated with a crosslinking agent, typically formaldehyde, to stabilize protein-DNA interactions. The chromatin is then sheared into small fragments via sonication or enzymatic digestion (Solomon et al., 1988).\nFormation of DNA-Protein Complexes: The fragmented chromatin contains various protein-DNA complexes that reflect in vivo interactions.\nImmunoprecipitation: Specific antibodies are used to selectively enrich DNA fragments bound by the protein of interest. These antibodies recognize and bind to the target protein, allowing the associated DNA to be co-precipitated and separated from non-specific chromatin (Orlando, 2000).\nDNA Purification and Sequencing: The crosslinks are reversed, and the purified DNA is subjected to high-throughput sequencing, producing millions of short reads that represent the protein-bound regions.\nData Analysis and Mapping: The sequencing reads are aligned to a reference genome to identify regions of enrichment—termed “peaks”—which indicate protein-binding sites. These data are further analyzed to determine patterns of binding and regulatory function (Park, 2009).\nReferences:\nJohnson, D. S., Mortazavi, A., Myers, R. M., & Wold, B. (2007). Genome-wide mapping of in vivo protein-DNA interactions. Science, 316(5830), 1497–1502.\nBarski, A., Cuddapah, S., Cui, K., Roh, T. Y., Schones, D. E., Wang, Z., … & Zhao, K. (2007). High-resolution profiling of histone methylations in the human genome. Cell, 129(4), 823–837.\nOrlando, V. (2000). Mapping chromosomal proteins in vivo by formaldehyde-crosslinked-chromatin immunoprecipitation. Trends in Biochemical Sciences, 25(3), 99–104.\nPark, P. J. (2009). ChIP-seq: advantages and challenges of a maturing technology. Nature Reviews Genetics, 10(10), 669–680.\nSolomon, M. J., Larsen, P. L., & Varshavsky, A. (1988). Mapping protein-DNA interactions in vivo with formaldehyde: evidence that histone H4 is retained on a highly transcribed gene. Cell, 53(6), 937–947.\n\nApplications of ChiP-seq\nChIP-seq is a widely used technique for identifying gene regulatory regions by mapping protein–DNA interactions across the genome. It is particularly valuable for studying transcription factors, which bind specific DNA sequences to regulate gene expression, and histone modifications, which influence chromatin structure and accessibility. By capturing these interactions, ChIP-seq provides insights into the mechanisms controlling gene activity, making it a fundamental tool in functional genomics, epigenetics, and developmental biology.\n\n\nChIP-seq for Transcription Factors: Key Considerations and Applications\n\n\n\nChiP-seq for transcription factors\n\n\nChIP-seq is a powerful tool for studying transcription factors (TFs) and their roles in gene regulation. By identifying where TFs bind across the genome, researchers can infer regulatory networks and understand transcriptional control mechanisms in specific cellular contexts. Accurate ChIP-seq profiling of TFs critically depends on the availability of high-quality, ChIP-grade antibodies. Each TF requires a specific antibody with strong affinity and low background binding. However, only about 40% of commercially available antibodies meet the stringent criteria for ChIP-seq applications, making antibody validation a crucial step in experimental design (Landt et al., 2012). Poor-quality antibodies can result in nonspecific binding and misleading data, compromising the biological interpretation.\nWhen properly performed, ChIP-seq for transcription factors enables the discovery of regulatory elements, transcriptional enhancers, and promoter architecture, and facilitates the construction of gene regulatory networks essential for understanding cellular identity, development, and disease.\nReferences:\nLandt, S. G., et al. (2012). ChIP-seq guidelines and practices of the ENCODE and modENCODE consortia. Genome Research, 22(9), 1813–1831.\nWhyte, W. A., et al. (2013). Master transcription factors and mediator establish super-enhancers at key cell identity genes. Cell, 153(2), 307–319.\nKim, T. H., & Ren, B. (2006). Genome-wide analysis of protein-DNA interactions. Annual Review of Genomics and Human Genetics, 7, 81–102.\n\n\nHistone Modifications and Their Detection by ChIP-seq\n\n\n\nHistone modifications\n\n\nIn eukaryotic cells, DNA is wrapped around histone proteins to form nucleosomes, the fundamental units of chromatin. The histone tails—flexible amino-terminal regions of histone proteins—are subject to a variety of post-translational modifications, including methylation, acetylation, phosphorylation, and ubiquitination. These histone modifications play a central role in regulating chromatin structure and gene expression by influencing the accessibility of DNA to transcriptional machinery (Kouzarides, 2007).\nSpecific histone modifications are associated with distinct functional states of chromatin. For example, trimethylation of lysine 4 on histone H3 (H3K4me3) is typically found near the promoters of actively transcribed genes, whereas trimethylation of H3K27 (H3K27me3) is associated with gene repression (Barski et al., 2007). These modifications serve as binding platforms for chromatin-associated proteins and contribute to the epigenetic regulation of gene activity.\nChIP-seq can be used to map these modifications genome-wide by using antibodies that specifically recognize individual histone marks. These antibodies must be rigorously validated for ChIP-seq to ensure they bind selectively to the modification of interest without cross-reactivity. This approach enables researchers to define chromatin states, identify regulatory elements such as enhancers and promoters, and explore how epigenetic changes contribute to development and disease (Rando & Chang, 2009).\nReferences:\nKouzarides, T. (2007). Chromatin modifications and their function. Cell, 128(4), 693–705.\nBarski, A., et al. (2007). High-resolution profiling of histone methylations in the human genome. Cell, 129(4), 823–837.\nRando, O. J., & Chang, H. Y. (2009). Genome-wide views of chromatin structure. Annual Review of Biochemistry, 78, 245–271.\n\n\nEpigenetic jargon cheat-sheet\n\n\n\n\n\n\n\nRegulatory Element\nMeaning\n\n\n\n\nPromoter\nDNA Sequence (100-1kb), initial secure binding site for: RNA Pol complex and Transfacs.\nAdjacent regulated gene, defined relative to TSS.\nPoised: simultaneous activation/repressive histone mods.\n\n\nEnhancer/Silencer\nDNA Seq (50-1.5kb), bound by transfacs (activator / repressor)\nCan act on gene up to 1Mb away: DNA folding brings it close to promoter.\nEnhancer: Bound by activator, which interacts with complex initiating transcription.\nSilencer: bound by repressor, which interferes with GTF assembly.\n\n\nInsulator\nDNA, 300-2kb, Block enhancers from acting on promoters: positioned between enhancer and promoter, form chromatin-loop domains.\n\n\nPolycomb-repressed\nPolycomb – group proteins actively remodel chromatin to silence genes.\n\n\n\n\n\nThe Histone Code and Chromatin State Annotation\nHistone modifications can be used to define functional elements across the genome by decoding the so-called “histone code.” Specific combinations of histone marks are associated with distinct regulatory roles, such as promoters, enhancers, and repressive elements. The seminal work by Ernst et al. (2011) introduced a systematic method for interpreting these combinations using a computational approach called ChromHMM (Chromatin Hidden Markov Model).\nIn this framework, ChIP-seq data for multiple histone modifications are collected across various cell types. For example, the analysis shown integrates data from 9 cell lines and 9 histone marks, including H3K4me1, H3K4me3, H3K27ac, H3K36me3, and others. These data are used to segment the genome into discrete chromatin states based on recurring patterns of histone modifications. Each chromatin state is then functionally annotated post hoc—for instance, State 1 corresponds to active promoters (high H3K4me3 and H3K27ac), while States 4 and 5 reflect strong enhancers (enriched in H3K4me1 and H3K27ac).\nThis approach allows for genome-wide annotation of regulatory elements in an unsupervised manner, without prior knowledge of genomic function. Once chromatin states are defined, researchers can assess how often specific genomic features (e.g., TF binding sites) overlap with these states to infer biological relevance.\nBy integrating epigenomic data into unified chromatin state models, this method provides a powerful tool for interpreting gene regulation and understanding the epigenetic landscape of different cell types.\nReferences:\nErnst, J., & Kellis, M. (2011). ChromHMM: automating chromatin-state discovery and characterization. Nature Methods, 9(3), 215–216.\nBernstein, B. E., et al. (2007). The mammalian epigenome. Cell, 128(4), 669–681.\nRoadmap Epigenomics Consortium et al. (2015). Integrative analysis of 111 reference human epigenomes. Nature, 518(7539), 317–330.\n\n\nHistone mark cheat sheet\n\n\n\n\n\n\n\n\nHistone mark\nCandidate State\nInterpretation\n\n\n\n\nH3K9me2,3\n-\nSilenced genes\n\n\nH3K27me3\nInactive/poised promoter, polycomb repressed\nDownregulation of nearby genes\n\n\nH3K36me3\nTranscriptional transition\nActively transcribed gene bodies.\n\n\nH4K20me1\nTranscriptional transition\nTranscriptional activation\n\n\nH3K4me1,2,3\nStrong enhancer\nPromoter of active genes\n\n\nH3K27ac\nActive promoter/strong enhancer\nActive transcription\n\n\nH3K9ac\nActive promoter\nSwitch from transcription initiation to elongation.\n\n\n\n\n\nChIP-seq Experimental Considerations\nSuccessful ChIP-seq experiments require careful optimization of several key parameters to ensure data quality and reproducibility. A critical factor is the quality of the antibody used for immunoprecipitation. Only high-affinity, ChIP-grade antibodies should be used, as approximately 60% of commercially available antibodies do not meet the required specificity and sensitivity standards, especially for transcription factors (Landt et al., 2012).\nCell number is another important consideration. For histone modification ChIP, 2–3 million cells are typically sufficient. However, transcription factor ChIP requires more material—often between 5 and 10 million cells—due to the lower abundance and weaker DNA-binding affinity of many TFs.\nCrosslinking conditions must also be optimized. A standard protocol involves treating cells with 1% formaldehyde for about 10 minutes at room temperature to preserve protein-DNA interactions without causing excessive crosslinking, which can impair downstream shearing and DNA recovery.\nFinally, the fragmentation of chromatin by sonication or enzymatic digestion should be monitored to ensure consistent fragment sizes (typically 100–500 bp), which are necessary for optimal resolution and accurate peak calling. Fragment size should be confirmed before proceeding to immunoprecipitation and sequencing.\nReference:\nLandt, S. G., et al. (2012). ChIP-seq guidelines and practices of the ENCODE and modENCODE consortia. Genome Research, 22(9), 1813–1831.\n\n\n\nChromatin Shearing and Sequencing Considerations in ChIP-seq\n\n\n\nShearing\n\n\nEffective chromatin shearing is essential for ChIP-seq success, as it determines the resolution and reliability of the resulting data. The target fragment size after shearing should fall within the 150–400 base pair (bp) range to ensure optimal resolution and efficient sequencing. Shearing efficiency can vary significantly depending on cell type, chromatin accessibility, and buffer conditions, so it must be empirically optimized.\nTo achieve consistent shearing, vary the number of sonication cycles or digestion times, and evaluate results by running input DNA on a Bioanalyzer or agarose gel. This step is critical to confirm that the fragment size distribution is within the desired range and that over- or under-shearing is avoided.\nWhen proceeding to sequencing, longer reads improve the ability to uniquely map sequences to the genome, particularly in repetitive or complex regions. Additionally, paired-end (PE) reads are preferred over single-end (SE) reads, as PE sequencing increases mapping accuracy and helps resolve ambiguous regions by anchoring both ends of each DNA fragment.\n\n\nSignal vs. Noise in ChIP-seq Experiments\n\n\n\nSignal to noise ratio in ChiP-seq experiments\n\n\nIn ChIP-seq, distinguishing true biological signal from background noise is essential for accurate interpretation of protein–DNA interactions. The signal represents DNA fragments that are specifically bound by the protein of interest and successfully captured during immunoprecipitation using a targeted antibody (step 3). These fragments are enriched in sequencing data and typically appear as peaks when mapped to the reference genome (step 5).\nIn contrast, noise arises from non-specific DNA–protein interactions, incomplete washing, or the presence of DNA fragments not associated with the protein of interest. These fragments are also sequenced but contribute to background signal that can obscure true binding events (step 2). Noise can originate from non-specific antibody binding, over-crosslinking, or poor chromatin shearing, and must be minimized through careful optimization of experimental conditions.\nHigh signal-to-noise ratio is critical for reliable peak calling and downstream analyses. This can be achieved by using validated, high-affinity antibodies, optimizing crosslinking and shearing conditions, and including appropriate controls (e.g., input DNA or IgG controls). Computational tools further help in distinguishing true signal from noise during data analysis by modeling background distribution and correcting for technical artifacts.\n\n\n\nSignal to noise ratio in ChiP-seq experiments\n\n\nIn the high signal-to-noise case (top panel), sequencing reads (represented as blue bars) are densely clustered over the true binding site—the genomic region where the protein of interest genuinely interacts with DNA. The surrounding regions show relatively few nonspecific reads, resulting in a distinct and confident peak that is easily identified during data analysis.\nIn the low signal-to-noise case (bottom panel), the signal over the true binding site is diluted or obscured by widespread background noise—reads scattered across surrounding regions not bound by the protein of interest. This makes it harder to distinguish real binding events from artifacts, reducing the reliability of peak calling and biological interpretation.\n\n\n\nSignal to noise ration in ChiP-seq experiments\n\n\nThe signal-to-noise ratio in ChIP-seq data can be visually assessed using genome browser tracks, where sequencing reads are aligned and displayed across genomic regions. In the figure above, each horizontal track represents a different ChIP-seq experiment or sample. The vertical spikes indicate enrichment of sequencing reads, corresponding to protein–DNA binding events or histone modification peaks.\nA high signal-to-noise ratio is characterized by distinct, narrow peaks rising well above a low background, which is typically observed in well-optimized ChIP experiments using high-quality antibodies. These peaks clearly mark regions of specific protein-DNA interaction or histone modification.\nIn contrast, high background noise, as indicated by the label on the right, results in elevated read coverage across large portions of the genome without clear peak definition. This can obscure true binding events and complicate data interpretation. High background may result from non-specific antibody binding, over-crosslinking, poor chromatin shearing, or insufficient washing during immunoprecipitation.\n\n\nFRiP Score and Library Complexity\nA key metric for evaluating ChIP-seq data quality is the FRiP score—Fragments In Peaks. This metric quantifies the proportion of total sequencing fragments that fall within confidently called peak regions, serving as a proxy for signal-to-noise ratio. It is calculated as:\nFRiP = (Number of fragments in peaks) / (Total number of mapped fragments)\nA FRiP score greater than 1% is generally considered acceptable, although higher values are expected in high-quality experiments, particularly for histone modifications and well-enriched transcription factors (Landt et al., 2012).\nAnother important consideration is library complexity, which reflects how diverse the sequenced fragments are. Low complexity may indicate that too few unique DNA fragments were captured, often due to limited starting material or low antibody efficiency. This leads to excessive duplication and poor coverage.\n\n\n\nLibrary complexity. This figure illustrates the difference between high and low library complexity in ChIP-seq experiments. In the high complexity example (top), many unique DNA fragments are sequenced at the true binding site, providing a strong and reliable signal with broad coverage and minimal redundancy. In contrast, the low complexity example (bottom) shows the same fragment repeated many times—these are PCR duplicates, often resulting from limited starting material or poor immunoprecipitation efficiency. Although a signal is still detected at the true binding site, it lacks diversity and may artificially inflate peak strength while reducing biological confidence.\n\n\n\n\n\nLibrary complexity. Tracks at the top display sharp, well-defined peaks and low background, indicating high complexity with many unique DNA fragments. In contrast, tracks at the bottom show broader, noisier profiles with less defined peaks—hallmarks of low complexity, often caused by excessive PCR duplication or insufficient starting material.\n\n\n\n\nMeasuring Library Complexity and Basic ChIP-seq Data Analysis\nA key metric for evaluating library complexity in ChIP-seq is the Non-Redundant Fraction (NRF), which assesses how many unique fragments are present relative to the total number of reads. It is calculated as:\nNRF = (Number of unique fragment positions) / (Total number of mapped fragments)\nAn NRF value greater than 0.8 is recommended by ENCODE standards, indicating high library complexity and low duplication levels. Low NRF values suggest that a large proportion of reads are duplicates—often due to over-amplification or limited input material—and may compromise the quality of downstream analysis.\nOnce data quality is confirmed, ChIP-seq analysis typically follows these key steps:\nRead Alignment: Sequence reads are aligned to a reference genome using tools like BWA or Bowtie.\nVisualization: Mapped reads are inspected in genome browsers (e.g., IGV, UCSC) to assess enrichment patterns and signal quality.\nPeak Calling: Statistical algorithms (e.g., MACS2) are used to identify regions with significant read enrichment—representing protein–DNA binding sites or histone modification peaks.\nPeak Annotation: Peaks are assigned to nearby genes or genomic features to interpret functional relevance.\nMotif Analysis: For transcription factors, enriched sequence motifs can be identified within peaks using tools like MEME or HOMER.\nDifferential Binding Analysis: Comparative analysis between conditions (e.g., treated vs. control, naïve vs. stimulated) is performed to identify regions with statistically significant changes in binding patterns.\n\n\nVisualisation of ChIP-seq Data in Genome Browsers\n\n\n\nVisualisation in a genome browser\n\n\nOnce ChIP-seq reads have been aligned to the reference genome, the resulting data can be visualized using genome browsers such as IGV, Ensembl, or the UCSC Genome Browser. These platforms allow researchers to view read coverage—often called “signal”—across specific genomic regions.\nTo enable visualization, mapped reads (BAM files) must be converted into signal tracks that reflect read depth at each base pair or across defined windows. This is typically done using file formats such as WIG or bedGraph, which summarize read density and can be readily interpreted by genome browsers.\nAs shown in the figure, the purple track displays ChIP-seq enrichment (signal) across the genome, overlaid with gene annotations and regulatory features. Peaks in the signal correspond to regions of protein-DNA interaction, and their location relative to gene features (e.g., promoters) can be easily inspected.\n\n\nPeak Calling in ChIP-seq Analysis\n\n\n\nPeak calling\n\n\nPeak calling is the computational process used to identify genomic regions enriched in sequencing reads, representing potential protein–DNA interaction sites or histone modifications. The figure illustrates how peak calling algorithms scan across the genome using a sliding window approach to count fragments and evaluate statistical enrichment.\nWithin each window, the observed read counts are compared to expected counts under a background model (typically assuming random distribution of reads). A Poisson test or similar statistical method is applied to calculate a p-value, which reflects the probability of observing that many fragments by chance. Peaks are defined where the signal exceeds a specified false discovery rate (FDR)-corrected p-value threshold.\nKey Considerations and Challenges in Peak Calling\n\nControl Samples: Accurate peak calling depends on defining what is “expected.” This often involves comparing the treatment sample (immunoprecipitated with antibody) to an input or control sample (DNA without immunoprecipitation) to account for background signal.\nBiological Replicates: Including at least two biological replicates is strongly recommended to ensure reproducibility and reduce false positives. More replicates increase statistical power.\nVariable Peak Sizes: Peak size can vary considerably depending on the target:\nSmall peaks are typical for transcription factors, which bind narrow DNA motifs.\nBroader peaks occur with histone modifications or RNA polymerase II, which can span larger chromatin domains.\n\n\n\nMotif Analysis in ChIP-seq\nMotif analysis is a key step in ChIP-seq data interpretation, particularly for transcription factor (TF) binding studies. It helps identify short, recurring DNA sequences (motifs) that are recognized by TFs. The process begins by extracting sequences from peak summits, which represent the highest enrichment points and are likely to contain the core TF binding site.\nThese sequences from multiple peaks are then aligned to detect conserved patterns. From this alignment, computational tools generate a consensus motif, typically visualized as a sequence logo.\nIn the logo:\nLetter height indicates the relative frequency of each base (A, T, C, G) at a given position.\nStack height represents information content, calculated as 2 minus the Shannon entropy of the base distribution at that position:\n\n\n\nEntropy formula\n\n\nLower entropy (more conserved) yields taller stacks; higher entropy (more variable) results in shorter stacks.",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module7/module7.html",
    "href": "course_modules/Module7/module7.html",
    "title": "Overview",
    "section": "",
    "text": "Module Title\nCHiP-Seq\n\n\nDuration\n3 hours\n\n\nKey topics\n\n\nLearning outcomes\n\n\nActivities\n\nLecture: 1 hour\nPractical exercises: 1.5 hours\nAssessment quiz: 0.5 hours\n\n\n\nManual\nModule manual\nExercises\nSolutions\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual. Instructions on how to that will be added here.)\n\n\nLecture recording, notes and scripts\n\n\nPractical exercises\n\nVirtual Machine\nCommand line walk through.\n\n\n\nDatasets\n\n\nAssessment quiz\nQuestions\nSolution",
    "crumbs": [
      "Home",
      "CHiP-Seq",
      "Overview"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html",
    "href": "course_modules/Module6/module6_manual.html",
    "title": "Manual",
    "section": "",
    "text": "Differential Expression using RNA-Seq\nInstructor: Vivek Iyer\nSource Material: Based extensively on slides by Victoria Offord (Wellcome Sanger Institute)\nDate: 7 June 2024",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#learning-objectives",
    "href": "course_modules/Module6/module6_manual.html#learning-objectives",
    "title": "Manual",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAppreciate the important aspects of RNASeq experiment design • Understand the various technical steps in RNASeq pipelines • Align RNA-Seq reads to a reference genome and a transcriptome • Visualise transcription data using standard tools • Quantify the expression values of your transcripts using standard tools • Perform QC of NGS transcriptomic data • Interpret differential gene expression data",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#introduction-and-context",
    "href": "course_modules/Module6/module6_manual.html#introduction-and-context",
    "title": "Manual",
    "section": "Introduction and Context",
    "text": "Introduction and Context\nThe transcriptome is defined as “the complete set of transcripts present in a cell, along with their quantities, at a specific developmental stage or under specific conditions” Wang Z, Gerstein M, Snyder M. RNA-Seq: a revolutionary tool for transcriptomics. Nat Rev Genet. 2009 Jan;10(1):57-63. doi: 10.1038/nrg2484. PMID: 19015660; PMCID: PMC2949280.\nIt represents a snapshot of gene expression at a fixed point in time and under fixed environmental or biological conditions. RNA sequencing (RNA-Seq), which relies on next-generation sequencing (NGS) technology, is commonly used to measure the transcriptome. For example, in Plasmodium berghei, the transcriptome is approximately 10 million bases, covering about 50% of the genome (20 million bases), with large exons relative to introns and intergenic regions. In contrast, in Homo sapiens, the transcriptome comprises about 10% of the genome, and exons are small compared to the much larger introns and intergenic regions.",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#experimental-design",
    "href": "course_modules/Module6/module6_manual.html#experimental-design",
    "title": "Manual",
    "section": "Experimental Design",
    "text": "Experimental Design\nA successful RNA-Seq study begins with a carefully planned experimental design tailored to the biological question of interest. Key factors to consider include the type of library preparation, the sequencing depth, and the number of replicates. These decisions significantly impact the quality and interpretability of the resulting data.\n\nLibrary Preparation\nTotal RNA includes a mixture of mRNA, rRNA, tRNA, and various regulatory RNAs. Ribosomal RNA (rRNA) can constitute over 90% of total RNA, so it is essential to either enrich for the small fraction of mRNA (1–2%) or deplete the rRNA. mRNA enrichment methods typically require high-quality RNA, often assessed using the RNA Integrity Number (RIN), and may not be suitable for certain sample types such as tissue biopsies. In bacteria, where mRNA is not polyadenylated, rRNA depletion is generally required instead of poly-A selection. It is also important to be aware of the specific library preparation protocol being used, as some may remove small RNAs that could be relevant to the study.\n\n\nLibrary Type\nThe choice between stranded and unstranded libraries is another critical consideration. Strand-specific protocols preserve the information about which DNA strand the RNA was transcribed from, which is particularly valuable for distinguishing between overlapping or antisense transcripts. Another decision involves using single-end or paired-end sequencing. Paired-end sequencing is more informative and is particularly advantageous for discovering new transcripts or analyzing isoform expression, as it helps reconstruct transcript structures. However, it’s worth noting that fewer than 55% of reads typically span more than one exon.\n\n\n\nReplicates\nReplication is vital for capturing variation and ensuring the robustness of RNA-Seq results. Biological replicates involve using distinct biological samples—such as different individuals or cultures—under the same experimental conditions. These replicates are essential for assessing biological variability and drawing meaningful conclusions. In contrast, technical replicates are repeated measurements of the same sample and are used to assess variation introduced by equipment or protocols. While technical replicates are generally not required, careful sample layout and sequencing lane assignments can help control for technical variation. Some analysis tools also allow adjustments based on these technical factors or known controls (“spike-ins”).\nCareful attention to these elements of experimental design will enhance the quality and interpretability of RNA-Seq data, ensuring that the results are both reliable and biologically meaningful.\n\n\n\nSequencing Depth\nSequencing depth is a key factor in RNA-Seq experimental design, as it determines how well transcripts—especially those with low expression—can be detected. For standard human transcriptome analysis using 100 base pair (bp) paired-end reads, a sequencing depth of approximately 30 million reads per sample is typically sufficient. However, for studies aiming to identify novel transcripts or rare isoforms, deeper sequencing of 50 to 100 million reads may be necessary. As an example, 30 million 100 bp paired-end reads provide roughly 3 × 10⁹ bases of sequence. Given that the human transcriptome is around 150 megabases (1.5 × 10⁸ bases), this corresponds to approximately 20× coverage, meaning one can reasonably distinguish between full, half, or quarter expression levels.\nWhile deeper sequencing can improve sensitivity for detecting differentially expressed (DE) genes, especially those with low expression, it is subject to diminishing returns beyond a certain point. On the other hand, increasing the number of biological replicates enhances the statistical power to detect true changes in gene expression. Replicates improve the accuracy of log fold-change (logFC) estimates and help quantify variability in expression, particularly for genes with low abundance. Therefore, when designing an RNA-Seq experiment, it’s crucial to balance sequencing depth and replication according to your study goals.\nTo optimize this balance, tools such as RNA-Seq power calculators—like the one available at https://cqs-vumc.shinyapps.io/rnaseqsamplesizeweb/—can help estimate the appropriate sample size for your experiment. For further guidance, see Liu et al. (2014), Bioinformatics, which discusses the trade-offs between sequencing depth and replication in differential expression studies.",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#rna-seq-data-analysis-pipelines",
    "href": "course_modules/Module6/module6_manual.html#rna-seq-data-analysis-pipelines",
    "title": "Manual",
    "section": "RNA-Seq Data Analysis Pipelines",
    "text": "RNA-Seq Data Analysis Pipelines\n\nKey Steps in an RNA-Seq Analysis Pipeline\nEvery RNA-Seq analysis pipeline, regardless of the tools used, typically follows three core steps. First, we determine which genes or transcripts our sequencing reads belong to—this is done through mapping to a reference genome or transcriptome, or through de novo assembly. Second, we quantify how many reads align to each gene or transcript. Third, we assess whether gene or transcript expression levels differ between sample groups through differential gene expression (DGE) analysis. While these steps are common, there is no universal pipeline that suits every analysis; the best approach depends on your organism, research goals, available resources, and data quality.\n\n\nMapping RNA-Seq Reads to the Genome with HISAT2\n\nMapping RNA-Seq reads to a reference genome is useful for assessing overall data quality and examining the structure of genes, particularly in eukaryotes where introns are spliced out of mature mRNA. Splice-aware aligners are required to handle these complexities. HISAT2 is a widely used aligner that balances speed, accuracy, and low memory usage, and is especially good at discovering novel splice junctions. Alternatives include STAR, known for its speed in high-throughput settings, and Bowtie2, which is often used for more general alignment tasks. HISAT2 is particularly efficient in terms of memory footprint and splice discovery, making it a good choice for many genome-based workflows. However, after mapping, separate quantification steps are needed to count reads per gene or transcript.\n\n\n\n\nMapping to the Transcriptome and Quantifying Reads with Kallisto\n\nKallisto offers a much faster alternative by mapping reads to a reference transcriptome rather than the genome. It uses a two-step process: first, it builds an index from the set of known spliced transcript sequences; second, it quantifies read abundance through pseudoalignment. Pseudoalignment significantly speeds up the process by determining which transcripts reads belong to without calculating exact alignment positions. This method is efficient and includes quantification as part of the process.\n\nOne limitation of Kallisto is that it cannot detect novel transcripts—it relies entirely on a predefined transcriptome. Therefore, it’s not suitable if your goal is to explore new splicing events or novel isoforms. Additionally, the presence of multiple isoforms per gene introduces mapping ambiguity. However, because Kallisto maps to spliced transcripts, it accounts for this ambiguity and can provide transcript-specific read counts.\n\n\nChoosing Between Genome and Transcriptome Mapping\nMapping to the genome using tools like HISAT2, followed by visualization in IGV (Integrative Genomics Viewer), is ideal for quality control and the discovery of novel splicing events. It does, however, require more computational resources and a separate quantification step. If resources permit and novel transcript discovery is important, this method is recommended.\nIn contrast, mapping to the transcriptome with Kallisto is extremely fast and includes quantification by default. This approach is optimal when a well-annotated transcriptome is available and the focus is on efficiency rather than discovery of new transcript variants. Ultimately, the choice between genome and transcriptome mapping should align with your analysis objectives and the biological questions you aim to answer.\n\n\nNormalisation in RNA-Seq Analysis\n\nNormalization is a crucial step in RNA-Seq analysis, helping to correct for technical biases that can distort biological interpretation. For example, sequencing runs with greater depth will naturally produce more reads for each gene, and longer genes will accumulate more reads simply due to their size. To account for these issues, most normalization methods adjust for both sequencing depth and gene length. Common normalization methods include RPKM (Reads Per Kilobase per Million), FPKM (Fragments Per Kilobase per Million), and TPM (Transcripts Per Million). While these approaches help standardize read counts, they can struggle with datasets containing a few highly expressed genes that skew the overall distribution. As a result, more advanced normalization techniques—such as DESeq2’s regularized log transformation (rlog) or Sleuth’s model-based normalization—are often preferred for differential expression analysis.\n\nFPKM is a refinement of RPKM, specifically designed for paired-end sequencing. It accounts for the fact that two reads may map to a single RNA fragment and prevents double-counting. While these methods provide a reasonable first-pass normalization, more robust statistical frameworks are generally required for high-quality differential expression analysis.",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#the-importance-of-quality-control-qc",
    "href": "course_modules/Module6/module6_manual.html#the-importance-of-quality-control-qc",
    "title": "Manual",
    "section": "The Importance of Quality Control (QC)",
    "text": "The Importance of Quality Control (QC)\nQuality control is essential in RNA-Seq workflows to ensure that the data used for downstream analysis is reliable and free from avoidable technical errors. During sample collection, library preparation, and sequencing, multiple things can go wrong. RNA may degrade if samples are mishandled, sequencing lanes might perform inconsistently, and certain genomic regions may pose technical challenges (e.g., sequencing hotspots or local capture inefficiencies). These issues can lead to false positives or false negatives in differential expression results by introducing noise or systematic bias into the dataset. Identifying and removing problematic samples or outliers helps preserve the integrity of the analysis.\n\nUsing PCA for QC\n\nPrincipal Component Analysis (PCA) is a widely used dimensionality reduction technique in RNA-Seq QC. RNA-Seq datasets often consist of tens of thousands of features (e.g., ~60,000 genes), making direct interpretation difficult. PCA transforms this high-dimensional data into a new coordinate system (principal components), where each new dimension captures maximal, independent variance in the data. By visualizing the first few principal components—typically PC1 to PC4—you can quickly spot outlying samples that behave differently from the rest. This helps flag potential issues early and improves the consistency and reliability of downstream results.\n\n\nDetermining Differential Expression\nTo identify differentially expressed genes, a variety of statistical tools are available, including DESeq2, EdgeR, Limma-Voom, and Sleuth (which is specifically designed to work with Kallisto output). These methods improve on simpler approaches like per-gene t-tests, which are usually inappropriate due to limited sample sizes and the need to model variance across genes. Modern RNA-Seq tools incorporate “size factors” to account for differences in sequencing depth and use advanced statistical modeling to estimate gene-specific variance. Most tools also include mechanisms for testing complex experimental designs and adjusting for multiple testing, often reporting both p-values and q-values to help control false discovery rates.\n\n\nQuality Control with Sleuth\n\nSleuth integrates normalization, statistical modeling, and visualization for data derived from pseudoalignment tools like Kallisto. It provides built-in quality control metrics and supports flexible experimental designs, making it a convenient and powerful option for users focused on transcript-level quantification. With Sleuth, users can perform PCA, visualize sample distributions, and explore model fits directly within the same environment, streamlining the QC and analysis workflow.\n\n\nWhat to Do with a List of Differentially Expressed Genes\nOnce you’ve generated a list of differentially expressed genes, the next challenge is interpreting what it means biologically. If you have a predefined hypothesis, now is the time to test it. Otherwise, you can begin exploring the list using tools for functional enrichment and pathway analysis, such as Gene Ontology (GO) term enrichment, pathway analysis tools like GSEA, TopGO, InnateDB, or commercial options like Ingenuity Pathway Analysis. Another approach is manual exploration—read relevant literature, investigate gene functions, and look for patterns. Visual tools like volcano plots (effect size vs. p-value) are also useful for identifying genes of interest. From there, you can formulate new hypotheses and decide whether to pursue further bioinformatic exploration or design the next set of wet lab experiments.",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#practical-exercise",
    "href": "course_modules/Module6/module6_manual.html#practical-exercise",
    "title": "Manual",
    "section": "Practical Exercise",
    "text": "Practical Exercise\n·Organism: Plasmodium chabaudi (rodent malaria parasite).\n·Compare transcriptomes between mosquito-transmitted (MT) and serial blood passage (SBP) parasites.\n·Biological Question: Is the parasite’s transcriptome different after mosquito passage?",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#appendix-key-tools-mentioned",
    "href": "course_modules/Module6/module6_manual.html#appendix-key-tools-mentioned",
    "title": "Manual",
    "section": "Appendix: Key Tools Mentioned",
    "text": "Appendix: Key Tools Mentioned\n\n\n\nTool\nPurpose\n\n\nHISAT2\nSplice-aware genome alignment\n\n\nKallisto\nFast transcriptome pseudoalignment\n\n\nIGV\nVisualization\n\n\nSleuth\nDGE and QC analysis\n\n\nDESeq2, EdgeR\nDGE analysis",
    "crumbs": [
      "Home",
      "RNA-Seq",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_solutions.html",
    "href": "course_modules/Module1/module1_solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "1. There are 10 sequences in this file. To count all the header lines, we can use\ngrep -c \"&gt;\"data/example.fasta\n2. There are 8 reads in this file. We can use grep to search for /1 or /2:\ngrep -c \"/1\" data/example.fastq\nAlternatively, we can use wc -l to count the lines in the file and then divide this by 4.\n3. RG = Read Group\n4. Illumina. See the __PL__field.\n5. SC. See the CN field.\n6. ERR003612. See the ID field.\n7. 2kbp. See the PI field.\n8. The quality is 48. We can use grep to find the id, followed by awk to print the fifth column:\ngrep \"ERR003762.5016205\" data/example.sam | awk '{print $5}'\n9. The CIGAR is 37M. We can use grep and awk to find it:\ngrep ERR003814.6979522 data/example.sam | awk '{print $6}'\n10. 213. The ninth column holds the insert size, so we can use awk to get this:\ngrep ERR003814.1408899 data/example.sam | awk '{print $9}'\n11. The CIGAR in Q9 was 37M, meaning all 37 bases in the read are either matches or mismatchesto the reference.\n12. CIGAR: 4M 4I 8M. The first four bases in the read are the same as in the reference, so we can represent these as 4M in the CIGAR string. Next comes 4 insertions, represented by 4I, followed by 8 alignment matches, represented by 8M.\n13. NCBI build v37\n14. There are 15 lanes in the file. We can count the @RG lines manually, or use standard UNIX commands such as:\nsamtools view -H data/NA20538.bam | grep ^@RG | wc -l\nor\nsamtools view -H data/NA20538.bam | awk '{if($1==\"@RG\")n++}END{print n}'\n15. Looking at the @PG records ID tags, we see that three programs were used: GATK IndelRealigner, GATK TableRecalibration and bwa.\n16. The @PG records contain a the tag VN. From this we see that bwa version 0.5.5 was used.\n17. The first collumn holds the name of the read: ERR003814.1408899\n18. Chromosome 1, position 19999970. Column three contains the name of the reference sequence and the fourth column holds the leftmost position of the clipped alignment.\n19. 320 reads are mapped to this region. We have already sorted and indexed the BAM file, so now we can search for the reagion using samtools view. Then we can pipe the output to wc to count the number of reads in this region:\nsamtools view data/NA20538_sorted.bam 1:20025000-20030000 | wc -l\n20. The reference version is 37. In the same way that we can use -h in samtools to include the header in the output, we can also use this with bcftools:\nbcftools view -h data/1kg.bcf | grep \"##reference\"\n21. There are 50 samples in the file. The -l option will list all samples in the file:\nbcftools query -l data/1kg.bcf | wc -l\n22. The genotype is A/T. With -f we specify the format of the output, -r is used to specify the region we are looking for, and with -s we select the sample.\nbcftools query -f'%POS [ %TGT]\\n' -r 20:24019472 -s HG00107 data/1kg.bcf\n23. There are 4778 positions with more than 10 alternate alleles. We can use -i to specify that we are looking for instances where the value of the INFO:AC tag (Allele Count) is greater than 10:\nbcftools query -f'%POS\\n' -i 'AC[0]&gt;10' data/1kg.bcf | wc -l\n24. There are 451 such positions. The first command picks out sample HG00107. We can then pipe the output to the second command to filter by depth and non-reference genotype. Then use wc to count the lines:\nbcftools view -s HG00107 data/1kg.bcf | bcftools query -i'FMT/DP&gt;10 & FMT/GT!=\"0/0\"' -f'%POS[ %GT %DP]\\n' | wc -l\n25. 26. The first base is at position 9923 and the last is at 9948.\n26. G. To reduce file size, only the first base is provided in the REF field.\n27. 10. See the MinDP tag in the INFO field.",
    "crumbs": [
      "Home",
      "File formats",
      "Solutions"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_solutions.html#qc-assessment-of-ngs-data",
    "href": "course_modules/Module1/module1_solutions.html#qc-assessment-of-ngs-data",
    "title": "Solutions",
    "section": "QC assessment of NGS data",
    "text": "QC assessment of NGS data\n1. The peak is at 140 bp, and the read length is 100 bp. This means that the forward and reverse reads overlap with 60 bp.\n2. There are 400252 reads in total.\nLook inside the file and locate the field “raw total sequences”. To extract the information quickly from multiple files, commands similar to the following can be used:\ngrep ^SN lane*.sorted.bam.bchk | awk -F'\\t' '$2==\"raw total sequences:\"'\n3. 76% of the reads were mapped. Divide “reads mapped” (303036) by “raw total sequences” (400252).\n4. 2235 pairs mapped to a different chromosome. Look for “pairs on different chromosomes”\n5. The mean insert size is 275.9 and the standard deviation is 47.7. Look for “insert size mean” and “insert size standard deviation”.\n6. 282478 reads were properly paired. Look for “reads properly paired”.\n7. 23,803 (7.9%) of the reads have zero mapping quality. Look for “zero MQ” in the “Reads” section.\n8. The forward reads. Look at the “Quality per cycle” graphs.\n\nFile conversion - Answers\n1. The CRAM file is ~18 MB. We can check this using:\nls -lh data/yeast.cram\n2. Yes, the BAM file is ~16 MB bigger than the CRAM file. We can check this using:\nls -lh data/yeast*",
    "crumbs": [
      "Home",
      "File formats",
      "Solutions"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_manual.html",
    "href": "course_modules/Module1/module1_manual.html",
    "title": "Manual",
    "section": "",
    "text": "These formats store raw or processed nucleotide and protein sequences. \n\n\nHere’s an example of 4 lines from a human reference genome FASTA file:\n\n\"&gt;chr1\" is the header for chromosome 1. Each chromosome has its own header line in the file. \nHeaders usually contain additional details like source, version, or length, e.g.:\n&gt;chr1 dna:chromosome chromosome:GRCh38:1:1:248956422:1.\nThe following lines are the “sequence lines”.\nThey contain nucleotide bases (A, T, C, G, and sometimes N for unknown bases). In FASTA files, the sequence is often wrapped to fit within 80 characters per line.\n\n\n\nFASTQ is a simple format for raw unaligned sequencing reads. This is an extension to the FASTA file format and it is composed of sequence and an associated per base quality score.\n\n\nThe quality of the sequenced nucleotides is encoded in ASCII characters with decimal codes 33-126.\n\nThe ASCII code of “A” is 65. For a nucleotide that has the quality score encoded by A, this can be translates to quality score of Q=65−33=32.\nThe formula to compute the phred quality score is: P = 10−Q/10\n\nThe figure above shows the interpretation of the quality scores, in terms of probability of error and accuracy.\nBeware:\n\nmultiple quality scores were in use: Sanger, Solexa, Illumina 1.3+.\npaired-end sequencing produces two FASTQ files.\n\n\nThe ASCII table (American Standard Code for Information Interchange) is a character encoding standard that maps 128 characters (0–127) to numeric codes. It includes letters (uppercase and lowercase), digits, punctuation marks, control characters (e.g., newline, tab), and special symbols, enabling computers to represent and process text.",
    "crumbs": [
      "Home",
      "File formats",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_manual.html#sequence-data-formats",
    "href": "course_modules/Module1/module1_manual.html#sequence-data-formats",
    "title": "Manual",
    "section": "",
    "text": "These formats store raw or processed nucleotide and protein sequences. \n\n\nHere’s an example of 4 lines from a human reference genome FASTA file:\n\n\"&gt;chr1\" is the header for chromosome 1. Each chromosome has its own header line in the file. \nHeaders usually contain additional details like source, version, or length, e.g.:\n&gt;chr1 dna:chromosome chromosome:GRCh38:1:1:248956422:1.\nThe following lines are the “sequence lines”.\nThey contain nucleotide bases (A, T, C, G, and sometimes N for unknown bases). In FASTA files, the sequence is often wrapped to fit within 80 characters per line.\n\n\n\nFASTQ is a simple format for raw unaligned sequencing reads. This is an extension to the FASTA file format and it is composed of sequence and an associated per base quality score.\n\n\nThe quality of the sequenced nucleotides is encoded in ASCII characters with decimal codes 33-126.\n\nThe ASCII code of “A” is 65. For a nucleotide that has the quality score encoded by A, this can be translates to quality score of Q=65−33=32.\nThe formula to compute the phred quality score is: P = 10−Q/10\n\nThe figure above shows the interpretation of the quality scores, in terms of probability of error and accuracy.\nBeware:\n\nmultiple quality scores were in use: Sanger, Solexa, Illumina 1.3+.\npaired-end sequencing produces two FASTQ files.\n\n\nThe ASCII table (American Standard Code for Information Interchange) is a character encoding standard that maps 128 characters (0–127) to numeric codes. It includes letters (uppercase and lowercase), digits, punctuation marks, control characters (e.g., newline, tab), and special symbols, enabling computers to represent and process text.",
    "crumbs": [
      "Home",
      "File formats",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_manual.html#alignment-and-mapping-formats",
    "href": "course_modules/Module1/module1_manual.html#alignment-and-mapping-formats",
    "title": "Manual",
    "section": "Alignment and Mapping Formats",
    "text": "Alignment and Mapping Formats\nThese formats store sequence alignments against a reference genome.\nThe SAM file formats\n\nA SAM (Sequence Alignment/Map) file is a text-based format for storing biological sequences aligned to a reference genome. It consists of a header section (starting with @) and an alignment section (one line per read). Here’s an example of the first 10 lines from a typical SAM file aligned to the human reference genome:\nHeader Lines (@):\n@HD: Specifies the version and sorting order.\n@SQ: Lists the reference sequences (e.g., chr1, chr2) with their lengths.\n@PG: Records the alignment program used.\nAlignment Lines (non-@) fields:\nColumn 1: Query name (read001, read002, etc.).\nColumn 2: Flag (alignment information, e.g., strand orientation).\nColumn 3: Reference name (e.g., chr1, chr2).\nColumn 4: Alignment position.\nColumn 5: Mapping quality.\nColumn 6: CIGAR string (alignment operations).\nColumn 7-9: Mate pair information (if applicable).\nColumn 10: Sequence of the read.\nColumn 11: Quality scores.\nNote that BAM can contain\n• unmapped reads\n• multiple alignments of the same read\n• supplementary (chimeric) reads\nIn a SAM file, the Flag column (usually the second column in an alignment line) is an integer value that encodes various information about a read’s alignment using a bitwise representation. Each bit in the flag represents a specific property of the alignment, and the hexadecimal (Hex), decimal (Dec), and binary representations can be used to interpret these flags.\nHere’s a detailed explanation:\n\nStructure of the Flag\nThe flag is an integer value whose binary representation consists of 12 bits, each indicating a specific property of the read. Here’s the breakdown:\n\n\n\n\n\n\n\n\n\nBit\nHex\nDec\nDescription\n\n\n\n\n0\n0x1\n1\nThe read is paired in sequencing.\n\n\n1\n0x2\n2\nThe read is mapped in a proper pair.\n\n\n2\n0x4\n4\nThe read is unmapped.\n\n\n3\n0x8\n8\nThe mate is unmapped.\n\n\n4\n0x10\n16\nThe read is on the reverse strand.\n\n\n5\n0x20\n32\nThe mate is on the reverse strand.\n\n\n6\n0x40\n64\nThe read is the first in a pair.\n\n\n7\n0x80\n128\nThe read is the second in a pair.\n\n\n8\n0x100\n256\nThe alignment is not primary.\n\n\n9\n0x200\n512\nThe read fails quality control checks.\n\n\n10\n0x400\n1024\nThe read is a PCR or optical duplicate.\n\n\n11\n0x800\n2048\nThe read is supplementary (e.g., split-read).\n\n\n\nThe flag is the sum of all applicable properties. For example:\nFor a flag with value 83 (a written in decimal), the binary is 0000001010011.\nTo decode the bits with value 1, start from right to left:\n\n1 (0x1): On position 0 from right to left, the bit value is 1. This means that the read is paired.\n2 (0x2): On position 1 from right to left, the bit value is 1. This means that the read is mapped in a proper pair.\n16 (0x10): On position 4 from right to left, the bit value is 1. This means that the read is on the reverse strand.\n64 (0x40): On position 6 from right to left, the bit value is 1. This means that the read is the first read in a pair.\n\nOverall interpretation of a flag with value 83: this is the first read in a properly paired alignment, and it is mapped to the reverse strand.\n\n\nHow to interpret a SAM file flag\n\nTake the decimal value in the second column.\nConvert it to binary (or refer to the above table).\nDecode each bit to determine the alignment properties.\n\n\n\nThe BAM file format\nThe BAM (Binary Alignment/Map) format is a binary, compressed version of the widely-used SAM (Sequence Alignment/Map) format. It was specifically developed to enable fast processing, efficient storage, and random access to large-scale genomic data. Utilizing BGZF (Block GZIP) compression, BAM files support indexing, making them highly efficient for quick retrieval and visualization of specific alignments.\n\n\nKey Features of the BAM Format\n\nUniversal Compatibility: BAM can store alignments from a variety of mapping tools and sequencing technologies, making it a versatile format for genomic data analysis.\nCompact Size: BAM files are designed to save disk space without compromising data integrity. For example, 112 billion base pairs (Gbp) of Illumina sequencing data require just 116GB of storage in BAM format.\nIndexing Support: BAM files can be indexed, allowing for the rapid retrieval and visualization of specific regions of interest in the genome.\nLogical Grouping: Reads can be organized into logical groups such as lanes, libraries, and samples, providing flexibility for downstream analyses.\nIntegration with Variant Calling: BAM files are widely supported by tools for variant calling and other bioinformatics workflows.\nVisualization Compatibility: BAM files are compatible with popular genome browsers and visualization tools, including\n\nIGV\nEnsembl, and\nUCSC Genome Browser.\n\n\n\n\nSAM/BAM Tools\nSeveral tools have been developed for interacting with SAM and BAM files:\n\nSamtools: A comprehensive toolkit developed by the Wellcome Sanger Institute for manipulating SAM and BAM files.\nPicard Tools: Developed by the Broad Institute, this suite provides utilities for processing and analyzing BAM files.\nVisualization Tools:\n\nGenome browsers such as IGV (Integrative Genomics Viewer),\nEnsembl, and\nUCSC Genome Browser allow users to explore BAM files interactively.\n\n\nThe BAM format is an essential component of modern genomic workflows, offering an efficient, versatile, and widely-supported solution for storing and analyzing alignment data.\n\n\nReference-Based Compression\nAs sequencing technologies continue to advance, the volume of genomic data generated has increased exponentially, creating a growing challenge for data storage and management. While BAM files are efficient compared to uncompressed formats, they remain relatively large, requiring approximately 1.5–2 bytes per base pair. This size makes BAM files increasingly difficult to handle as disk capacity improvements fail to keep pace with the rapid growth in sequencing throughput.\n\n\nLimitations of BAM Compression\nBAM files employ a single conventional compression technique (BGZF compression) to store various types of data, including:\n\nRead bases: Every nucleotide in the sequencing read is stored in the file.\nBase qualities: Quality scores for each base are preserved, often contributing significantly to the file size.\n\nWhile effective, this approach compresses all data types uniformly, which limits opportunities for optimizing compression specific to genomic contexts.\n\n\nThe Case for Reference-Based Compression\nReference-based compression addresses these limitations by leveraging the known reference genome to reduce redundancy in storage. Instead of storing the full sequence and quality scores for every read:\n\nAlignment Information: Only deviations or differences from the reference genome (e.g., mismatches, insertions, deletions) are stored.\nSimplified Data Representation: Reads that match the reference genome perfectly require minimal data storage.\nSelective Compression: Different data types (e.g., quality scores, sequence alignments) can be compressed differently, optimizing file sizes further.\n\n\n\n\n\nBenefits of Reference-Based Compression\n\nDrastic Reduction in File Size: By only storing differences from the reference genome, file sizes can be reduced significantly compared to BAM files.\nEfficient Data Retrieval: Reference-based compressed files can still support random access, enabling efficient querying of specific genomic regions.\nScalability: Smaller files make it easier to store and transfer the growing amounts of sequencing data generated by high-throughput technologies.\n\n\n\nLooking Ahead\nWhile BAM files have been a cornerstone of genomic data storage, reference-based compression offers a promising alternative to address the challenges of handling ever-larger datasets. By focusing on reducing redundancy and optimising compression techniques, reference-based approaches can ensure that storage solutions keep pace with advancements in sequencing technology.\n\n\nThe CRAM file format\nCRAM is a highly efficient file format for storing aligned sequencing data, offering significant compression improvements over BAM by incorporating three key concepts:\n\nReference-Based Compression: Instead of storing full sequences, CRAM files record differences from a reference genome, reducing redundancy and file size.\nControlled Loss of Quality Information: Users can opt for lossy compression by discarding or simplifying base quality scores, further minimizing storage requirements without heavily impacting downstream analyses.\nOptimized Compression: Different types of data (e.g., base qualities, metadata, and extra tags) are compressed using tailored methods for maximum efficiency.\n\n\n\nKey Features of CRAM\n\nSmaller File Sizes: CRAM files are approximately 60% the size of BAM files in lossless mode, with further reductions possible in lossy mode.\nMaturity and Adoption: CRAM is well-integrated into production pipelines, with:\n\nSupport in Samtools/HTSlib since 2014.\n\nIntegration with Picard and GATK in 2015.\nCRAM’s efficient compression and flexibility make it a powerful alternative to BAM, especially for large-scale sequencing projects.",
    "crumbs": [
      "Home",
      "File formats",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_manual.html#variant-calling-formats",
    "href": "course_modules/Module1/module1_manual.html#variant-calling-formats",
    "title": "Manual",
    "section": "Variant Calling Formats",
    "text": "Variant Calling Formats\nThese formats store genetic variations detected in sequencing data.\n\nVariant Call Format (VCF) Summary\nThe Variant Call Format (VCF) is a widely-used, standardized file format for storing genetic variation data. Designed to handle diverse types of genomic variations, including SNPs, indels, and structural variations, VCF is integral to genomic workflows and supports the analysis of datasets involving multiple samples.\n\n\nKey Features\nVCF is a tab-delimited text file format that efficiently captures and organizes genomic variations. It supports all types of variants, from single nucleotide polymorphisms (SNPs) to large structural changes. With its flexibility and extensibility, VCF accommodates per-site and per-sample annotations, enabling rich metadata integration. Key fields include:\n\nINFO: This column contains site-level annotations such as functional impacts or allele frequencies.\nFORMAT: This column stores sample-specific details, like sequencing depth (DP) and genotype quality (GQ), with a dedicated column for each sample.\n\nTo ensure efficient storage and retrieval, VCF files are compressed using BGZF (bgzip) and indexed with TBI/CSI (tabix), allowing quick random access to specific genomic regions.\n\n\nGenotype Representation\nVCF employs a straightforward system to represent genotypes, using numbers to refer to the reference and alternate alleles. The reference allele is always denoted as 0, while the first alternate allele is 1, the second is 2, and so on. This encoding supports diploid organisms by using combinations of alleles:\n\nHomozygous Reference: 0/0 (e.g., A/A if A is the reference).\nHomozygous Alternative: 1/1 (e.g., G/G if G is the first alternate allele).\nHeterozygous: 0/1 or 1/2 (e.g., A/G or G/T).\n\nThis allele numbering system simplifies the representation of complex genotypes while maintaining clarity.\n\n\nRole in Genomic Analysis\nThe VCF format is a cornerstone of genomic workflows, supporting data storage, sharing, and analysis. Starting with sequencing data in FASTQ format, genomic analysis pipelines align reads (producing BAM files) and call variants, which are stored in VCF format. VCF captures both per-site details and per-sample information, ensuring that all relevant data is preserved. Its ability to handle multiple samples in a single file makes it scalable for population-level studies.\n\n\nTumor-Specific Annotations in VCF\nIn cancer genomics, VCF files often include additional annotations tailored for tumor samples. For example:\n\nAF (Allelic Fraction) records the proportion of alternate alleles observed in a tumor sample.\nGERMQ (Germline Quality) provides a Phred-scaled quality score indicating whether alternate alleles are unlikely to be germline variants.\nPON (Panel of Normals) marks sites that are found in a reference panel of normal samples, helping to identify potential artifacts.\n\nThese specialized annotations enhance the utility of VCF in clinical and research applications focused on cancer.\nBinary Call Format (BCF)\n\n\n\nTo further improve efficiency, the Binary Call Format (BCF) was developed as a binary version of VCF. BCF rearranges fields for faster access and compresses data more effectively, making it ideal for large-scale genomic studies. It retains all the capabilities of VCF while optimizing for speed and storage.\n\n\n\nExample VCF File\n##fileformat=VCFv4.2\n##source=VariantCallerTool\n##reference=GRCh38\n##INFO=&lt;ID=AF,Number=A,Type=Float,Description=\"Allele Frequency\"&gt;\n##INFO=&lt;ID=GERMQ,Number=1,Type=Integer,Description=\"Phred-scaled quality that ALT alleles are not germline\"&gt;\n##INFO=&lt;ID=PON,Number=0,Type=Flag,Description=\"Present in panel of normals\"&gt;\n##FORMAT=&lt;ID=GT,Number=1,Type=String,Description=\"Genotype\"&gt;\n##FORMAT=&lt;ID=DP,Number=1,Type=Integer,Description=\"Read Depth\"&gt;\n##FORMAT=&lt;ID=GQ,Number=1,Type=Integer,Description=\"Genotype Quality\"&gt;\n#CHROM  POS     ID      REF     ALT     QUAL    FILTER  INFO                    FORMAT      SAMPLE1     SAMPLE2\nchr17   7674236 .       G       A       99      PASS    AF=0.35;GERMQ=99;PON    GT:DP:GQ    0/1:30:99   0/0:28:98\nchr12   25398284 .      T       C       200     PASS    AF=0.12                 GT:DP:GQ    0/1:25:70   0/0:22:65\nchr5    11234567 .      C       T       150     PASS    AF=0.50;GERMQ=80        GT:DP:GQ    1/1:45:99   0/1:18:50\n\n\nExplanation of Each Column\nHeader Lines:\n##fileformat: Indicates the VCF version (v4.2 in this case).\n##INFO and ##FORMAT: Define metadata for INFO and FORMAT fields.\nData Columns:\nCHROM: Chromosome name (e.g., chr17).\nPOS: Position of the variant on the chromosome.\nID: Variant ID (e.g., . means no specific ID is provided).\nREF: Reference allele at this position (e.g., G).\nALT: Alternate allele(s) at this position (e.g., A).\nQUAL: Quality score for the variant call (e.g., 99 indicates high confidence).\nFILTER: Indicates whether the variant passed filters (e.g., PASS).\nINFO: Additional annotations, such as:\nAF: Allele frequency (e.g., AF=0.35 means 35% of reads support the alternate allele).\nGERMQ: Phred-scaled quality that the variant is not germline.\nPON: Indicates the site is present in a panel of normals.\nFORMAT: Defines the per-sample fields, such as:\nGT: Genotype (e.g., 0/1 = heterozygous, 1/1 = homozygous alternate).\nDP: Read depth at the site.\nGQ: Genotype quality score.\nSAMPLES: Individual sample data, such as: - SAMPLE1: Tumor sample (e.g., 0/1:30:99 means heterozygous, 30 reads, and high genotype quality). - SAMPLE2: Normal sample for comparison.\nWhat This File Represents\n\nchr17:7674236: A heterozygous G&gt;A mutation is observed in SAMPLE1, with an allele frequency of 35%. It is also flagged as present in a panel of normals (PON), indicating it might not be tumor-specific.\nchr12:25398284: A low-frequency T&gt;C mutation (allele frequency 12%) is observed in SAMPLE1, likely a subclonal event.\nchr5:11234567: A homozygous C&gt;T mutation in the tumor sample (SAMPLE1) but only heterozygous in the normal sample (SAMPLE2).",
    "crumbs": [
      "Home",
      "File formats",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_manual.html#other-important-information",
    "href": "course_modules/Module1/module1_manual.html#other-important-information",
    "title": "Manual",
    "section": "Other important information",
    "text": "Other important information\nA CIGAR string (Compact Idiosyncratic Gapped Alignment Report) is a sequence of operations used in SAM/BAM files to describe how a read is aligned to a reference genome. It specifies matches, mismatches, insertions, deletions, and other events in the alignment.\n\nStructure of a CIGAR String\nA CIGAR string consists of a series of operations, each represented by a length (number) followed by a character (operation type).\n\nCommon Operations:\n\nM: Match (alignment match or mismatch).\nExample: 10M = 10 bases aligned (could include mismatches).\nI: Insertion (relative to the reference).\nExample: 5I = 5 bases inserted in the query sequence.\nD: Deletion (relative to the reference).\nExample: 3D = 3 bases deleted in the reference sequence.\nS: Soft clipping (query sequence bases not aligned but present in the sequence).\nExample: 8S = 8 bases clipped from the start or end.\nH: Hard clipping (query sequence bases not aligned and removed from the sequence).\nExample: 10H = 10 bases clipped and not stored in the sequence.\nN: Skipped region (in the reference).\nExample: 100N = 100 bases skipped in the reference (e.g., introns in RNA-Seq).\nP: Padding (used with insertions or deletions in multiple sequence alignment).\n=: Exact match to the reference.\nX: Mismatch to the reference.\n\nExamples:\nRef: ACGTACGTACGTACGT\nRead: ACGT----ACGTACGA\nCigar: 4M 4D 8M\nRef: ACGT----ACGTACGT\nRead: ACGTACGTACGTACGT\nCigar: 4M 4I 8M\n\nRef: ACTCAGTG--GT\nRead: ACGCA-TGCAGTtagacgt\nCigar: 5M 1D 2M 2I 2M 7S\n\n\n\n** Key Takeaways:**\n\nCIGAR strings describe how the query sequence aligns to the reference genome.\nThey are critical for understanding alignments in SAM/BAM files.\nTools like samtools or IGV can help visualize alignments and interpret CIGAR strings.\n\n\n\nThe MAF file format\nMAF stands for “Mutation-Annotation Format” and is a tab-delimited text file containing aggregated information from VCF files (NCI-GDC). It aggregates lots of information – 120+ fields per mutation! You can review all these at: https://docs.gdc.cancer.gov/Data/File_Formats/MAF_Format/\nYou can convert between VCF and MAF via vcf2maf tools!\nperl vcf2maf.pl –input-vcf [vcf_file] –output-maf\n[maf_file] –vep-path /cm/shared/apps/vep/ensembl-vep-\nrelease-106.1/ –vep-data /mnt/Archives/vep/106/38/ –ref-\nfasta [fasta_file] –tumor-id [tumor] –normal-id [normal]",
    "crumbs": [
      "Home",
      "File formats",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_manual.html#ga4gh",
    "href": "course_modules/Module1/module1_manual.html#ga4gh",
    "title": "Manual",
    "section": "GA4GH",
    "text": "GA4GH\nThe Global Alliance for Genomics and Health (GA4GH) is an international coalition dedicated to advancing human health by creating frameworks and standards for sharing genomic and clinical data. Its mission is to enable responsible, ethical, and secure global collaboration in genomics research and healthcare.\nCore Mission:\n\nEstablish a common framework to facilitate the sharing of genomic and clinical data.\n\n\n\nImprove human health through international collaboration and innovation.\n\n\nWorking Groups\nGA4GH is organized into several working groups, each focusing on a specific aspect of genomic data sharing:\n\nClinical: Applies genomics to healthcare and medicine.\nRegulatory and Ethics: Addresses legal, ethical, and policy issues related to genomic data.\nSecurity: Ensures the privacy and protection of sensitive genomic data.\nData: Develops tools, resources, and initiatives to enhance data sharing and analysis.\n\n\n\nKey Projects of the Data Working Group\n\nBeacon Project: Tests global willingness to share genetic data.\nBRCA Challenge: Advances understanding of breast and other cancers by studying BRCA-related genetic variants.\nMatchmaker Exchange: Connects researchers with data on rare phenotypes or genotypes.\nReference Variation: Standardizes how genomes are described to improve interpretation and assembly.\nBenchmarking: Creates toolkits for evaluating variant calling in germline, cancer, and transcriptomic data.\nFile Formats: Develops and maintains standards such as CRAM, SAM/BAM, and VCF/BCF for efficient genomic data storage and processing.\n\n\n\nFile Format Standards\nGA4GH maintains file format standards through resources like the HTS Specifications repository (http://samtools.github.io/hts-specs/), which supports interoperability and efficient data management in genomics.\nAdditional Resources:\n1. File format tutorial - University of Connecticut\n2. UCSC Galaxy - Data file formats\n3. 12 Common Bioinformatics Files Types Explained (Youtube Video)",
    "crumbs": [
      "Home",
      "File formats",
      "Manual"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1.html",
    "href": "course_modules/Module1/module1.html",
    "title": "Overview",
    "section": "",
    "text": "Module Title\nNGS Data formats and QC\n\n\nAuthors\nThis tutorial was written by Jacqui Keane and Sara Sjunnebo based on material from Petr Danecek and Thomas Keane.\n\n\nDuration\n3 hours\n\n\nKey topics\nIn this module, learners will look at data formats in detail. At the end of the theory, there is a mandatory to test their knowledge.\nFASTQ: Unaligned read sequences with base qualities\nSAM/BAM: Unaligned or aligned reads, text and binary formats\nCRAM: Better compression than BAM\nVCF/BCF: Flexible variant call format; SNPs, indels, structural variations\n\n\nLearning outcomes\n• Describe the different NGS data formats available (FASTQ, SAM/BAM, CRAM, VCF/BCF)\n• Perform a QC assessment of high throughput sequence data\n• Perform conversions between the different data formats\n\n\nActivities\n\nLecture: 1 hour\nPractical exercises: 1.5 hours\nAssessment quiz: 0.5 hours\n\n\n\nManual and practical execises\nModule manual\nExercises\nSolutions\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual. Instructions on how to that will be added here.)\n\n\nLecture recording, notes and scripts\nPre-recorded lecture, temporally stored on the LMS\n\n\nCheck you knowledge quiz\nQuestions\n\n\nSummary of this module\n\n\nTeaching guidance\nInstructor notes\n\n\nDatasets\nPractice files for this module can be found on Github.\n\n\nPlatform Guidance\nGuide",
    "crumbs": [
      "Home",
      "File formats",
      "Overview"
    ]
  },
  {
    "objectID": "course_modules/Module1/module1_platform_guidance.html",
    "href": "course_modules/Module1/module1_platform_guidance.html",
    "title": "Platform guidance",
    "section": "",
    "text": "You can use any text editor or command-line tools (on Linux/Mac or Windows Subsystem for Linux) to view FASTA/FASTQ files. Ensure samtools (or an equivalent tool) is available to view and index BAM files; these tools run on any OS (or via Docker) so learners can follow along regardless of platform.",
    "crumbs": [
      "Home",
      "File formats",
      "Platform guidance"
    ]
  }
]