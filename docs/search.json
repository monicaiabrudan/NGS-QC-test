[
  {
    "objectID": "course_modules/Module1/module1.html",
    "href": "course_modules/Module1/module1.html",
    "title": "Overview",
    "section": "",
    "text": "Content by Eric Dawson (Bioinformatics Scientist (Genomics / AI) - Nvidia) and Carla Daniela Robles Espinoza (Laboratorio Internacional de Investigación sobre el Genoma Humano, Universidad Nacional Autónoma de México)\n\nModule Title\nNGS Data formats and QC\n\n\nDuration\n3 hours\n\n\nKey topics\nIn this module, learners will look at data formats in detail. At the end of the theory, there is a mandatory to test their knowledge.\nFASTQ\n• Unaligned read sequences with base qualities\nSAM/BAM\n• Unaligned or aligned reads\n• Text and binary formats\nCRAM\n• Better compression than BAM\nVCF/BCF\n• Flexible variant call format\n• Arbitrary types of sequence variation\n• SNPs, indels, structural variations\n\n\nActivities\n\nLecture: 1 hour\nPractical exercises: 1.5 hours\nAssessment quiz: 0.5 hours\n\n\n\nManual\nModule manual\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual. Instructions on how to that will be added here.)\n\n\nLecture recording, notes and scripts\nPre-recorded lecture, temporally stored on the LMS\n\n\nPractical exercises\n\nGoogle Colab\nYou can practice working with the file formats presented in this modules using a Google Colab developed specifically for this purpose.\nFor detailed information on what is Google Colab, check: What is google colab?\nTo get started with Google Colab use a shared link from Google Drive:\na. Open the Shared Link: Click on the shared link provided to you. It will usually look like this: https://drive.google.com/file/d/....\nThe shared link is IntroductionToLinux.ipynb\nb. Make a Copy: If the link opens a view-only version of the notebook, you can create your own editable copy by selecting File &gt; Save a copy in Drive. This will save the notebook to your Google Drive.\nc. Open Your Copy: Once the copy is saved, you can open it from your Google Drive. You can also access it anytime through Google Colab by selecting File &gt; Open notebook and navigating to your Drive.\n\n\nVirtual Machine\nCommand line walk through.\n\n\n\nDatasets\nPractice files for this module can be found on Github.\n\n\nAssessment quiz\nQuestions"
  },
  {
    "objectID": "course_modules/Module1/module1_manual.html",
    "href": "course_modules/Module1/module1_manual.html",
    "title": "Manual",
    "section": "",
    "text": "These formats store raw or processed nucleotide and protein sequences. \n\n\nHere’s an example of 4 lines from a human reference genome FASTA file:\n\n\"&gt;chr1\" is the header for chromosome 1. Each chromosome has its own header line in the file. \nHeaders usually contain additional details like source, version, or length, e.g.:\n&gt;chr1 dna:chromosome chromosome:GRCh38:1:1:248956422:1.\nThe following lines are the “sequence lines”.\nThey contain nucleotide bases (A, T, C, G, and sometimes N for unknown bases). In FASTA files, the sequence is often wrapped to fit within 80 characters per line.\n\n\n\nFASTQ is a simple format for raw unaligned sequencing reads. This is an extension to the FASTA file format and it is composed of sequence and an associated per base quality score.\n\n\nThe quality of the sequenced nucleotides is encoded in ASCII characters with decimal codes 33-126.\n\nThe ASCII code of “A” is 65. For a nucleotide that has the quality score encoded by A, this can be translates to quality score of Q=65−33=32.\nThe formula to compute the phred quality score is: P = 10−Q/10\n\nThe figure above shows the interpretation of the quality scores, in terms of probability of error and accuracy.\nBeware:\n\nmultiple quality scores were in use: Sanger, Solexa, Illumina 1.3+.\npaired-end sequencing produces two FASTQ files.\n\n\nThe ASCII table (American Standard Code for Information Interchange) is a character encoding standard that maps 128 characters (0–127) to numeric codes. It includes letters (uppercase and lowercase), digits, punctuation marks, control characters (e.g., newline, tab), and special symbols, enabling computers to represent and process text."
  },
  {
    "objectID": "course_modules/Module1/module1_manual.html#other-important-information",
    "href": "course_modules/Module1/module1_manual.html#other-important-information",
    "title": "Manual",
    "section": "Other important information",
    "text": "Other important information\nA CIGAR string (Compact Idiosyncratic Gapped Alignment Report) is a sequence of operations used in SAM/BAM files to describe how a read is aligned to a reference genome. It specifies matches, mismatches, insertions, deletions, and other events in the alignment.\n\nStructure of a CIGAR String\nA CIGAR string consists of a series of operations, each represented by a length (number) followed by a character (operation type).\n\nCommon Operations:\n\nM: Match (alignment match or mismatch).\nExample: 10M = 10 bases aligned (could include mismatches).\nI: Insertion (relative to the reference).\nExample: 5I = 5 bases inserted in the query sequence.\nD: Deletion (relative to the reference).\nExample: 3D = 3 bases deleted in the reference sequence.\nS: Soft clipping (query sequence bases not aligned but present in the sequence).\nExample: 8S = 8 bases clipped from the start or end.\nH: Hard clipping (query sequence bases not aligned and removed from the sequence).\nExample: 10H = 10 bases clipped and not stored in the sequence.\nN: Skipped region (in the reference).\nExample: 100N = 100 bases skipped in the reference (e.g., introns in RNA-Seq).\nP: Padding (used with insertions or deletions in multiple sequence alignment).\n=: Exact match to the reference.\nX: Mismatch to the reference.\n\nExamples:\nRef: ACGTACGTACGTACGT\nRead: ACGT----ACGTACGA\nCigar: 4M 4D 8M\nRef: ACGT----ACGTACGT\nRead: ACGTACGTACGTACGT\nCigar: 4M 4I 8M\n\nRef: ACTCAGTG--GT\nRead: ACGCA-TGCAGTtagacgt\nCigar: 5M 1D 2M 2I 2M 7S\n\n\n\n** Key Takeaways:**\n\nCIGAR strings describe how the query sequence aligns to the reference genome.\nThey are critical for understanding alignments in SAM/BAM files.\nTools like samtools or IGV can help visualize alignments and interpret CIGAR strings.\n\n\n\nThe MAF file format\nMAF stands for “Mutation-Annotation Format” and is a tab-delimited text file containing aggregated information from VCF files (NCI-GDC). It aggregates lots of information – 120+ fields per mutation! You can review all these at: https://docs.gdc.cancer.gov/Data/File_Formats/MAF_Format/\nYou can convert between VCF and MAF via vcf2maf tools!\nperl vcf2maf.pl –input-vcf [vcf_file] –output-maf\n[maf_file] –vep-path /cm/shared/apps/vep/ensembl-vep-\nrelease-106.1/ –vep-data /mnt/Archives/vep/106/38/ –ref-\nfasta [fasta_file] –tumor-id [tumor] –normal-id [normal]"
  },
  {
    "objectID": "course_modules/Module1/module1_manual.html#ga4gh",
    "href": "course_modules/Module1/module1_manual.html#ga4gh",
    "title": "Manual",
    "section": "GA4GH",
    "text": "GA4GH\nThe Global Alliance for Genomics and Health (GA4GH) is an international coalition dedicated to advancing human health by creating frameworks and standards for sharing genomic and clinical data. Its mission is to enable responsible, ethical, and secure global collaboration in genomics research and healthcare.\nCore Mission:\n\nEstablish a common framework to facilitate the sharing of genomic and clinical data.\n\n\n\nImprove human health through international collaboration and innovation.\n\n\nWorking Groups\nGA4GH is organized into several working groups, each focusing on a specific aspect of genomic data sharing:\n\nClinical: Applies genomics to healthcare and medicine.\nRegulatory and Ethics: Addresses legal, ethical, and policy issues related to genomic data.\nSecurity: Ensures the privacy and protection of sensitive genomic data.\nData: Develops tools, resources, and initiatives to enhance data sharing and analysis.\n\n\n\nKey Projects of the Data Working Group\n\nBeacon Project: Tests global willingness to share genetic data.\nBRCA Challenge: Advances understanding of breast and other cancers by studying BRCA-related genetic variants.\nMatchmaker Exchange: Connects researchers with data on rare phenotypes or genotypes.\nReference Variation: Standardizes how genomes are described to improve interpretation and assembly.\nBenchmarking: Creates toolkits for evaluating variant calling in germline, cancer, and transcriptomic data.\nFile Formats: Develops and maintains standards such as CRAM, SAM/BAM, and VCF/BCF for efficient genomic data storage and processing.\n\n\n\nFile Format Standards\nGA4GH maintains file format standards through resources like the HTS Specifications repository (http://samtools.github.io/hts-specs/), which supports interoperability and efficient data management in genomics.\nAdditional Resources:\n1. File format tutorial - University of Connecticut\n2. UCSC Galaxy - Data file formats\n3. 12 Common Bioinformatics Files Types Explained (Youtube Video)"
  },
  {
    "objectID": "course_modules/Module6/module6.html",
    "href": "course_modules/Module6/module6.html",
    "title": "Overview",
    "section": "",
    "text": "Module Title\nRNA-Seq\n\n\nDuration\n3 hours\n\n\nKey topics\n\n\nActivities\n\nLecture: 1 hour\nPractical exercises: 1.5 hours\nAssessment quiz: 0.5 hours\n\n\n\nManual\nModule manual\nExercises\nSolutions\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual. Instructions on how to that will be added here.)\n\n\nLecture recording, notes and scripts\n\n\nPractical exercises\n\nVirtual Machine\nCommand line walk through.\n\n\n\nDatasets\n\n\nAssessment quiz\nQuestions\nSolution"
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html",
    "href": "course_modules/Module6/module6_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "RNA sequencing (RNA-Seq) is a high-throughput method used to profile the transcriptome, quantify gene expression and discover novel RNA molecules. This tutorial uses RNA sequencing of malaria parasites to walk you through transcriptome visualisation, performing simple quality control checks and will show you how to profile transcriptomic differences by identifying differentially expressed genes.\nFor an introduction to RNA-Seq principles and best practices see:\nA survey of best practices for RNA-Seq data analysis, Ana Conesa, Pedro Madrigal, Sonia Tarazona, David Gomez-Cabrero, Alejandra Cervera, Andrew McPherson, Michał Wojciech Szcześniak, Daniel J. Gaffney, Laura L. Elo, Xuegong Zhang and Ali Mortazavi, Genome Biol. 2016 Jan 26;17:13 doi:10.1186/s13059-016-0881-8\n\n\n\nBy the end of this tutorial you can expect to be able to:\n• Align RNA-Seq reads to a reference genome and a transcriptome\n• Visualise transcription data using standard tools\n• Perform QC of NGS transcriptomic data\n• Quantify the expression values of your transcripts using standard tools\n\n\n\nThis tutorial comprises the following sections:\n1. Introducing the tutorial dataset\n2. Mapping RNA-Seq reads to the genome with HISAT2\n3. Visualising transcriptomes with IGV\n4. Transcript quantification with Kallisto\n5. Identifying differentially expressed genes with Sleuth\n6. Interpreting the results\n7. Key aspects of differential expression analysis\n\n\n\nThis tutorial was written by Victoria Offord based on materials from Adam Reid.\nHISAT2 https://ccb.jhu.edu/software/hisat2/index.shtml 2.1.0\nsamtools https://github.com/samtools/samtools 1.10\nIGV https://software.broadinstitute.org/software/igv/ 2.7.2\nkallisto https://pachterlab.github.io/kallisto/download 0.46.2\nR https://www.r-project.org/ 4.0.2\nsleuth https://pachterlab.github.io/sleuth/download 0.30.0\nbedtools http://bedtools.readthedocs.io/en/latest/content/installation.html 2.29.2\n\n\n\nThis tutorial assumes that you have the following software or packages and their dependencies installed on your computer. The software or packages used in this tutorial may be updated from time to time so, we have also given you the version which was used when writing the tutorial.\n\n\n\nYou can find the data for this tutorial by typing the following command in a new terminal window.\ncd /home/manager/course_data/rna_seq\nNow, let’s head to the first section of this tutorial which will be introducing the tutorial dataset."
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#rna-seq-expression-analysis",
    "href": "course_modules/Module6/module6_exercises.html#rna-seq-expression-analysis",
    "title": "Exercises",
    "section": "",
    "text": "RNA sequencing (RNA-Seq) is a high-throughput method used to profile the transcriptome, quantify gene expression and discover novel RNA molecules. This tutorial uses RNA sequencing of malaria parasites to walk you through transcriptome visualisation, performing simple quality control checks and will show you how to profile transcriptomic differences by identifying differentially expressed genes.\nFor an introduction to RNA-Seq principles and best practices see:\nA survey of best practices for RNA-Seq data analysis, Ana Conesa, Pedro Madrigal, Sonia Tarazona, David Gomez-Cabrero, Alejandra Cervera, Andrew McPherson, Michał Wojciech Szcześniak, Daniel J. Gaffney, Laura L. Elo, Xuegong Zhang and Ali Mortazavi, Genome Biol. 2016 Jan 26;17:13 doi:10.1186/s13059-016-0881-8\n\n\n\nBy the end of this tutorial you can expect to be able to:\n• Align RNA-Seq reads to a reference genome and a transcriptome\n• Visualise transcription data using standard tools\n• Perform QC of NGS transcriptomic data\n• Quantify the expression values of your transcripts using standard tools\n\n\n\nThis tutorial comprises the following sections:\n1. Introducing the tutorial dataset\n2. Mapping RNA-Seq reads to the genome with HISAT2\n3. Visualising transcriptomes with IGV\n4. Transcript quantification with Kallisto\n5. Identifying differentially expressed genes with Sleuth\n6. Interpreting the results\n7. Key aspects of differential expression analysis\n\n\n\nThis tutorial was written by Victoria Offord based on materials from Adam Reid.\nHISAT2 https://ccb.jhu.edu/software/hisat2/index.shtml 2.1.0\nsamtools https://github.com/samtools/samtools 1.10\nIGV https://software.broadinstitute.org/software/igv/ 2.7.2\nkallisto https://pachterlab.github.io/kallisto/download 0.46.2\nR https://www.r-project.org/ 4.0.2\nsleuth https://pachterlab.github.io/sleuth/download 0.30.0\nbedtools http://bedtools.readthedocs.io/en/latest/content/installation.html 2.29.2\n\n\n\nThis tutorial assumes that you have the following software or packages and their dependencies installed on your computer. The software or packages used in this tutorial may be updated from time to time so, we have also given you the version which was used when writing the tutorial.\n\n\n\nYou can find the data for this tutorial by typing the following command in a new terminal window.\ncd /home/manager/course_data/rna_seq\nNow, let’s head to the first section of this tutorial which will be introducing the tutorial dataset."
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#introducing-the-tutorial-dataset",
    "href": "course_modules/Module6/module6_exercises.html#introducing-the-tutorial-dataset",
    "title": "Exercises",
    "section": "2 Introducing the tutorial dataset",
    "text": "2 Introducing the tutorial dataset\nWorking through this tutorial, you will investigate the effect of vector transmission on gene expression of the malaria parasite. The dataset you will be using for this tutorial and Figure 1 have been taken from the following publication:\nVector transmission regulates immune control of Plasmodium virulence, Philip J. Spence, William Jarra, Prisca Lévy, Adam J. Reid, Lia Chappell, Thibaut Brugat, Mandy Sanders, Matthew Berriman and Jean Langhorne, Nature. 2013 Jun 13; 498(7453): 228–231 doi:10.1038/nature12231\nFigure 1. Serial blood passage increases virulence of malaria parasites.\n\n\n2.1 Is the transcriptome of a mosquito-transmitted parasite different from one which has not passed through a mosquito?\nThe key reason for asking this question is that parasites which are transmitted by mosquito (MT) are less virulent (severe/harmful) than those which are serially blood passaged (SBP) in the laboratory.\nFigure 1A shows the malaria life cycle, the red part highlighting the mosquito stage. Figure 1B shows the difference in virulence, measured by blood parasitemia (presence of parasites in the blood), between mosquito-transmitted and serially blood passaged parasites.\nFigure 1C shows that increasing numbers of blood passage post mosquito transmission results in increasing virulence, back to around 20% parasitemia. Subsequent mosquito transmission of high virulence parasites render them low virulence again.\nWe hypothesise that parasites which have been through the mosquito are somehow better able to control the mosquito immune system than those which have not. This control of the immune system would result in lower parasitemia because this is advantageous for the parasite. Too high a parasitemia is bad for the mouse and therefore bad for the parasite.\n\n\n2.2 Exercise 1\nIn this tutorial, you will be analysing five RNA samples, each of which has been sequenced on an Illumina HiSeq sequencing machine. There are two conditions: serially blood-passaged parasites (SBP) and mosquito transmitted parasites (MT). One with three biological replicates (SBP), one with two biological replicates (MT).\nSample name Experimental condition\nReplicate number MT1 mosquito transmitted parasites 1\nMT2 mosquito transmitted parasites 2\nSBP1 serially blood-passaged parasites 1\nSBP2 serially blood-passaged parasites 2\nSBP3 serially blood-passaged parasites 3\nCheck that you can see the tutorial FASTQ files in the data directory.\nls data/*.fastq.gz\nThe FASTQ files contain the raw sequence reads for each sample. There are four lines per read:\n1. Header\n2. Sequence\n3. Separator (usually a ‘+’)\n4. Encoded quality value\nTake a look at one of the FASTQ files.\nzless data/MT1_1.fastq.gz | head\nFind out more about FASTQ formats at https://en.wikipedia.org/wiki/FASTQ_format.\n\n\n2.3 Questions\n2.3.1 Q1: Why is there more than one FASTQ file per sample?\nHint: think about why there is a MT1_1.fastq.gz and a MT1_2.fastq.gz\n2.3.2 Q2: How many reads were generated for the MT1 sample?\nHint: we want the total number of reads from both files (MT1_1.fastq.gz and MT1_2.fastq.gz) so perhaps think about the FASTQ format and the number of lines for each read or whether there’s anything you can use in the FASTQ header to search and count…\nNow let’s move on to mapping RNA-Seq reads to the genome using HISAT2."
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#mapping-rna-seq-reads-to-the-genome-using-hisat2",
    "href": "course_modules/Module6/module6_exercises.html#mapping-rna-seq-reads-to-the-genome-using-hisat2",
    "title": "Exercises",
    "section": "3 Mapping RNA-Seq reads to the genome using HISAT2",
    "text": "3 Mapping RNA-Seq reads to the genome using HISAT2\n\n3.1 Introduction\nFor this exercise, we have reduced the number of reads in each sample to around 2.5 million to reduce the mapping time. However, this is sufficient to detect most differentially expressed genes.\nThe objectives of this part of the tutorial are:\n• use HISAT2 to build an index from the reference genome\n• use HISAT2 to map RNA-Seq reads to the reference genome\n\n\n3.1.1 Mapping RNA-Seq reads to a genome\nBy this stage, you should have already performed a standard NGS quality control check on your reads to see whether there were any issues with the sample preparation or sequencing. In the interest of time, we won’t be doing that as part of this tutorial, but feel free to use the tools from earlier modules to give that a go later if you have time.\nNext, we map our RNA-Seq reads to a reference genome to get context. This allows you to visually inspect your RNA-Seq data, identify contamination, novel exons and splice sites as well as giving you an overall feel for your transcriptome.\nHISAT2 To map the RNA-Seq reads from our five samples to the reference genome, we will be using HISAT2, a fast and sensitive splice-aware aligner. HISAT2 compresses the genome using an indexing scheme based on the Burrows-Wheeler transform (BWT) and Ferragina-Manzini (FM) index to reduce the amount of space needed to store the genome. This also makes the genome quick to search, using a whole-genome FM index to anchor each alignment and then tens of thousands local FM indexes for very rapid extensions of these alignments.\nFor more information, and to find the original version of Figure 2, please see the HISAT paper:\nHISAT: a fast spliced aligner with low memory requirements, Daehwan Kim, Ben Langmead and Steven L Salzberg, Nat Methods. 2015 Apr;12(4):357-60. doi:10.1038/nmeth.3317\nHISAT2 is a splice-aware aligner which means it takes into account that when a read is mapped it may be split across multiple exons with (sometimes large) intronic gaps between aligned regions.\nAs you can see in Figure 2, HISAT2 splits read alignments into five classes based on the number of exons the read alignment is split across and the length of the anchor (longest continuously mapped portion of a split read):\n• Aligns to a single exon (M)\n• Alignment split across 2 exons with long anchors over 15bp (2M_gt_15)\n• Alignment split across 2 exons with intermediate anchors between 8bp and 15bp (2M_8_15)\n• Alignment split across 2 exons with short anchors less than 7bp (2M_1_7)\n• Alignment split across more than 2 exons (gt_2M)\nHISAT2 used the global index to place the longest continuously mapped portion of a read (anchor).\nThis information is then used to identify the relevant local index. In most cases, HISAT2 will only need to use a single local index to place the remaining portion of the read without having to search the rest of the genome.\nFor the human genome, HISAT2 will build a single global index and 48,000 local FM indexes. Each of the local indexes represents a 64kb genomic region. The majority of human introns are significantly shorter than 64kb, so &gt;90% of human introns fall into a single local index. Moreover, each of the local indexes overlaps its neighbour by ~1kb which means that it also has the ability to detect reads spanning multiple indexes.\nFigure 2. Read types and their relative proportions from 20 million simulated 100-bp reads\nThere are five HISAT2 RNA-seq read mapping categories: (i) M, exonic read; (ii) 2M_gt_15, junction reads with long, &gt;15-bp anchors in both exons; (iii) 2M_8_15, junction reads with intermediate, 8- to 15-bp anchors; (iv) 2M_1_7, junction reads with short, 1- to 7-bp, anchors; and (v) gt_2M, junction reads spanning more than two exons (Figure 2A). Exoninc reads span only a single exon and represent over 60% of the read mappings in the 20 million 100-bp simulated read dataset.\n\n\n3.2 Exercise 2\nBe patient, each of the following steps will take a couple of minutes!\nLook at the usage instructions for hisat2-build.\nhisat2-build -h\nThis not only tells us the version of HISAT2 we’re using (essential for publication methods):\nHISAT2 version 2.1.0 by Daehwan Kim (infphilo@gmail.com, http://www.ccb.jhu.edu/people/infphilo\nBut, that we also need to give histat2-build two pieces of information:\nUsage: hisat2-build [options]* &lt;reference_in&gt; &lt;ht2_index_base&gt;\nThese are:\n• &lt;reference_in&gt; location of our reference sequence file (PccAS_v3_genome.fa)\n• &lt;ht2_index_base&gt; what we want to call our HISAT2 index files (PccAS_v3_hisat2.idx)\nBuild a HISAT2 index for our Plasmodium chabaudi chabaudi AS (P. chabaudi) reference genome using hisat2-build.\nhisat2-build data/PccAS_v3_genome.fa data/PccAS_v3_hisat2.idx\nYou can see the generated index files using:\nls data/PccAS_v3_hisat2.idx*\nLook at the usage for hisat2.\nhisat2 -h\nHere we can see that hisat2 needs several bits of information so that it can do the mapping:\nhisat2 [options]* -x &lt;ht2-idx&gt; {-1 &lt;m1&gt; -2 &lt;m2&gt; | -U &lt;r&gt;} [-S &lt;sam&gt;]\n• -x &lt;ht2-idx&gt; the prefix that we chose for our index files with hisat2-build (PccAS_v3_hisat2.idx)\n• {-1 &lt;m1&gt; -2 &lt;m2&gt; | -U &lt;r&gt;} the left (-1) and right (-2) read files for the sample (MT1_1.fastq and MT1_2.fastq respectively\n• [-S &lt;sam&gt;] the name of the file we want to write the output alignment to (MT1.sam) as, by default, hisat2 will print the results to the terminal (stdout)\nWe will also be adding one more piece of information, the maximum intron length (default 500,000 bases). For this analysis, we want to set the maximum intron length to 10,000. We can do this by adding the option –max-intronlen 10000.\nMap the reads for the MT1 sample using HISAT2.\nhisat2 --max-intronlen 10000 -x data/PccAS_v3_hisat2.idx -1 data/MT1_1.fastq.gz -2 data/MT1_2.fastq.gz -S data/MT1.sam\nHISAT2 has written the alignment in SAM format. This is a format which allows humans to look at our alignments. However, we need to convert the SAM file to its binary version, a BAM file. We do this for several reasons. Mainly we do it because most downstream programs require our alignments to be in BAM format and not SAM format. However, we also do it because the BAM file is smaller and so takes up less (very precious!) storage space. For more information, see the format guide:\nhttp://samtools.github.io/hts-specs/SAMv1.pdf.\nConvert the SAM file to a BAM file.\nsamtools view -b -o data/MT1.bam data/MT1.sam\nWe now need to sort the BAM file ready for indexing. When we aligned our reads with HISAT2, alignments were produced in the same order as the sequences in our FASTQ files. To index the BAM file, we need the alignments ordered by their respective positions in the reference genome. We can do this using samtools sort to sort the alignments by their co-ordinates for each chromosome.\nSort the BAM file.\nsamtools sort -o data/MT1_sorted.bam data/MT1.bam\nNext, we need to index our BAM file. This makes searching the alignments much more efficient.\nIt allows programs like IGV (which we will be using to visualise the alignment) to quickly get the alignments that overlap the genomic regions you’re looking at. We can do this with samtools index which will generate an index file with the extension .bai.\nIndex the BAM file so that it can be read efficiently by IGV.\nsamtools index data/MT1_sorted.bam\nNow, repeat this process of mapping, converting (SAM to BAM), sorting and indexing with the reads from the MT2 sample. You can run the previous steps as a single command.\nhisat2 –max-intronlen 10000 -x data/PccAS_v3_hisat2.idx -1 data/MT2_1.fastq.gz -2 data/MT2_2.fastq.gz | samtools view -b -| samtools sort -o data/MT2_sorted.bam - && samtools index data/MT2_sorted.bam\nLet’s not forget our SBP samples. We’ve already provided the BAM files for these samples.\nPlease check that you can see the BAM files before continuing.\nls -al data/SBP*.bam\nYou should see three files:\ndata/SBP1_sorted.bam\ndata/SBP2_sorted.bam\ndata/SBP3_sorted.bam\nIf you can’t see these files, please let your instructor know!!\nWe’ve previously shown you how to run HISAT2 and samtools with individual and one-line commands. For the SBP samples, a bash script was used to generate our genome alignments prior to this tutorial.\nTo take a look at the script you can run:\nless data/map_SBP_samples.sh\nIf you have time at the end of the tutorial, feel free to take a closer look at the script and a more detailed breakdown of what it does in Running commands on multiple samples. Bash scripts and loops are a useful way of automating an analysis and running the same commands for multiple samples. Imagine if you had 50 samples and not 5! In truth, you’d probably want to run that many samples on a compute cluster, in parallel. But that’s outside the scope of this tutorial.\n\n\n3.3 Questions\n3.3.1 Q1: How many index files were generated when you ran hisat2-build?\nHint: look for the files with the .ht2 extension\n3.3.2 Q2: What was the overall alignment rate for each of the MT samples (MT1 and MT2) to the reference genome?\nHint: look at the the output from the hisat2 commands\n3.3.3 Q3: How many MT1 and MT2 reads were not aligned to the reference genome?\nHint: look at the the output from the hisat2 commands, you’re looking for reads (not read pairs) which have aligned 0 times (remember that one read from a pair may map even if the other doesn’t)"
  },
  {
    "objectID": "course_modules/Module6/module6_exercises.html#visualising-transcriptomes-with-igv",
    "href": "course_modules/Module6/module6_exercises.html#visualising-transcriptomes-with-igv",
    "title": "Exercises",
    "section": "4 Visualising transcriptomes with IGV",
    "text": "4 Visualising transcriptomes with IGV\n\n4.1 Introduction\nIntegrative Genome Viewer (IGV) allows us to visualise genomic datasets. We have provided a quick start guide which contains the information you need to complete Exercise 3. There is also an IGV user guide online which contains more information on all of the IGV features and functions: http://software.broadinstitute.org/software/igv/UserGuide.\nThe objectives of this part of the tutorial are:\n• load a reference genome into IGV and navigate the genome\n• load an annotation file into IGV and explore gene structure\n• load read alignments into IGV and inspect read alignments\n\n\n4.2 Exercise 3\nFirst, we will use samtools to create an index for the P. chabaudi reference genome, which IGV will use to traverse the genome. This index file will have the extension .fai and should always be in the same directory as the reference genome.\nFirst, index the genome fasta file (required by IGV).\nsamtools faidx data/PccAS_v3_genome.fa\nNow, start IGV.\nigv &\nThis will open the IGV main window. Now, we need to tell IGV which genome we want to use. IGV has many pre-loaded genomes available, but P. chabaudi is not one of them. This means we will need to load our genome from a file.\nLoad your reference genome into IGV. Go to “Genomes -&gt; Load Genome from File…”. Select “PccAS_v3_genome.fa” and click “Open”. For more information, see Loading a reference genome in our quick start guide.\nWe not only want to see where our reads have mapped, but what genes they have mapped to. For this, we have an annotation file in GFF3 format. This contains a list of features, their co-ordinates and orientations which correspond to our reference genome.\nExample from PccAS_v3 GFF3\nLoad your annotation file into IGV. Go to ”“File -&gt; Load from File…”. Select “PccAS_v3.gff3” and click “Open”. For more information, see Loading gene annotations in our quick start guide.\nThis will load a new track called “PccAS_v3.gff3”. The track is currently shown as a density plot.\nYou will need to zoom in to see individual genes.\nSearch for the gene PCHAS_0505200 by typing “PCHAS_0505200” in the search box to zoom in and centre the view on PCHAS_0505200.\nIGV - PCHAS_0505200\nTo get a clearer view of the gene structure, right click on the annotation track and click “Expanded”.\nIGV - PCHAS_0505200 expanded\nIn the annotation track, genes are presented as blue boxes and lines. These boxes represent exons, while the lines represent intronic regions. Arrows indicate the direction (or strand) of transcription for each of the genes. Now we have our genome and its annotated features, we just need the read alignments for our five samples.\nLoad your alignment file for the MT1 sample into IGV. Go to ”“File -&gt; Load from File…”. Select “MT1_sorted.bam” and click “Open”. For more information, see Loading alignment files in our quick start guide.\nNote: BAM files and their corresponding index files must be in the same directory for IGV to load them properly.\nIGV - MT1 read alignment\nThis will load a new track called “MT1_sorted.bam” which contains the read alignments for the MT1 sample. We can change how we visualise our data by altering the view options. By default, IGV will display reads individually so they are compactly arranged. If you were to hover over a read in the default view, you will only get the details for that read. However, if we change our view so that the reads are visualised as pairs, the read pairs will be joined together by line and when we hover over either of the reads, we will get information about both of the reads in that pair.\nTo view our reads as pairs, right click on the MT1_sorted.bam alignment track and click “View as pairs”.\nIGV - paired view To condense the alignment, right click on the MT1_sorted.bam alignment track and click “Squished”.\nIGV - squished view\nFor more information on sorting, grouping and visualising read alignments, see the IGV user guide.\nLoad the remaining sorted BAM files for the MT2 sample and the three SBP samples.\nUsing the search box in the toolbar, go to PCHAS_1409500. For more information, see Jump to gene or locus in our quick start guide.\nIGV - search PCHAS_1409500\nThe first thing to look at is the coverage range for this viewing window on the left-hand side. The three SBP samples have 2-3 times more reads mapping to this gene than the two MT samples. While at first glance it may seem like this gene may be differentially expressed between the two conditions,\nremember that some samples may have been sequenced to a greater depth than others. So, if a\nsample has been sequenced to a greater depth we would expect more reads to map in general.\nIGV - coverage PCHAS_1409500\n134 Visualising transcriptomes with IGV 4.3 Questions\nFrom the gene annotation at the bottom we can also see that there are three annotated exon/CDS\nfeatures for this gene. However, the coverage plot suggests there may be a fourth unannotated exon\nwhich, given the direction of the gene, could suggest a 5’ untranslated region (UTR). Note the clean\ndrop off of the coveraged at around position 377,070.\n4.3 Questions\n4.3.1 Q1: How many CDS features are there in “PCHAS_1402500”?\nHint: Look at Jump to gene or locus in our quick start guide.\n4.3.2 Q2: Does the RNA-seq mapping agree with the gene model in blue?\nHint: Look at the coverage track and split read alignments.\n4.3.3 Q3: Do you think this gene is differentially expressed and is looking at the coverage\nplots alone a reliable way to assess differential expression?\nHint: Look at the coverage similarities/differences between the MT and SBP samples.\n145 Transcript quantification with Kallisto\n5 Transcript quantification with Kallisto\n5.1 Introduction\nAfter visually inspecting the genome alignment, the next step in a typical RNA-Seq analysis is to\nestimate transcript abundance. To do this, reads are assigned to the transcripts they came from.\nThese assignments are then used to quantify gene or transcript abundance (expression level).\nFor this tutorial, we are using Kallisto to assign reads to a set of transcript sequences and quantify\ntranscript abundance. Kallisto does not assemble transcripts and cannot identify novel isoforms. So,\nwhen a reference transcriptome isn’t available, the transcripts will need to be assembled de novo\nfrom the reads. However, for this tutorial, we already have a reference transcriptome available.\nThe objectives of this part of the tutorial are:\n• use Kallisto to index a transcriptome\n• use Kallisto to estimate transcript abundance\n5.1.1 Quantifying transcripts with Kallisto\nMany of the existing methods used for estimating transcript abundance are alignment-based. This\nmeans they rely on mapping reads onto the reference genome. The gene expression levels are then\ncalculated by counting the number of reads overlapping the transcripts. However, read alignment is\na computationally and time intensive process. So, in this tutorial, we will be running Kallisto which\nuses a fast, alignment-free method for transcript quantification.\nNear-optimal probabilistic RNA-seq quantification\nNicolas L Bray, Harold Pimentel, Páll Melsted and Lior Pachter\nNat Biotechnol. 2016 May;34(5):525-7. doi: 10.1038/nbt.3519\nKallisto uses pseudoalignment to make it efficient. Rather than looking at where the reads map,\nKallisto uses the compatibility between the reads and transcripts to estimate transcript abundance.\nThus, most transcript quantification with Kallisto can be done on a simple laptop (Figure 3).\nFigure 3. Performance of kallisto and other methods\nFigure 3. Performance of kallisto and other methods\n(a) Accuracy of kallisto, Cufflinks, Sailfish, EMSAR, eXpress and RSEM on 20 RSEM simulations of 30\nmillion 75-bp paired-end reads. (b) Total running time in minutes for processing the 20 simulated data\nsets of 30 million paired-end reads described in a. Please see the Kallisto publication for original figure\nand more information.\n155 Transcript quantification with Kallisto 5.2 Exercise 4\nStep 1: building a Kallisto index As with alignment-based methods, Kallisto needs an index. To\ngenerate the index, Kallisto first builds a transcriptome de Bruijn Graph (T-BDG) from all of the\nk-mers (short sequences of k nucleotides) that it finds in the transcriptome. Each node in the graph\ncorresponds to a k-mer and each transcript is represented by its path through the graph. Using these\npaths, each k-mer is assigned a k-compatibility class. Some k-mers will be redundant i.e. shared by\nthe same transcripts. These are skipped to make the index compact and quicker to search. A great\nworked example of this process can be found here.\nThe command kallisto index can be used to build a Kallisto index from transcript sequences.\n[ ]: kallisto index\nHere we can see the version of Kallisto that we’re using (useful for publication methods) and the\ninformation that we’ll need to give kallisto index. The only information we need to give kallisto\nindex is the location of our transcript sequences (PccAS_v3_transcripts.fa). However, it’s useful to\nhave a meaningful filename for the resulting index. We can add this by using the option -i which\nexpects a value, our index prefix (PccAS_v3_kallisto).\nStep 2: estimating transcript abundance With this Kallisto index, you can use kallisto quant\nto estimate transcript abundances. You will need to run this command separately for each sample.\n[ ]: kallisto quant\nWe can see that kallisto quant needs us to tell it where our sample read are. Although we don’t\nhave to, it’s usually a good idea to keep the results of each quantification in a different directory.\nThis is because the output filename are always the same (e.g. abundances.tsv). If we ran a second\nanalysis, these could get overwritten. To use a different output directory, we can use the -o option.\nWe will also be using the -b option for bootstrapping.\nBootstrapping Not all reads will be assigned unambiguously to a single transcript. This means\nthere will be “noise” in our abundance estimates where reads can be assigned to multiple transcripts.\nKallisto quantifies the uncertainty in its abundance estimates using random resampling and replace-\nment. This process is called bootstrapping and indicates how reliable the expression estimates are\nfrom the observed pseudoalignment. Bootstrap values can be used downstream to distinguish the\ntechnical variability from the biological variability in your experiment.\n5.2 Exercise 4\nBuild an index called PccAS_v3_kallisto from transcript sequences in PccAS_v3_transcripts.fa.\n[ ]: kallisto index -i data/PccAS_v3_kallisto data/PccAS_v3_transcripts.fa\n165 Transcript quantification with Kallisto 5.2 Exercise 4\nQuantify the transcript expression levels for the MT1 sample with 100 bootstrap samples and\ncalling the output directory MT1.\n[ ]: kallisto quant -i data/PccAS_v3_kallisto -o data/MT1 -b 100 \\\ndata/MT1_1.fastq.gz data/MT1_2.fastq.gz\nYou’ll find your Kallisto results in a new output directory which we called MT1. Let’s take a look.\n[ ]: ls data/MT1\nRunning kallisto quant generated three output files in our MT1 folder:\n• abundance.h5\nHDF5 binary file containing run info, abundance esimates, bootstrap estimates, and transcript\nlength information length.\n• abundance.tsv\nPlain text file containing abundance estimates (doesn’t contain bootstrap estimates).\n• run_info.json\nJSON file containing information about the run.\nNote: when the number of bootstrap values (-b) is very high, Kallisto will generate a large amount\nof data. To help, it outputs bootstrap results in HDF5 format (abundance.h5). This file can be read\ndirectly by sleuth.\nIn the MT1/abundance.tsv file we have the abundance estimates for each gene for the MT1 sample.\nLet’s take a quick look.\n[ ]: head data/MT1/abundance.tsv\nIn MT1/abundance.tsv there are five columns which give us information about the transcript abun-\ndances for our MT1 sample.\n• target_id\nUnique transcript identifier.\n• length\nNumber of bases found in exons.\n• eff_length\nEffective length. Uses fragment length distribution to determine the effective number of posi-\ntions that can be sampled on each transcript.\n• est_counts\nEstimated counts*. This may not always be an integer as reads which map to multiple tran-\nscripts are fractionally assigned to each of the corresponding transcripts.\n• tpm\nTranscripts per million. Normalised value accounting for length and sequence depth bias.\nIn the last column we have our normalised abundance value for each gene. These are our transcripts\nper million or TPM. If you have time at the end of this tutorial, see our normalisation guide which\ncovers common normalisation methods and has a bonus exercise.\n175 Transcript quantification with Kallisto 5.3 Questions\nTo get the result for a specific gene, we can use grep.\n[ ]: grep PCHAS_0100100 data/MT1/abundance.tsv\nIf we wanted to get the TPM value for a particular gene, we can use awk.\n[ ]: awk -F”\\t” ‘$1==“PCHAS_0100100” {print $5}’ data/MT1/abundance.tsv\nUse kallisto quant four more times, for the MT2 sample and the three SBP samples.\n5.3 Questions\n5.3.1 Q1: What k-mer length was used to build the Kallisto index?\nHint: look at the terminal output from kallisto index\n5.3.2 Q2: How many transcript sequences are there in PccAS_v3_transcripts.fa?\nHint: you can use grep or look at the terminal output from kallisto quant or in the run_info.json\nfiles\n5.3.3 Q3: What is the transcripts per million (TPM) value for PCHAS_1402500 in each of\nthe samples?\nHint: use grep to look at the abundance.tsv files\n5.3.4 Q4: Do you think PCHAS_1402500 is differentially expressed?\n186 Identifying differentially expressed genes with Sleuth\n6 Identifying differentially expressed genes with Sleuth\n6.1 Introduction\nIn the previous sections, we have quantified our transcript abundance and looked at why counts are\nnormalised. In this section, you will be using sleuth to do some simple quality checks and get a first\nlook at the results.\nThe objectives of this part of the tutorial are:\n• use sleuth to perform quality control checks\n• use sleuth to identify differentially expressed (DE) transcripts\n• use sleuth to investigate DE transcripts\n6.1.1 Differential expression analysis (DEA)\nDifferential expression analysis tries to identify genes whose expression levels differ between exper-\nimental conditions. We don’t normally have enough replicates to do traditional tests of significance\nfor RNA-Seq data. So, most methods look for outliers in the relationship between average abundance\nand fold change and assume most genes are not differentially expressed.\nRather than just using a fold change threshold to determine which genes are differentially expressed,\nDEAs use a variety of statistical tests for significance. These tests give us a p-value which is an\nestimate of how often your observations would occur by chance.\nHowever, we perform these comparisons for each one of the thousands of genes/transcripts in our\ndataset. A p-value of 0.01 estimates a probability of 1% for seeing our observation just by chance. In\nan experiment like ours with 5,000 genes we would expect 5 genes to be significantly differentially\nexpressed by chance (i.e. even if there were no difference between our conditions). Instead of using\na p-value we use a q-value to account for multiple testing and adjusts the p-value accordingly.\n6.1.2 sleuth\nsleuth is a companion tool for Kallisto. Unlike most other tools, sleuth can utilize the technical\nvariation information generated by Kallisto so that you can look at both the technical and biological\nvariation in your dataset.\nFor the DEA, sleuth essentially tests two models, one which assumes that the abundances are equal\nbetween the two conditions (reduced) and one that does not (full). To identify DE transcripts it\nidentifies those with a significantly better fit to the “full” model. For more information on sleuth\nand how it works, see Lior Pachter’s blog post A sleuth for RNA-Seq (https://liorpachter.word-\npress.com/2015/08/17/a-sleuth-for-rna-seq/).\nsleuth is written in the R statistical programming language, as is almost all RNA-Seq analysis soft-\nware. Helpfully, it produces a web page that allows interactive graphical analysis of the data. How-\never, we strongly recommend learning R for anyone doing a significant amount of RNA-seq analysis.\nIt is nowhere near as hard to get started with as full-blown programming languages such as Perl or\nPython!\n196 Identifying differentially expressed genes with Sleuth 6.2 Exercise 5\n6.2 Exercise 5\nFor this tutorial, we’ve provided a series of R commands as an R script that will get sleuth running.\n6.2.1 Running sleuth\nThe commands we need to run sleuth are in the file sleuth.R. There’s a great overview of the com-\nmands and what they do by the developers of sleuth here: https://pachterlab.github.io/sleuth_walk-\nthroughs/trapnell/analysis.html. Using R is not as hard as it seems, most of this script was copied\nfrom the manual!\nOpen sleuth.R and have a quick look at the commands.\n[ ]: cat data/sleuth.R\nYou may also want to have a look at hiseq_info.txt which is where we define which condition each\nsample is associated with.\n[ ]: cat data/hiseq_info.txt\nYou can run scripts containing R commands using Rscript followed by the script name. Run\nsleuth.R.\n[ ]: Rscript data/sleuth.R\nYou won’t see any output from this script in the notebook, just a * next to the command input ([*])\nto let you know it’s running.\nIf you were to run the script directly on the command line, sleuth will return a link which you can\nfollow (http://127.0.0.1:42427). This will take you to a web page where you can navigate and\nexplore the sleuth results.\nType the URL below into your a web browser (e.g. chrome or firefox) to open the sleuth results.\nhttp://127.0.0.1:42427\nYou should now see a page with the heading “sleuth live”. If not, just give the script a little longer\nand then refresh the page.\n206 Identifying differentially expressed genes with Sleuth 6.2 Exercise 5\n6.2.2 Using sleuth to quality check (QC) transcript quanification\nQuality control checks are absolutely vital at every step of the experimental process. We can use\nsleuth to perform simple quality checks (QC) on our dataset.\nAt the top of the page, sleuth provides several tabs which we can use to determine whether the data\nis of good quality and whether we should trust the results we get.\nFirst, lets take a look at a summary of our dataset.\nIn the web page that has been launched, click on “summaries -&gt; processed data”.\nNotice that the number of reads mapping differs quite a bit between MT and SBP samples? This is\nwhy we QC our data. In the MT samples &gt;95% of the reads mapped to the genome, but only 15-30%\nare assigned to the transcriptome compared to &gt;75% for the SBP samples. This suggests that there\nmay be some residual ribosomal RNA left over from the RNA preparation. It’s not a problem as we\nhave enough reads and replicates for our analysis.\nsleuth - processed data table\nIn some cases, we can identify samples which don’t agree with other replicates (outliers) and sam-\nples which are related by experimental bias (batch effects). If we don’t have many replicates, it’s\nhard to detect outliers and batch effects meaning our power to detect DE genes is reduced.\nPrincipal component analysis (PCA) plots can be used to look at variation and strong patterns\nwithin the dataset. Batch effects and outliers often stand out quite clearly in the PCA plot and mean\nthat you can account for them in any downstream analysis.\nsleuth - PCA plot\n216 Identifying differentially expressed genes with Sleuth 6.2 Exercise 5\nOur samples form two condition-related clusters with the two MT samples (red) on the left and the\nthree SBP samples on the right (blue). If we look at the variance bar plot, we can see that the first\nprincipal component (PC1) accounts for &gt;90% of the variation in our dataset. As the samples are\nclearly clustered on the x-axis (PC1) this suggests that most of the variation in the dataset is related\nto our experimental condition (Mt vs SBP).\nsleuth - variance bar plot\n6.2.3 Using sleuth to look at DE transcripts\nWe used the output from Kallisto to identify DE transcripts using sleuth. Let’s take a look and see if\nwe found any.\nTo see the results of the sleuth DEA, go to “analyses -&gt; test table”.\nsleuth - transcript table\nThe important columns here are the q-value and the beta value (analagous to fold change). By\ndefault, the table is sorted by the q-value. We can see that our top transcript is PCHAS_0420800, a\nhypothetical protein/pseudogene. Now let’s take a closer look at that transcript.\n226 Identifying differentially expressed genes with Sleuth 6.2 Exercise 5\nGo to “analyses -&gt; transcript view”. Enter “PCHAS_0420800” into the “transcript” search box.\nClick “view”.\nsleuth - transcript view\nOn the left you have the abundances for the MT replicates and on the right, the SBP replicates. We\ncan see that this transcript is more highly expressed in the MT samples than in the SBP samples.\nThis is also reflected by the fold change in the test table (b = -4.5). The b value is negative as it\nrepresents the fold change in SBP samples relative to those in the MT samples.\nFinally, let’s take a look at the gene level.\nTo see the results of the sleuth DEA, go to “analyses -&gt; test_table”. Under “table type” select\n“gene table”. Click on the column header “qval” in the table to sort the rows by ascending\nq-value.\nsleuth - gene table\nThe transcripts have now been grouped by their descriptions. Let’s take a closer look at the CIR\nproteins.\n236 Identifying differentially expressed genes with Sleuth 6.3 Questions\nGo to “analyses -&gt; gene view”. In the “gene” search box enter “CIR protein” (without the\nquotes).\nsleuth - gene view\nHere we can see the individual CIR protein transcript abundances. We can see that PCHAS_1100300\nis more highly expressed in the SBP samples while PCHAS_0302100 and PCHAS_0302100 are more\nhighly expressed in the MT samples.\n6.3 Questions\n6.3.1 Q1: Is our gene from earlier, PCHAS_1402500, significantly differentially expressed?\n247 Interpreting the results\n7 Interpreting the results\n7.1 Introduction\nThe main objective of this part of the tutorial is to use simple Unix commands to get a list of signifi-\ncantly differentially expressed genes. Using this gene list and the quantitative information from our\nanalysis we can then start to make biological inferences about our dataset.\nUsing the R script (sleuth.R), we printed out a file of results describing the differentially expressed\ngenes in our dataset. This file is called kallisto.results.\nThe file contains several columns, of which the most important are:\n• Column 1: target_id (gene id)\n• Column 2: description (some more useful description of the gene than its id)\n• Column 3: pval (p value)\n• Column 4: qval (p value corrected for multiple hypothesis testing)\n• Column 5: b (fold change)\nWith a little Linux magic we can get the list of differentially expressed genes with only the columns\nof interest as above.\n## Exercise 6\nTo get the genes which are most highly expressed in our SBP samples, we must first filter our re-\nsults. There are two columns we want to filter our data on: b (column 5) and qval (column 4).\nThese columns represent whether the gene is differentially expressed and whether that change is\nsignificant.\nThe following command will get those genes which have an adjusted p value (qval) less than 0.01\nand a positive fold change. These genes are more highly expressed in the SBP samples.\n[ ]: awk -F “\\t” ‘$4 &lt; 0.01 && $5 &gt; 0’ data/kallisto.results | cut -f1,2,3,4,5 | head\nWe used awk to filter the gene list and print only the lines which met our search criteria (qval &gt;\n0.01, b &gt; 0). The option -F tells awk what delimiter is used to separate the columns. In this case,\nit was a tab or its regular expression ””. We then use cut to only print out columns 1-5. You can also\ndo that within the awk command. Finally, we use head to get the first 10 lines of the output.\nAlternatively, we can look for the genes which are more highly expressed in the MT samples.\n[ ]: awk -F “\\t” ‘$4 &lt; 0.01 && $5 &lt; 0’ data/kallisto.results | cut -f1,2,3,4,5 | head\nIt can be useful to have a quick look and compare gene lists. For example, whether a certain gene\nproduct is seen more often in the genes most highly expressed in one condition or another. A quick\nand dirty method would be to use the gene descriptions (or gene products).\n257 Interpreting the results 7.2 Questions\nYou could extract the gene products (column 2) for genes which are more highly expressed in the\nSBP samples using sort and then uniq.\n[ ]: awk -F “\\t” ‘$4 &lt; 0.01 && $5 &lt; 0 {print $2}’ data/kallisto.results | sort | uniq\nWe can count each time these unique gene products occur in the list using uniq -c.\n[ ]: awk -F “\\t” ‘$4 &lt; 0.01 && $5 &lt; 0 {print $2}’ data/kallisto.results | \\\nsort | uniq -c\nAnd, if we wanted to make it a bit easier to see commonly found gene products we can sort this\nagain by the frequency count we got from the uniq command. The sort command will put these\nin ascending numerical (-n) order.\n[ ]: awk -F “\\t” ‘$4 &lt; 0.01 && $5 &lt; 0 {print $2}’ data/kallisto.results | \\\nsort | uniq -c | sort -n\nIf you wanted to look for the frequency of a particular gene product you could also use grep.\n[ ]: awk -F “\\t” ‘$4 &lt; 0.01 && $5 &lt; 0 {print $2}’ data/kallisto.results | grep -c CIR\nOr building on the earlier command:\n[ ]: awk -F “\\t” ‘$4 &lt; 0.01 && $5 &lt; 0 {print $2}’ data/kallisto.results | \\\nsort | uniq -c | grep CIR\nIf you want to read more about this work related to this data it is published:\nVector transmission regulates immune control of Plasmodium virulence\nPhilip J. Spence, William Jarra, Prisca Lévy, Adam J. Reid, Lia Chappell, Thibaut Brugat,\nMandy Sanders, Matthew Berriman and Jean Langhorne\nNature. 2013 Jun 13; 498(7453): 228–231 doi:10.1038/nature12231\nQ1: How many genes are more highly expressed in the SBP samples?\nHint: try replacing head in the earlier command with another unix command to count the number of\n7.2 Questions\n7.2.1 lines\n7.2.2 Q2: How many genes are more highly expressed in the MT samples?\nHint: try replacing head in the earlier command with another unix command to count the number of\nlines\n267 Interpreting the results 7.2 Questions\n7.2.3 Q3: Do you notice any particular genes that came up in the analysis?\nHint: look for gene products that are seen more often in genes more highly expressed in the SBP samples\nthan those more highly expressed in the MT samples\n278 Key aspects of differential expression analysis\n8 Key aspects of differential expression analysis\n8.1 Replicates and power\nIn order to accurately ascertain which genes are differentially expressed and by how much it is nec-\nessary to use replicated data. As with all biological experiments doing it once is simply not enough.\nThere is no simple way to decide how many replicates to do, it is usually a compromise of statistical\npower and cost. By determining how much variability there is in the sample preparation and se-\nquencing reactions, we can better assess how highly genes are really expressed and more accurately\ndetermine any differences. The key to this is performing biological rather than technical replicates.\nThis means, for instance, growing up three batches of parasites, treating them all identically, ex-\ntracting RNA from each and sequencing the three samples separately. Technical replicates, whereby\nthe same sample is sequenced three times do not account for the variability that really exists in\nbiological systems or the experimental error between batches of parasites and RNA extractions.\nNote: more replicates will help improve power for genes that are already detected at high levels, while\ndeeper sequencing will improve power to detect differential expression for genes which are expressed at\nlow levels.\n8.2 p-values vs. q-values\nWhen asking whether a gene is differentially expressed we use statistical tests to assign a p-value. If\na gene has a p-value of 0.05, we say that there is only a 5% chance that it is not really differentially\nexpressed. However, if we are asking this question for every gene in the genome (~5500 genes for\nPlasmodium), then we would expect to see p-values less than 0.05 for many genes even though they\nare not really differentially expressed. Due to this statistical problem, we must correct the p-values\nso that we are not tricked into accepting a large number of erroneous results. Q-values are p-values\nwhich have been corrected for what is known as multiple hypothesis testing. Therefore, it is a q-\nvalue of less than 0.05 that we should be looking for when asking whether a gene is differentially\nexpressed.\n8.3 Alternative software\nIf you have a good quality genome and genome annotation such as for model organisms e.g. human,\nmouse, Plasmodium; map to the transcriptome to determine transcript abundance. This is even more\nrelevant if you have variant transcripts per gene as you need a tool which will do its best to determine\nwhich transcript is really expressed. As well as Kallisto (Bray et al. 2016; PMID: 27043002), there\nis eXpress (Roberts & Pachter, 2012; PMID: 23160280) which will do this.\nAlternatively, you can map to the genome and then call abundance of genes, essentially ignoring\nvariant transcripts. This is more appropriate where you are less confident about the genome an-\nnotation and/or you don’t have variant transcripts because your organism rarely makes them or\nthey are simply not annotated. Tophat2 (Kim et al., 2013; PMID: 23618408), HISAT2 (Pertea et\nal. 2016; PMID: 27560171), STAR (Dobinet al., 2013; PMID: 23104886) and GSNAP (Wu & Nacu,\n2010; PMID: 20147302) are all splice-aware RNA-seq read mappers appropriate for this task. You\nthen need to use a tool which counts the reads overlapping each gene model. HTSeq (Anders et al.,\n288 Key aspects of differential expression analysis 8.4 What do I do with a gene list?\n2015; PMID: 25260700) is a popular tool for this purpose. Cufflinks (Trapnell et al. 2012; PMID:\n22383036) will count reads and determine differentially expressed genes.\nThere are a variety of programs for detecting differentially expressed genes from tables of RNA-seq\nread counts. DESeq2 (Love et al., 2014; PMID: 25516281), EdgeR (Robinson et al., 2010; PMID:\n19910308) and BaySeq (Hardcastle & Kelly, 2010; PMID: 20698981) are good examples.\n8.4 What do I do with a gene list?\nDifferential expression analysis results are a list of genes which show differences between two con-\nditions. It can be daunting trying to determine what the results mean. On one hand, you may find\nthat that there are no real differences in your experiment. Is this due to biological reality or noisy\ndata? On the other hand, you may find several thousands of genes are differentially expressed.\nWhat can you say about that?\nOther than looking for genes you expect to be different or unchanged, one of the first things to\ndo is look at Gene Ontology (GO) term enrichment. There are many different algorithms for\nthis, but you could annotate your genes with functional terms from GO using for instance Blast2GO\n(Conesa et al., 2005; PMID: 16081474) and then use TopGO (Alexa et al., 2005; PMID: 16606683)\nto determine whether any particular sorts of genes occur more than expected in your differentially\nexpressed genes.\n8.5 Congratulations, you have reached the end of this tutorial!\nWe hope you’ve enjoyed our RNA-Seq tutorial!\n299 Normalisation\n9 Normalisation\n9.1 Introduction\nIn the previous section, we looked at estimating transcript abundance with Kallisto. The abundances\nare reported as transcripts per million (TPM), but what does TPM mean and how is it calculated?\nThe objectives of this part of the tutorial are:\n• understand why RNA-Seq normalisation metrics are used\n• understand the difference between RPKM, FPKM and TPM\n• calculate RPKM and TPM for a gene of interest\nThere are many useful websites, publications and blog posts which go into much more detail about\nRNA-Seq normalisation methods. Here are just a couple (in no particular order):\n• What the FPKM? A review of RNA-Seq expression units\n• RPKM, FPKM and TPM, clearly explained\n• A survey of best practices for RNA-seq data analysis\n• The RNA-seq abundance zoo\n9.2 Why do we use normalisation units instead of raw counts?\nRaw reads counts are the number of reads originating from each transcript which can be affected\nby several factors:\n• sequencing depth (total number of reads)\nThe more we sequence a sample, the more reads we expect to be assigned.\n• gene/transcript length\nThe longer the gene or transcript, the more reads we expect to be assigned to it.\nFigure 4. Effect of sequencing depth and gene length on raw read counts\n309 Normalisation 9.2 Why do we use normalisation units instead of raw counts?\nLook at the top part of Figure 4. In which sample, X or Y, is the gene more highly expressed?\nNeither, it’s the same in both. What we didn’t tell you was that the total number of reads generated\nfor sample A was twice the number than for sample B. That meant almost twice the number of reads\nare assigned to the same gene in sample A than in sample B.\nLook at the bottom part of Figure 4. Which gene, X or Y, has the greatest gene level expression?\nNeither, they are both expressed at the same level. This time we didn’t tell you that gene X is twice\nthe length of gene Y. This meant that almost twice the number reads were assigned to gene X than\ngene Y.\nIn the top part of Figure 4, the gene in sample X has twice the number of reads assigned to it than the\nsame gene in sample Y. What isn’t shown is that sample X had twice the number or total reads than\nsample Y so we would expect more reads to be assigned in sample X. Thus, the gene is expressed at\nroughly the same level in both samples. In the bottom part of Figure 4, gene X has twice the number\nof reads assigned to it than gene Y. However, gene X is twice the length of gene Y and so we expect\nmore reads to be assigned to gene X. Again, the expression level is roughly the same.\n9.2.1 Reads per kilobase per million (RPKM)\nReads per kilobase (of exon) per million (reads mapped) or RPKM is a within sample normalisation\nmethod which takes into account sequencing depth and length biases.\nTo calculate RPKM, you first normalise by sequencing depth and then by gene/transcript length.\n1. Get your per million scaling factor\nCount up the total number of reads which have been assigned (mapped) in the sample. Divide\nthis number by 1,000,000 (1 million) to get your per million scaling factor (N).\n2. Normalise for sequencing depth\nDivide the number of reads which have been assigned to the gene or transcript (C) by the per\nmillion scaling factor you calculated in step 1. This will give you your reads per million (RPM).\n3. Get your per kilobase scaling factor\nDivide the total length of the exons in your transcript or gene in base pairs by 1,000 (1 thou-\nsand) to get your per kilobase scaling factor (L).\n4. Normalise for length\nDivide your RPM value from step 2 by your per kilobase scaling factor (length of the gene/-\ntranscript in kilobases) from step 3. This will give you your reads per kilobase per million or\nRPKM.\nThis can be simplified into the following equation:\nC\nRP KM=\nLN\n319 Normalisation 9.2 Why do we use normalisation units instead of raw counts?\nWhere:\n• C is number of reads mapped to the transcript or gene\n• L is the total exon length of the transcript or gene in kilobases\n• N is the total number of reads mapped in millions\n9.2.2 Fragments per kilobase per million (FPKM)\nFragments per kilobase per million or FPKM is essentially the same as RPKM except that:\n• RPKM is designed for single-end RNA-Seq experiments\n• FPKM is designed for paired-end RNA-Seq experiments\nIn a paired-end RNA-Seq experiment, two reads may be assigned to a single fragment (in any ori-\nentation). Also, in some cases, only one of those reads will be assigned to a fragment (singleton).\nThe only difference between RPKM and FPKM is that FPKM takes into consideration that two reads\nmay be assigned to the same fragment.\n9.2.3 Transcripts per million (TPM)\nCalculating the transcripts per million or TPM is a similar process to RPKM and FPKM. The main\ndifference is that you will first normalise for length bias and then for sequencing depth bias. In a\nnut shell, we are swapping the order of normalisations.\n1. Get your per kilobase scaling factor\nDivide the total length of the exons in your transcript in base pairs by 1,000 (1 thousand) to\nget your per kilobase scaling factor.\n2. Normalise for length\nDivide the number of reads which have been assigned to the transcript by the per kilobase\nscaling factor you calculated in step 1. This will give you your reads per kilobase (RPK).\n3. Get the sum of all RPK values in your sample\nCalculate the RPK value for all of the transcripts in your sample. Add all of these together to\nget your total RPK value.\n4. Get your per million scaling factor\nDivide your total RPK value from step 3 by 1,000,000 (1 million) to get your per million scaling\nfactor.\n5. Normalise for sequencing depth\nDivide your RPK value calculated in step 2 by the per million scaling factor from step 4. You\nnow have your transcripts per millions value or TPM.\n329 Normalisation 9.3 Calculating RPKM and TPM values\n9.3 Calculating RPKM and TPM values\nTo try and answer this, let’s look at a worked example. Here, we have three genes (A-C) and three\nbiological replicates (1-3).\nGene Length Replicate 1 Replicate 2 Replicate 3\nA 2,000 bases 10 12 30\nB 4,000 bases 20 25 60\nC 1,000 bases 5 8 15\nThere are two things to notice in our dataset:\n• Gene B has twice number reads mapped than gene A, possibly as it’s twice the length\n• Replicate 3 has more reads mapped than any of the other replicates, regardless of which gene\nwe look at\n9.3.1 Calculating RPKM\nStep 1: get your per million scaling factor In the table below is the total number of reads which\nmapped for each of the replicates. To get our per million scaling factor, we divide each of these\nvalues by 1,000,000 (1 million).\nGene Replicate 1 Replicate 2 Replicate 3\nTotal reads mapped 3,500,000 4,500,000 10,600,000\nPer million reads 3.5 4.5 10.6\nStep 2: normalise for sequencing depth factor to get our reads per million (RPM).\nBefore:\nWe now divide our read counts by the per million scaling\nGene Replicate 1 Replicate 2 Replicate 3\nA 10 12 30\nB 20 25 60\nC 5 8 15\n339 Normalisation 9.3 Calculating RPKM and TPM values\nAfter:\nGene Replicate 1 RPM Replicate 2 RPM Replicate 3 RPM\nA 2.857 2.667 2.830\nB 5.714 5.556 5.660\nC 1.429 1.778 1.415\nStep 3: get your per kilobase scaling factor Here we have our gene length in base pairs. For our\nper kilobase scaling factor we need to get our gene length in kilobases by dividing it by 1,000.\nGene Length (base pairs) Length (kilobases)\nA 2,000 2\nB 4,000 4\nC 1,000 1\nStep 4: normalise for length Finally, we divide our RPM values from step 2 by our per kilobase\nscaling factor from step 3 to get our reads per kilobase per million (RPKM).\nBefore:\nGene Replicate 1 RPM Replicate 2 RPM Replicate 3 RPM\nA 2.857 2.667 2.830\nB 5.714 5.556 5.660\nC 1.429 1.778 1.415\nAfter:\nGene Replicate 1 RPKM Replicate 2 RPKM Replicate 3 RPKM\nA 1.43 1.33 1.42\nB 1.43 1.39 1.42\nC 1.43 1.78 1.42\nNotice that even though replicate 3 had more reads assigned than the other samples and a greater\nsequencing depth, its RPKM is quite similar. And, that although gene B had twice the number of\n349 Normalisation 9.3 Calculating RPKM and TPM values\nreads assigned than gene A, its RPKM is the same. This is because we have normalised by both\nlength and sequencing depth.\n9.3.2 Calculating TPM\nNow we’re going to calculate the TPM values for the same example data. As a reminder, here are\nour three genes (A-C) and three biological replicates (1-3).\nGene Length Replicate 1 Replicate 2 Replicate 3\nA 2,000 bases 10 12 30\nB 4,000 bases 20 25 60\nC 1,000 bases 5 8 15\nStep 1: get your per kilobase scaling factor Again, our gene lengths are in base pairs. For our\nper kilobase scaling factor we need to get our gene length in kilobases by dividing it by 1,000.\nGene Length (base pairs) Length (kilobases)\nA 2,000 2\nB 4,000 4\nC 1,000 1\nStep 2: normalise for length Now we divide the number of reads which have been assigned to\neach gene by the per kilobase scaling factor we just calculated. This will give us our reads per\nkilobase (RPK).\nBefore:\nGene Replicate 1 Replicate 2 Replicate 3\nA 10 12 30\nB 20 25 60\nC 5 8 15\n359 Normalisation 9.3 Calculating RPKM and TPM values\nAfter:\nGene Replicate 1 Replicate 2 Replicate 3\nA 5 6 15\nB 5 6.25 15\nC 5 8 15\nStep 3: get the sum of all RPK values in your sample Next, we sum the RPK values for each of\nour replices. This will give use our total RPK value for each replicate. To make this example scalable,\nwe assume there are other genes so the total RPK is made up.\nGene Replicate 1 Replicate 2 Replicate 3\nA 5 6 15\nB 5 6.25 15\nC 5 8 15\n… … … …\nTotal RPK 150,000 202,500 450,000\nStep 4: get your per million scaling factor Here, instead of dividing our total mapped reads\nby 1,000,000 (1 million) to get our per million scaling factor, we divide our total RPK values by\n1,000,000 (1 million).\nGene Replicate 1 Replicate 2 Replicate 3\nTotal RPK 150,000 202,500 450,000\nPer million RPK 0.1500 0.2025 0.4500\n369 Normalisation 9.4 Which normalisation unit should I use?\nStep 5: normalise for sequencing depth Finally, we divide our individual RPK values from step\n2 by the per million scaling factor in step 4 to give us our TPM values.\nBefore:\nGene Replicate 1 Replicate 2 Replicate 3\nA 5 6 15\nB 5 6.25 15\nC 5 8 15\nAfter:\nGene Replicate 1 Replicate 2 Replicate 3\nA 33.33 29.63 33.33\nB 33.33 30.86 33.33\nC 33.33 39.51 33.33\n9.4 Which normalisation unit should I use?\nWell, there’s a lot of debate around this, so let’s look at our total normalised values for each replicate.\n9.4.1 RPKM\nGene Replicate 1 RPKM Replicate 2 RPKM Replicate 3 RPKM\nA 1.43 1.33 1.42\nB 1.43 1.39 1.42\nC 1.43 1.78 1.42\nTotal RPKM 4.29 4.50 4.25\n9.4.2 TPM\nGene Replicate 1 Replicate 2 Replicate 3\nA 33.33 29.63 33.33\nB 33.33 30.86 33.33\n379 Normalisation 9.5 Questions\nGene Replicate 1 Replicate 2 Replicate 3\nC 33.33 39.51 33.33\nTotal TPM 100 100 100\nNotice that that total TPM value for each of the replicates is the same. This is not true for RPKM\nand FPKM where the total values differ. With TPM, having the same total value for each replicate\nmakes it easier to compare the proportion of reads mapping to each gene across replicates (although\nyou shouldn’t really compare across experiments). With RPKM and FPKM, the differing total values\nmake it much harder to compare replicates.\n9.5 Questions\nBelow is the information for each of the five samples. You will need this information to answer the\nquestions. We have put all of commands used to get this information in the answers.\nSample Total mapped reads Transcript length Assigned reads Total RPK\nMT1 2,353,750 3,697 2,541 293,431\nMT2 2,292,271 3,709 3,392 675,190\nSBP1 2,329,235 3,699 14,605 1,719,970\nSBP2 2,187,718 3,696 17,302 1,429,540\nSBP3 2,163,979 3,699 14,646 1,561,310\nNote: values have been rounded up to integers to make calculations easier. Assigned reads are the\nest_count from Kallisto for PCHAS_1402500. Transcript lengths are the est_length from Kallisto for\nPCHAS_1402500.\n389 Normalisation 9.5 Questions\nQ1: Using the abundance.tsv files generated by Kallisto and the information above, calculate\nthe RPKM for PCHAS_1402500 in each of our five samples.\nSample Per million scaling factor RPM Per kilobase scaling factor RPKM\nMT1\nMT2\nSBP1\nSBP2\nSBP3\nQ2: Using the abundance.tsv files generated by Kallisto and the information above, calculate\nthe TPM for PCHAS_1402500 in each of our five samples.\nHint: don’t forget to get your per million scaling factor.\nSample Per kilobase scaling factor Reads per kilobase (RPK) TPM\nMT1\nMT2\nSBP1\nSBP2\nSBP3\nQ3: Do these match the TPM values from Kallisto?\nHint: look at the abundance.tsv files for each of your samples.\nQ4: Do you think PCHAS_1402500 is differentially expressed between the MT and SBP sam-\nples?\n3910 Running commands on multiple samples\n10 Running commands on multiple samples\nNow, fair warning, you’re going to wish we’d told you this earlier on. However, then you wouldn’t\nhave had the fun of running and updating each of the previous commands, growling at typos and\ngenerally wishing that you’d gone for that cup of coffee before starting this tutorial.\nHere we go….we can use a loop to run the same commands for multiple samples.\nThere’s a great introduction to bash scripting and loops as part of our Unix module. But let’s take a\nlook at how we could have generated genome alignments for all of our samples using a single loop.\nWhenever you write a loop, it’s always a good idea to build it up slowly to check that it’s doing what\nyou think.\n[ ]: for r in data/*.fastq.gz\ndo\necho $r\ndone\nThis loop looks for all (*) files which end with “.fastq.gz”. The for loop then executes a sequence\nof commands for each file name that it finds. In the first iteration its “data/MT1_1.fastq.gz”, then\n“data/MT1_2.fastq.gz” and so on… In each iteration, we assigned each filename that it found to a\nvariable called “r”.\nfor r in *.fastq.gz\nThen, to check we got what we expected, we printed what the variable “r” represented back to the\nterminal. Because we want to use the variable (“r”) we created we need to use dollar ($) symbol.\necho $r\nNow, if we left things as they are, we would be running the commands twice for each sample. This\nis because we have two FASTQ files for each sample i.e. ”_1.fastq.gz” and ”_2.fastq.gz“. Let’s change\nour loop so that we only get the”_1.fastq.gz” files.\n[ ]: for r1 in data/*_1.fastq.gz\ndo\necho $r1\ndone\nGreat! Now, the only problem here is that we’re going to want to use both the ”_1.fastq.gz” and the\n”_2.fastq.gz” files in our mapping. We can get around this by removing the “data/” directory and\n”_1.fastq.gz” suffix from the filename to give us our sample name.\nsample=$(basename $r1) sample=${sample/_1.fastq.gz/}\nThis will get the base filename (e.g. “MT1_1.fastq.gz”) and replace the ”_1.fastq.gz” at the end of\nthe filename we stored as “r1” with nothing.\nWe’ve added a little descriptive message so that when we run our loop we know which iteration it’s\non and what it’s doing. Let’s try adding our HISAT2 mapping command.\n4010 Running commands on multiple samples\nNote: we assume that the HISAT2 index has already been generated as that’s a command you’ll only\nneed to run once.\n[ ]: for r1 in data/*_1.fastq.gz\ndo\nsample=$(basename $r1)\nsample=${sample/_1.fastq.gz/}\necho “Processing sample:”$sample\necho “Mapping sample:”$sample\nhisat2 –max-intronlen 10000 -x data/PccAS_v3_hisat2.idx \\\n-1 “data/${sample}_1.fastq.gz” -2 “data/${sample}_2.fastq.gz” -S “data/\n→${sample}.sam”\ndone\nNotice that because we’re using a variable as part of the filename, we need to write the filename in\ndouble quotes.\ndata/${sample}_1.fastq.gz\nNow let’s add in our samtools commands.\n[ ]: for r1 in data/*_1.fastq.gz\ndo\nsample=$(basename $r1)\nsample=${sample/_1.fastq.gz/}\necho “Processing sample:”$sample\necho “Mapping sample:”$sample\nhisat2 –max-intronlen 10000 -x data/PccAS_v3_hisat2.idx \\\n-1 “data/${sample}_1.fastq.gz” -2 “data/${sample}_2.fastq.gz” -S “data/\n→${sample}.sam”\necho “Converting SAM to BAM:”$sample\nsamtools view -b -o “data/${sample}.bam” “data/${sample}.sam”\necho “Sorting BAM:”$sample\nsamtools sort -o “data/${sample}_sorted.bam” “data/${sample}.bam”\necho “Indexing BAM:”$sample\nsamtools index “data/${sample}_sorted.bam”\ndone\n4110 Running commands on multiple samples 10.1 Taking a closer look at the SBP genome mapping bash script\nFinally, we don’t really want to keep intermediate SAM and unsorted BAM files if we don’t have to.\nThey just take up precious space. So, let’s make our samtools command a one-liner, passing the\nstdout from one command to another.\n[ ]: for r1 in data/*_1.fastq.gz\ndo\nsample=$(basename $r1)\nsample=${sample/_1.fastq.gz/}\necho “Processing sample:”$sample\nhisat2 –max-intronlen 10000 -x data/PccAS_v3_hisat2.idx \\\n-1 “data/${sample}_1.fastq.gz” -2 “data/${sample}_2.fastq.gz” \\\n| samtools view -b - \\\n| samtools sort -o “data/${sample}_sorted.bam” - \\\n&& samtools index “data/${sample}_sorted.bam”\ndone\nYou could also have used this approach for transcript quantification with Kallisto, assuming you had\nalready generated the Kallisto index.\n[ ]: for r1 in data/*_1.fastq.gz\ndo\nsample=$(basename $r1)\nsample=${sample/_1.fastq.gz/}\necho “Quantifying transcripts for sample:”$sample\nkallisto quant -i data/PccAS_v3_kallisto -o “data/${sample}” -b 100 \\\n“data/${sample}_1.fastq.gz” “data/${sample}_2.fastq.gz”\ndone\n10.1 Taking a closer look at the SBP genome mapping bash script\nIn the genome mapping section of this tutorial, we mentioned that the sorted genome alignments\nhad been provided for the three SBP samples and that to generate them, we had run a bash script.\nTo take a look at the script you can run:\nless data/map_SBP_samples.sh\nThe script contains commands to run the mapping, converting, sorting and indexing for all of the\nSBP samples. There’s a great introduction to bash scripting and loops in your Unix module.\nFirst, the bash script looks for all files in the data directory which start with “SBP” and end with\n”_1.fastq.gz”. This is so that we get one filename per sample.\ndata/SBP*_1.fastq.gz\n4210 Running commands on multiple samples 10.1 Taking a closer look at the SBP genome mapping bash script\nTo run the commands for each of our SBP samples: SBP1, SBP2 and SBP3, the script uses a for\nloop. Often, scripts like these can take a while to run and it can be difficult to track what’s going on\nif there is limited or indistinguisable output. Here, we are printing the file path that gets returned\nby our search.\nfor r1 in data/SBP*_1.fastq.gz\ndo\necho $r1\ndone\nThis will print out:\nSBP1_1.fastq.gz\nSBP2_1.fastq.gz\nSBP3_1.fastq.gz\nNext, the script removes parts of the filename to get the name of the sample it belongs to. It does this\nbecuase both FASTQ files (r1 and r2) are required to align each sample. There are many different\nways to do this. This is one example:\nfor r1 in data/SBP*_1.fastq.gz\ndo\necho $r1\nsample=$(basename $r1)\nsample=${sample/_1.fastq.gz/}\necho “Processing sample:”$sample\ndone\nWhich will print out:\nProcessing sample: SBP1\nProcessing sample: SBP2\nProcessing sample: SBP3\nFinally, the script runs the single command we were using above for the sample:\nhisat2 –max-intronlen 10000 -x data/PccAS_v3_hisat2.idx \\\n-1 “data/${sample}_1.fastq.gz” -2 “data/${sample}_2.fastq.gz” \\\n| samtools view -b - \\\n| samtools sort -o “data/${sample}_sorted.bam” - \\\n&& samtools index “data/${sample}_sorted.bam”\nNote, when it extracted the sample name in the commands above, it stored it as a variable $sample.\nIt can then use the $sample variable to create a dynamic command which will run for any of the\nsamples."
  },
  {
    "objectID": "course_modules/Module7/module7.html",
    "href": "course_modules/Module7/module7.html",
    "title": "Overview",
    "section": "",
    "text": "Module Title\nCHiP-Seq\n\n\nDuration\n3 hours\n\n\nKey topics\n\n\nActivities\n\nLecture: 1 hour\nPractical exercises: 1.5 hours\nAssessment quiz: 0.5 hours\n\n\n\nManual\nModule manual\nExercises\nSolutions\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual. Instructions on how to that will be added here.)\n\n\nLecture recording, notes and scripts\n\n\nPractical exercises\n\nVirtual Machine\nCommand line walk through.\n\n\n\nDatasets\n\n\nAssessment quiz\nQuestions\nSolution"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html",
    "title": "File conversion",
    "section": "",
    "text": "In this section we are going to look at how to convert from one file format to another. There are many tools available for converting between file formats, and we will use some of the most common ones: samtools, bcftools and Picard."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#sam-to-bam",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#sam-to-bam",
    "title": "File conversion",
    "section": "SAM to BAM",
    "text": "SAM to BAM\nTo convert from SAM to BAM format we are going to use the samtools view command. In this instance, we would like to include the SAM header, so we use the -h option:\n\nsamtools view -h data/NA20538.bam &gt; data/NA20538.sam\n\nNow, have a look at the first ten lines of the SAM file. They should look like they did in the previous section when you viewed the BAM file header.\n\nhead data/NA20538.sam\n\nWell that was easy! And converting SAM to BAM is just as straightforward. This time there is no need for the -h option, however we have to tell samtools that we want the output in BAM format. We do so by adding the -b option:\n\nsamtools view -b data/NA20538.sam &gt; data/NA20538_2.bam\n\nSamtools is very well documented, so for more usage options and functions, have a look at the samtools manual http://www.htslib.org/doc/samtools-1.0.html."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#bam-to-cram",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#bam-to-cram",
    "title": "File conversion",
    "section": "BAM to CRAM",
    "text": "BAM to CRAM\nThe samtools view command can be used to convert a BAM file to CRAM format. In the data directory there is a BAM file called yeast.bam that was created from S. cerevisiae Illumina sequencing data. There is also a reference genome in the directory, called Saccharomyces_cerevisiae.EF4.68.dna.toplevel.fa. For the conversion, an index file (.fai) for the reference must be created. This can be done using samtools faidx. However, as we will see, samtools will generate this file on the fly when we specify a reference file using the -F option.\nTo convert to CRAM, we use the -C option to tell samtools we want the output as CRAM, and the -T option to specify what reference file to use for the conversion. We also use the -o option to specify the name of the output file. Give this a try:\n\nsamtools view -C -T data/Saccharomyces_cerevisiae.EF4.68.dna.toplevel.fa -o data/yeast.cram data/yeast.bam\n\nHave a look at what files were created:\n\nls -l data\n\nAs you can see, this has created an index file for the reference genome called Saccharomyces_cerevisiae.EF4.68.dna.toplevel.fa.fai and the CRAM file yeast.cram.\n\nExercises\nQ1: Since CRAM files use reference-based compression, we expect the CRAM file to be smaller than the BAM file. What is the size of the CRAM file?\nQ2: Is your CRAM file smaller than the original BAM file?\nTo convert CRAM back to BAM, simply change -C to -b and change places for the input and output CRAM/BAM:\nsamtools view -b -T data/Saccharomyces_cerevisiae.EF4.68.dna.toplevel.fa -o data/yeast.bam data/yeast.cram"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#fastq-to-sam",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#fastq-to-sam",
    "title": "File conversion",
    "section": "FASTQ to SAM",
    "text": "FASTQ to SAM\nSAM format is mainly used to store alignment data, however in some cases we may want to store the unaligned data in SAM format and for this we can use the picard tools FastqToSam application. Picard tools is a Java application that comes with a number of useful options for manipulating high-throughput sequencing data. .\nTo convert the FASTQ files of lane 13681_1#18 to unaligned SAM format, run:\n\npicard FastqToSam F1=data/13681_1#18_1.fastq.gz F2=data/13681_1#18_2.fastq.gz O=data/13681_1#18.sam SM=13681_1#18\n\nFrom here you can go on and convert the SAM file to BAM and CRAM, as described previously. There are also multiple options for specifying what metadata to include in the SAM header. To see all available options, run:\n\npicard FastqToSam -h"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#cram-to-fastq",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#cram-to-fastq",
    "title": "File conversion",
    "section": "CRAM to FASTQ",
    "text": "CRAM to FASTQ\nIt is possible to convert CRAM to FASTQ directly using the samtools fastq command. However, for many applications we need the fastq files to be ordered so that the order of the reads in the first file match the order of the reads in the mate file. For this reason, we first use samtools collate to produce a collated BAM file.\n\nsamtools collate data/yeast.cram data/yeast.collated\n\nThe newly produced BAM file will be called yeast.collated.bam. Let’s use this to create two FASTQ files, one for the forward reads and one for the reverse reads:\n\nsamtools fastq -1 data/yeast.collated_1.fastq -2 data/yeast.collated_2.fastq data/yeast.collated.bam\n\nFor further information and usage options, have a look at the samtools manual page (http://www.htslib.org/doc/samtools.html)."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#vcf-to-bcf",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion.html#vcf-to-bcf",
    "title": "File conversion",
    "section": "VCF to BCF",
    "text": "VCF to BCF\nIn a similar way that samtools view can be used to convert between SAM, BAM and CRAM, bcftools view can be used to convert between VCF and BCF. To convert the file called 1kg.bcf to a compressed VCF file called 1kg.vcf.gz, run:\n\nbcftools view -O z -o data/1kg.vcf.gz data/1kg.bcf\n\nThe -O option allows us to specify in what format we want the output, compressed BCF (b), uncompressed BCF (u), compressed VCF (z) or uncompressed VCF (v). With the -o option we can select the name of the output file.\nHave a look at what files were generated (the options -lrt will list the files in reverse chronological order):\n\nls -lrt data\n\nThis also generated an index file, 1kg.bcf.csi.\nTo convert a VCF file to BCF, we can run a similar command. If we want to keep the original BCF, we need to give the new one a different name so that the old one is not overwritten:\n\nbcftools view -O b -o data/1kg_2.bcf data/1kg.vcf.gz\n\nCongratulations you have reached the end of the Data formats and QC tutorial!"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html",
    "title": "NGS Data formats and QC",
    "section": "",
    "text": "There are several file formats for storing Next Generation Sequencing (NGS) data. In this tutorial we will look at some of the most common formats for storing NGS reads and variant data. We will cover the following formats:\nFASTQ - This format stores unaligned read sequences with base qualities\nSAM/BAM - This format stores unaligned or aligned reads (text and binary formats)\nCRAM - This format is similar to BAM but has better compression than BAM\nVCF/BCF - Flexible variant call format for storing SNPs, indels, structural variations (text and binary formats)\nFollowing this, we will work through some examples of converting between the different formats.\nFurther to understanding the different file formats, it is important to remember that all sequencing platforms have technical limitations that can introduce biases in your sequencing data. Because of this it is very important to check the quality of the data before starting any analysis, whether you are planning to use something you have sequenced yourself or publicly available data. In the latter part of this tutorial we will describe how to perform a QC assessment for your NGS data."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#introduction",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#introduction",
    "title": "NGS Data formats and QC",
    "section": "",
    "text": "There are several file formats for storing Next Generation Sequencing (NGS) data. In this tutorial we will look at some of the most common formats for storing NGS reads and variant data. We will cover the following formats:\nFASTQ - This format stores unaligned read sequences with base qualities\nSAM/BAM - This format stores unaligned or aligned reads (text and binary formats)\nCRAM - This format is similar to BAM but has better compression than BAM\nVCF/BCF - Flexible variant call format for storing SNPs, indels, structural variations (text and binary formats)\nFollowing this, we will work through some examples of converting between the different formats.\nFurther to understanding the different file formats, it is important to remember that all sequencing platforms have technical limitations that can introduce biases in your sequencing data. Because of this it is very important to check the quality of the data before starting any analysis, whether you are planning to use something you have sequenced yourself or publicly available data. In the latter part of this tutorial we will describe how to perform a QC assessment for your NGS data."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#learning-outcomes",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#learning-outcomes",
    "title": "NGS Data formats and QC",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nOn completion of the tutorial, you can expect to be able to:\n\nDescribe the different NGS data formats available (FASTQ, SAM/BAM, CRAM, VCF/BCF)\nPerform a QC assessment of high throughput sequence data\nPerform conversions between the different data formats"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#tutorial-sections",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#tutorial-sections",
    "title": "NGS Data formats and QC",
    "section": "Tutorial sections",
    "text": "Tutorial sections\nThis tutorial comprises the following sections:\n1. Data formats\n2. QC assessment\nIf you have time you can also complete:\n\nFile conversion"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#authors",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#authors",
    "title": "NGS Data formats and QC",
    "section": "Authors",
    "text": "Authors\nThis tutorial was written by Jacqui Keane and Sara Sjunnebo based on material from Petr Danecek and Thomas Keane."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#running-the-commands-from-this-tutorial",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#running-the-commands-from-this-tutorial",
    "title": "NGS Data formats and QC",
    "section": "Running the commands from this tutorial",
    "text": "Running the commands from this tutorial\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal on your computer and type the command below:\n\ncd ~/course_data/data_formats/\n\nNow you can follow the instructions in the tutorial from here."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#lets-get-started",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/index.html#lets-get-started",
    "title": "NGS Data formats and QC",
    "section": "Let’s get started!",
    "text": "Let’s get started!\nThis tutorial assumes that you have samtools, bcftools and Picard tools installed on your computer. These are already installed on the VM you are using. To check that these are installed, you can run the following commands:\n\nsamtools --help\n\n\nbcftools --help\n\n\npicard -h\n\nThis should return the help message for samtools, bcftools and picard tools respectively.\nTo get started with the tutorial, go to the first section: Data formats"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html",
    "title": "QC assessment of NGS data",
    "section": "",
    "text": "QC is an important part of any analysis. In this section we are going to look at some of the metrics and graphs that can be used to assess the QC of NGS data."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#base-quality",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#base-quality",
    "title": "QC assessment of NGS data",
    "section": "Base quality",
    "text": "Base quality\nIllumina sequencing technology relies on sequencing by synthesis. One of the most common problems with this is dephasing. For each sequencing cycle, there is a possibility that the replication machinery slips and either incorporates more than one nucleotide or perhaps misses to incorporate one at all. The more cycles that are run (i.e. the longer the read length gets), the greater the accumulation of these types of errors gets. This leads to a heterogeneous population in the cluster, and a decreased signal purity, which in turn reduces the precision of the base calling. The figure below shows an example of this.\n\n\n\nMean Base Quality\n\n\nBecause of dephasing, it is possible to have high-quality data at the beginning of the read but really low-quality data towards the end of the read. In those cases you can decide to trim off the low-quality reads, for example using a tool called Trimmomatic.\nThe figures below shows an example of a good sequencing run (left) and a poor sequencing run (right).\n\n\n\nBase quality"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#other-base-calling-errors",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#other-base-calling-errors",
    "title": "QC assessment of NGS data",
    "section": "Other base calling errors",
    "text": "Other base calling errors\nThere are several different reasons for a base to be called incorrectly, as shown in the figure below. Phasing noise and signal decay is a result of the dephasing issue described above. During library preparation, mixed clusters can occur if multiple templates get co-located. These clusters should be removed from the downstream analysis. Boundary effects occur due to optical effects when the intensity is uneven across each tile, resulting in higher intensity found toward the center. Cross-talk occurs because the emission frequency spectra for each of the four base dyes partly overlap, creating uncertainty. Finally, for previous sequencing cycle methods T fluorophore accumulation was an issue, where incomplete removal of the dye coupled to thymine lead to an ambient accumulation the nucleotides, causing a false high Thymine trend.\n\n\n\nBase Calling Errors\n\n\nBase-calling for next-generation sequencing platforms, doi: 10.1093/bib/bbq077"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#mismatches-per-cycle",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#mismatches-per-cycle",
    "title": "QC assessment of NGS data",
    "section": "Mismatches per cycle",
    "text": "Mismatches per cycle\nAligning reads to a high-quality reference genome can provide insight to the quality of a sequencing run by showing you the mismatches to the reference sequence. This can help you detect cycle-specific errors. Mismatches can occur due to two main causes, sequencing errors and differences between your sample and the reference genome, which is important to bear in mind when interpreting mismatch graphs. The figure below shows an example of a good run (left) and a bad one (right). In the graph on the left, the distribution of the number of mismatches is even between the cycles, which is what we would expect from a good run. However, in the graph on the right, two cycles stand out with a lot of mismatches compared to the other cycles.\n\n\n\nMismatches per cycle"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#gc-content",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#gc-content",
    "title": "QC assessment of NGS data",
    "section": "GC content",
    "text": "GC content\nIt is a good idea to compare the GC content of the reads against the expected distribution in a reference sequence. The GC content varies between species, so a shift in GC content like the one seen below could be an indication of sample contamination. In the left graph below, we can see that the GC content of the sample is about the same as for the reference, at ~38%. However, in the right graph, the GC content of the sample is closer to 55%, indicating that there is an issue with this sample.\n\n\n\nGC Content"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#gc-content-by-cycle",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#gc-content-by-cycle",
    "title": "QC assessment of NGS data",
    "section": "GC content by cycle",
    "text": "GC content by cycle\nLooking at the GC content per cycle can help detect if the adapter sequence was trimmed. For a random library, it is expected to be little to no difference between the different bases of a sequence run, so the lines in this plot should be parallel with each other like in the graph on the left below. In the graph on the right, the initial spikes are likely due to adapter sequences that have not been removed.\n\n\n\nGC content by cycle"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#fragment-size",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#fragment-size",
    "title": "QC assessment of NGS data",
    "section": "Fragment size",
    "text": "Fragment size\nFor paired-end sequencing the size of DNA fragments also matters. In the first of the examples below, the fragment size peaks around 440 bp. In the second however, there is also a peak at around 200 bp. This indicates that there was an issue with the fragment size selection during library prep.\n\n\n\nFragment size distribution\n\n\n\nExercises\nQ1: The figure below is from a 100bp paired-end sequencing. Can you spot any problems?\n\n\n\nQ1 Insert size distribution"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#insertionsdeletions-per-cycle",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#insertionsdeletions-per-cycle",
    "title": "QC assessment of NGS data",
    "section": "Insertions/Deletions per cycle",
    "text": "Insertions/Deletions per cycle\nSometimes, air bubbles occur in the flow cell, which can manifest as false indels. The spike in the right image provides an example of how this can look.\n\n\n\nIndels per cycle"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#generating-qc-stats",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment.html#generating-qc-stats",
    "title": "QC assessment of NGS data",
    "section": "Generating QC stats",
    "text": "Generating QC stats\nNow let’s try this out! We will generate QC stats for two lanes of Illumina paired-end sequencing data from yeast. The reads have already been aligned to the Saccromyces cerevisiae reference genome to produce the BAM file lane1.sorted.bam.\nNow we will use samtools stats to generate the stats for the primary alignments. The option -f can be used to filter reads with specific tags, while -F can be used to filter out reads with specific tags. The following command will include only primary alignments:\n\nsamtools stats -F SECONDARY data/lane1.sorted.bam &gt; data/lane1.sorted.bam.bchk\n\nHave a look at the first 47 lines of the statistics file that was generated:\n\nhead -n 47 data/lane1.sorted.bam.bchk\n\nThis file contains a number of useful stats that we can use to get a better picture of our data, and it can even be plotted with plot-bamstats, as you will see soon. First let’s have a closer look at some of the different stats. Each part of the file starts with a # followed by a description of the section and how to extract it from the file. Let’s have a look at all the sections in the file:\n\ngrep ^'#' data/lane1.sorted.bam.bchk | grep 'Use'\n\n\nSummary Numbers (SN)\nThis initial section contains a summary of the alignment and includes some general statistics. In particular, you can see how many bases mapped, and how much of the genome that was covered.\n\n\nExercises\nNow look at the output and try to answer the questions below.\nQ2: What is the total number of reads?\nQ3: What proportion of the reads were mapped?\nQ4: How many pairs were mapped to a different chromosome?\nQ5: What is the insert size mean and standard deviation?\nQ6: How many reads were paired properly?\n\n\nGenerating QC plots\nFinally, we will create some QC plots from the output of the stats command using the command plot-bamstats which is included in the samtools package:\n\nplot-bamstats -p data/lane1-plots/ data/lane1.sorted.bam.bchk\n\nNow in your web browser open the file lane1-plots/index.html to view the QC information.\nQ7: How many reads have zero mapping quality?\nQ8: Which read (forward/reverse) of the first fragments and second fragments are higher base quality on average?\nNow continue to the next section of the tutorial: File conversion."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html",
    "title": "Data formats for NGS data",
    "section": "",
    "text": "Here we will take a closer look at some of the most common NGS data formats. First, check you are in the correct directory.\npwd\nIt should display something like:\n/home/manager/course_data/data_formats/"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#fasta",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#fasta",
    "title": "Data formats for NGS data",
    "section": "FASTA",
    "text": "FASTA\nThe FASTA format is used to store both nucleotide data and protein sequences. Each sequence in a FASTA file is represented by two parts, a header line and the actual sequence. The header always starts with the symbol “&gt;” and is followed by information about the sequence, such as a unique identifier. The following lines show two sequences represented in FASTA format:\n&gt;Sequence_1\nCTTGACGACTTGAAAAATGACGAAATCACTAAAAAACGTGAAAAATGAGAAATG\nAAAATGACGAAATCACTAAAAAACGTGACGACTTGAAAAATGACCAC\n&gt;Sequence_2\nCTTGAGACGAAATCACTAAAAAACGTGACGACTTGAAGTGAAAAATGAGAAATG\nAAATCATGACGACTTGAAGTGAAAAAGTGAAAAATGAGAAATGAACGTGACGAC\nAAAATGACGAAATCATGACGACTTGAAGTGAAAAATAAATGACC\n\nExercises\nQ1: How many sequences are there in the fasta file data/example.fasta? (Hint: is there a grep option you can use?)"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#fastq",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#fastq",
    "title": "Data formats for NGS data",
    "section": "FASTQ",
    "text": "FASTQ\nFASTQ is a data format for sequencing reads. It is an extension to the FASTA file format, and includes a quality score for each base. Have a look at the example below, containing two reads:\n@ERR007731.739 IL16_2979:6:1:9:1684/1\nCTTGACGACTTGAAAAATGACGAAATCACTAAAAAACGTGAAAAATGAGAAATG\n+\nBBCBCBBBBBBBABBABBBBBBBABBBBBBBBBBBBBBABAAAABBBBB=@&gt;B\n@ERR007731.740 IL16_2979:6:1:9:1419/1\nAAAAAAAAAGATGTCATCAGCACATCAGAAAAGAAGGCAACTTTAAAACTTTTC\n+\nBBABBBABABAABABABBABBBAAA&gt;@B@BBAA@4AAA&gt;.&gt;BAA@779:AAA@A\nWe can see that for each read we get four lines:\n\nThe read metadata, such as the read ID. Starts with @ and, for paired-end Illumina reads, is terminated with /1 or /2 to show that the read is the member of a pair.\nThe read\nStarts with + and optionally contains the ID again\nThe per base Phred quality score\n\nThe quality scores range (in theory) from 1 to 94 and are encoded as ASCII characters. The first 32 ASCII codes are reserved for control characters which are not printable, and the 33rd is reserved for space. Neither of these can be used in the quality string, so we need to subtract 33 from whatever the value of the quality character is. For example, the ASCII code of “A” is 65, so the corresponding quality is:\nQ = 65 - 33 = 32\nThe Phred quality score Q relates to the base-calling error probability P as\n       P = 10-Q/10\nThe Phred quality score is a measure of the quality of base calls. For example, a base assigned with a Phred quality score of 30 tells us that there is a 1 in 1000 chance that this base was called incorrectly.\n\n\n\n\n\n\n\n\nPhred Quality Score\nProbability of incorrect base call\nBase call accuracy\n\n\n\n\n10\n1 in 10\n90%\n\n\n20\n1 in 100\n99%\n\n\n30\n1 in 1000\n99.9%\n\n\n40\n1 in 10,000\n99.99%\n\n\n50\n1 in 100,000\n99.999%\n\n\n60\n1 in 1,000,000\n99.9999%\n\n\n\n\nExercises\nQ2: How many reads are there in the file example.fastq? (Hint: remember that @ is a possible quality score. Is there something else in the header that is unique?)"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#sam",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#sam",
    "title": "Data formats for NGS data",
    "section": "SAM",
    "text": "SAM\nSAM (Sequence Alignment/Map) format is a unified format for storing read alignments to a reference genome. It is a standard format for storing NGS sequencing reads, base qualities, associated meta-data and alignments of the data to a reference genome. If no reference genome is available, the data can also be stored unaligned.\nThe files consist of a header section (optional) and an alignment section. The alignment section contains one record (a single DNA fragment alignment) per line describing the alignment between fragment and reference. Each record has 11 fixed columns and optional key:type:value tuples. Open the SAM/BAM file specification document https://samtools.github.io/hts-specs/SAMv1.pdf either in a web browser or you can find a copy in the QC directory as you may need to refer to it throughout this tutorial.\nNow let us have a closer look at the different parts of the SAM/BAM format.\n\nHeader Section\nThe header section of a SAM file looks like:\n@HD VN:1.0  SO:coordinate\n@SQ SN:test_ref LN:17637\n@RG ID:ERR003612 PL:ILLUMINA LB:g1k-sc-NA20538-TOS-1 PI:2000 DS:SRP000540 SM:NA20538 CN:SC\nEach line in the SAM header begins with an @, followed by a two-letter header record type code as defined in the SAM/BAM format specification document. Each record type can contain meta-data captured as a series of key-value pairs in the format of ‘TAG:VALUE’.\n\nRead groups\nOne useful record type is RG which can be used to describe each lane of sequencing. The RG code can be used to capture extra meta-data for the sequencing lane. Some common RG TAGs are:\n\nID: SRR/ERR number\nPL: Sequencing platform\nPU: Run name\nLB: Library name\nPI: Insert fragment size\nSM: Individual/Sample\nCN: Sequencing centre\n\nWhile most of these are self explanitory, insert fragment size may occasionally be negative. This simply indicates that the reads found are overlapping while its size is less than 2 x read length.\n\n\n\nExercises\nLook at the following line from the header of a SAM file and answering the questions that follow:\n@RG ID:ERR003612 PL:ILLUMINA LB:g1k-sc-NA20538-TOS-1 PI:2000 DS:SRP000540 SM:NA20538 CN:SC\nQ3: What does RG stand for?\nQ4: What is the sequencing platform?\nQ5: What is the sequencing centre?\nQ6: What is the lane identifier?\nQ7: What is the expected fragment insert size?\n\n\nAlignment Section\nThe alignment section of SAM files contains one line per read alignment, an example is\nERR005816.1408831 163 Chr1    19999970    23  40M5D30M2I28M   =   20000147    213 GGTGGGTGGATCACCTGAGATCGGGAGTTTGAGACTAGGTGG...    &lt;=@A@??@=@A@A&gt;@BAA@ABA:&gt;@&lt;&gt;=BBB9@@2B3&lt;=@A@...\nEach of the lines are composed of multiple columns listed below. The first 11 columns are mandatory.\n\nQNAME: Query NAME of the read or the read pair i.e. DNA sequence\nFLAG: Bitwise FLAG (pairing, strand, mate strand, etc.)\nRNAME: Reference sequence NAME\nPOS: 1-Based leftmost POSition of clipped alignment\nMAPQ: MAPping Quality (Phred-scaled)\nCIGAR: Extended CIGAR string (operations: MIDNSHPX=)\nMRNM: Mate Reference NaMe (’=’ if same as RNAME)\nMPOS: 1-Based leftmost Mate POSition\nISIZE: Inferred Insert SIZE\nSEQ: Query SEQuence on the same strand as the reference\nQUAL: Query QUALity (ASCII-33=Phred base quality)\nOTHER: Optional fields\n\nThe image below provides a visual guide to some of the columns of the SAM format.\n\n\n\nSAM format\n\n\n\n\nExercises\nLet’s have a look at example.sam. Notice that we can use the standard UNIX operations like cat on this file.\n\ncat data/example.sam\n\nQ8: What is the mapping quality of ERR003762.5016205? (Hint: can you use grep and awk to find this?)\nQ9: What is the CIGAR string for ERR003814.6979522? (Hint: we will go through the meaning of CIGAR strings in the next section)\nQ10: What is the inferred insert size of ERR003814.1408899?\n\n\nCIGAR string\nColumn 6 of the alignment is the CIGAR string for that alignment. The CIGAR string provides a compact representation of sequence alignment. Have a look at the table below. It contains the meaning of all different symbols of a CIGAR string:\n\n\n\nSymbol\nMeaning\n\n\n\n\nM\nalignment match or mismatch\n\n\n=\nsequence match\n\n\nX\nsequence mismatch\n\n\nI\ninsertion into the read (sample sequenced)\n\n\nD\ndeletion from the read (sample sequenced)\n\n\nS\nsoft clipping (clipped sequences present in SEQ)\n\n\nH\nhard clipping (clipped sequences NOT present in SEQ)\n\n\nN\nskipped region from the reference\n\n\nP\npadding (silent deletion from padded reference)\n\n\n\nBelow are two examples describing the CIGAR string in more detail.\nExample 1:\nRef:     ACGTACGTACGTACGT\nRead:  ACGT- - - - ACGTACGA\nCigar: 4M 4D 8M\nThe first four bases in the read are the same as in the reference, so we can represent these as 4M in the CIGAR string. Next comes 4 deletions, represented by 4D, followed by 7 alignment matches and one alignment mismatch, represented by 8M. Note that the mismatch at position 16 is included in 8M. This is because it still aligns to the reference.\nExample 2:\nRef:     ACTCAGTG- - GT\nRead:  ACGCA- TGCAGTtagacgt\nCigar: 5M 1D 2M 2I 2M 7S\nHere we start off with 5 alignment matches and mismatches, followed by one deletion. Then we have two more alignment matches, two insertions and two more matches. At the end, we have seven soft clippings, 7S. These are clipped sequences that are present in the SEQ (Query SEQuence on the same strand as the reference).\n\n\nExercises\nQ11: What does the CIGAR from Q9 mean?\nQ12: How would you represent the following alignment with a CIGAR string?\nRef:     ACGT- - - - ACGTACGT\nRead:  ACGTACGTACGTACGT\n\n\nFlags\nColumn 2 of the alignment contains a combination of bitwise FLAGs describing the alignment. The following table contains the information you can get from the bitwise FLAGs:\n\n\n\n\n\n\n\n\n\nHex\nDec\nFlag\nDescription\n\n\n\n\n0x1\n1\nPAIRED\npaired-end (or multiple-segment) sequencing technology\n\n\n0x2\n2\nPROPER_PAIR\neach segment properly aligned according to the aligner\n\n\n0x4\n4\nUNMAP\nsegment unmapped\n\n\n0x8\n8\nMUNMAP\nnext segment in the template unmapped\n\n\n0x10\n16\nREVERSE\nSEQ is reverse complemented\n\n\n0x20\n32\nMREVERSE\nSEQ of the next segment in the template is reversed\n\n\n0x40\n64\nREAD1\nthe first segment in the template\n\n\n0x80\n128\nREAD2\nthe last segment in the template\n\n\n0x100\n256\nSECONDARY\nsecondary alignment\n\n\n0x200\n512\nQCFAIL\nnot passing quality controls\n\n\n0x400\n1024\nDUP\nPCR or optical duplicate\n\n\n0x800\n2048\nSUPPLEMENTARY\nsupplementary alignment\n\n\n\nFor example, if you have an alignment with FLAG set to 113, this can only be represented by decimal codes 64 + 32 + 16 + 1, so we know that these four flags apply to the alignment and the alignment is paired-end, reverse complemented, sequence of the next template/mate of the read is reversed and the read aligned is the first segment in the template.\n\nPrimary, secondary and supplementary alignments\nA read that aligns to a single reference sequence (including insertions, deletions, skips and clipping but not direction changes), is a linear alignment. If a read cannot be represented as a linear alignment, but instead is represented as a group of linear alignments without large overlaps, it is called a chimeric alignment. These can for instance be caused by structural variations. Usually, one of the linear alignments in a chimeric alignment is considered to be the representative alignment, and the others are called supplementary.\nSometimes a read maps equally well to more than one spot. In these cases, one of the possible alignments is marked as the primary alignment and the rest are marked as secondary alignments."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#bam",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#bam",
    "title": "Data formats for NGS data",
    "section": "BAM",
    "text": "BAM\nBAM (Binary Alignment/Map) format, is a compressed binary version of SAM. This means that, while SAM is human readable, BAM is only readable for computers. BAM files can be viewed using samtools, and will then have the same format as a SAM file. The key features of BAM are:\n\nCan store alignments from most mappers\nSupports multiple sequencing technologies\nSupports indexing for quick retrieval/viewing\nCompact size (e.g. 112Gbp Illumina = 116GB disk space)\nReads can be grouped into logical groups e.g. lanes, libraries, samples\nWidely supported by variant calling packages and viewers\n\nSince BAM is a binary format, we can’t use the standard UNIX operations directly on this file format. Samtools is a set of programs for interacting with SAM and BAM files. Using the samtools view command, print the header of the BAM file:\n\nsamtools view -H data/NA20538.bam\n\n\nExercises\nQ13: What version of the human assembly was used to perform the alignments? (Hint: Can you spot this somewhere in the @SQ records?)\nQ14: How many lanes are in this BAM file? (Hint: Do you recall what RG represents?)\nQ15: What programs were used to create this BAM file? (Hint: have a look for the program record, @PG)\nQ16: What version of bwa was used to align the reads? (Hint: is there anything in the @PG record that looks like it could be a version tag?)\nThe output from running samtools view on a BAM file without any options is a headerless SAM file. This gets printed to STDOUT in the terminal, so we will want to pipe it to something. Let’s have a look at the first read of the BAM file:\n\nsamtools view data/NA20538.bam | head -n 1\n\nQ17: What is the name of the first read? (Hint: have a look at the alignment section if you can’t recall the different fields)\nQ18: What position does the alignment of the read start at?"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#cram",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#cram",
    "title": "Data formats for NGS data",
    "section": "CRAM",
    "text": "CRAM\nEven though BAM files are compressed, they are still very large. Typically they use 1.5-2 bytes for each base pair of sequencing data that they contain, and while disk capacity is ever improving, increases in disk capacity are being far outstripped by sequencing technologies.\nBAM stores all of the data, this includes every read base, every base quality, and it uses a single conventional compression technique for all types of data. CRAM was designed for better compression of genomic data than SAM/BAM. CRAM uses three important concepts:\n\nReference based compression\nControlled loss of quality information\nDifferent compression methods to suit the type of data, e.g. base qualities vs. metadata vs. extra tags\n\nThe figure below displays how reference-based compression works. Instead of saving all the bases of all the reads, only the nucleotides that differ from the reference, and their positions, are kept.\n\n\nIn lossless (no information is lost) mode a CRAM file is 60% of the size of a BAM file, so archives and sequencing centres have moved from BAM to CRAM.\nSince samtools 1.3, CRAM files can be read in the same way that BAM files can. We will look closer at how you can convert between SAM, BAM and CRAM formats in the next section."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#indexing",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#indexing",
    "title": "Data formats for NGS data",
    "section": "Indexing",
    "text": "Indexing\nTo allow for fast random access of regions in BAM and CRAM files, they can be indexed. The files must first be coordinate-sorted rather that sorted by read name. This can be done using samtools sort. If no options are supplied, it will by default sort by the left-most position of the reference.\n\nsamtools sort -o data/NA20538_sorted.bam data/NA20538.bam\n\nNow we can use samtools index to create an index file (.bai) for our sorted BAM file:\n\nsamtools index data/NA20538_sorted.bam\n\nTo look for reads mapped to a specific region, we can use samtools view and specify the region we are interested in as: RNAME[:STARTPOS[-ENDPOS]]. For example, to look at all the reads mapped to a region called chr4, we could use:\nsamtools view alignment.bam chr4\nTo look at the region on chr4 beginning at position 1,000,000 and ending at the end of the chromosome, we can do:\nsamtools view alignment.bam chr4:1000000\nAnd to explore the 1001bp long region on chr4 beginning at position 1,000 and ending at position 2,000, we can use:\nsamtools view alignment.bam chr4:1000-2000\n\nExercises\nQ19: How many reads are mapped to region 20025000-20030000 on chromosome 1?"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#vcf",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#vcf",
    "title": "Data formats for NGS data",
    "section": "VCF",
    "text": "VCF\nThe VCF file format was introduced to store variation data. VCF consists of tab-delimited text and is parsable by standard UNIX commands which makes it flexible and user-extensible. The figure below provides an overview of the different components of a VCF file:\n\n\n\nVCF format\n\n\n\nVCF header\nThe VCF header consists of meta-information lines (starting with ##) and a header line (starting with #). All meta-information lines are optional and can be put in any order, except for fileformat. This holds the information about which version of VCF is used and must come first.\nThe meta-information lines consist of key=value pairs. Examples of meta-information lines that can be included are ##INFO, ##FORMAT and ##reference. The values can consist of multiple fields enclosed by &lt;&gt;. More information about these fields is available in the VCF specification http://samtools.github.io/hts-specs/VCFv4.3.pdf. This can be accessed using a web browser and there is a copy in the QC directory.\n\nHeader line\nThe header line starts with # and consists of 8 required fields:\n\nCHROM: an identifier from the reference genome\nPOS: the reference position\nID: a list of unique identifiers (where available)\nREF: the reference base(s)\nALT: the alternate base(s)\nQUAL: a phred-scaled quality score\nFILTER: filter status\nINFO: additional information\n\nIf the file contains genotype data, the required fields are also followed by a FORMAT column header, and then a number of sample IDs. The FORMAT field specifies the data types and order. Some examples of these data types are:\n\nGT: Genotype, encoded as allele values separated by either / or |\nDP: Read depth at this position for this sample\nGQ: Conditional genotype quality, encoded as a phred quality\n\n\n\n\nBody\nIn the body of the VCF, each row contains information about a position in the genome along with genotype information on samples for each position, all according to the fields in the header line."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#bcf",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#bcf",
    "title": "Data formats for NGS data",
    "section": "BCF",
    "text": "BCF\nBCF is a compressed binary representation of VCF.\nVCF can be compressed with BGZF (bgzip) and indexed with TBI or CSI (tabix), but even compressed it can still be very big. For example, a compressed VCF with 3781 samples of human data will be 54 GB for chromosome 1, and 680 GB for the whole genome. VCFs can also be slow to parse, as text conversion is slow. The main bottleneck is the “FORMAT” fields. For this reason the BCF format was developed.\nIn BCF files the fields are rearranged for fast access. The following images show the process of converting a VCF file into a BCF file.\n\n\nBcftools comprises a set of programs for interacting with VCF and BCF files. It can be used to convert between VCF and BCF and to view or extract records from a region.\n\nbcftools view\nLet’s have a look at the header of the file 1kg.bcf in the data directory. Note that bcftools uses -h to print only the header, while samtools uses -H for this.\n\nbcftools view -h data/1kg.bcf\n\nSimilarly to BAM, BCF supports random access, that is, fast retrieval from a given region. For this, the file must be indexed:\n\nbcftools index data/1kg.bcf\n\nNow we can extract all records from the region 20:24042765-24043073, using the -r option. The -H option will make sure we don’t include the header in the output:\n\nbcftools view -H -r 20:24042765-24043073 data/1kg.bcf\n\n\n\nbcftools query\nThe versatile bcftools query command can be used to extract any VCF field. Combined with standard UNIX commands, this gives a powerful tool for quick querying of VCFs. Have a look at the usage options:\n\nbcftools query -h\n\nLet’s try out some useful options. As you can see from the usage, -l will print a list of all the samples in the file. Give this a go:\n\nbcftools query -l data/1kg.bcf\n\nAnother very useful option is -s which allows you to extract all the data relating to a particular sample. This is a common option meaning it can be used for many bcftools commands, like bcftools view. Try this for sample HG00131:\n\nbcftools view -s HG00131 data/1kg.bcf | head -n 50\n\nThe format option, -f can be used to select what gets printed from your query command. For example, the following will print the position, reference base and alternate base for sample HG00131, separated by tabs:\n\nbcftools query -f'%POS\\t%REF\\t%ALT\\n' -s HG00131 data/1kg.bcf | head\n\nFinally, let’s look at the -i option. With this option we can select only sites for which a particular expression is true. For instance, if we only want to look at sites that have at least 2 alternate alleles across all samples, we can use the following expression (piped to head to only show a subset of the output):\n\nbcftools query -f'%CHROM\\t%POS\\n' -i 'AC[0]&gt;2' data/1kg.bcf | head\n\nWe use -i with the expression AC[0]&gt;2. AC is an info field that holds the __a__llele __c__ount. Some fields can hold multiple values, so we use AC[0]&gt;2 to indicate that we are looking for the first value (this is zero indexed, and hence starts at 0 instead of 1), and that this value should be &gt; 2. To format our output, we use -f to specify that we want to print the chromosome name and position.\nThere is more information about expressions on the bcftools manual page http://samtools.github.io/bcftools/bcftools.html#expressions\n\n\nExercises\nNow, try and answer the following questions about the file 1kg.bcf in the data directory. For more information about the different usage options you can open the bcftools query manual page http://samtools.github.io/bcftools/bcftools.html#query in a web browser.\nQ20: What version of the human assembly do the coordinates refer to?\nQ21: How many samples are there in the BCF?\nQ22: What is the genotype of the sample HG00107 at the position 20:24019472? (Hint: use the combination of -r, -s, and -f options)\nQ23: How many positions are there with more than 10 alternate alleles? (Hint: use the -i filtering option)\nQ24: In how many positions does HG00107 have a non-reference genotype and a read depth bigger than 10? (Hint: you can use pipes to combine bcftools queries)"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#gvcf",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats.html#gvcf",
    "title": "Data formats for NGS data",
    "section": "gVCF",
    "text": "gVCF\nOften it is not enough to know variant sites only. For instance, we don’t know if a site was dropped because it matches the reference or because the data is missing. We sometimes need evidence for both variant and non-variant positions in the genome. In gVCF format, blocks of reference-only sites can be represented in a single record using the “INFO/END” tag. Symbolic alleles (&lt;*&gt;) are used for incremental calling:\n\n\n\ngVCF\n\n\n\nExercises\nQ25: In the above example, what is the size of the reference-only block starting at position 9923?\nQ26: For the same block, what is the first base?\nQ27: How many reference reads does the block have?\nNow continue to the next section of the tutorial: QC assessment of NGS data."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html",
    "title": "Identifying contamination",
    "section": "",
    "text": "It is always a good idea to check that your data is from the species you expect it to be. A very useful tool for this is Kraken. In this tutorial we will go through how you can use Kraken to check your samples for contamination.\nNote if using the Sanger cluster: Kraken is run as part of the automatic qc pipeline and you can retreive the results using the pf qc script. For more information, run pf man qc."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#setting-up-a-database",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#setting-up-a-database",
    "title": "Identifying contamination",
    "section": "Setting up a database",
    "text": "Setting up a database\nTo run Kraken you need to either build a database or download an existing one. The standard database is very large (33 GB), but thankfully there are some smaller, pre-built databased available. To download the smallest of them, the 4 GB MiniKraken. If you don’t already have a kraken database set up, run:\n\nwget https://ccb.jhu.edu/software/kra\\\n    ken/dl/minikraken_20171019_4GB.tgz\n\nThen all you need to do is un-tar it:\n\ntar -zxvf minikraken_20171019_4GB.tgz\n\nThis version of the database is constructed from complete bacterial, archaeal, and viral genomes in RefSeq, however it contains only around 3 percent of the kmers from the original kraken database (more information here). If the pre-packaged databases are not quite what you are looking for, you can create your own customized database instead. Details about this can be found here.\nNote if using the Sanger cluster: There are several pre-built databases available centrally on the Sanger cluster. For more information, please contact the Pathogen Informatics team."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#running-kraken",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#running-kraken",
    "title": "Identifying contamination",
    "section": "Running Kraken",
    "text": "Running Kraken\nTo run Kraken, you need to provide the path to the database you just created. By default, the input files are assumed to be in FASTA format, so in this case we also need to tell Kraken that our input files are in FASTQ format, gzipped, and that they are paired end reads:\n\nkraken --db ./minikraken_20171013_4GB --output kraken_results \\\n    --fastq-input --gzip-compressed --paired \\\n    data/13681_1#18_1.fastq.gz data/13681_1#18_2.fastq.gz\n\nThe five columns in the file that’s generated are:\n\n“C”/“U”: one letter code indicating that the sequence was either classified or unclassified.\nThe sequence ID, obtained from the FASTA/FASTQ header.\nThe taxonomy ID Kraken used to label the sequence; this is 0 if the sequence is unclassified.\nThe length of the sequence in bp.\nA space-delimited list indicating the LCA mapping of each k-mer in the sequence.\n\nTo get a better overview you can create a kraken report:\n\nkraken-report --db ./minikraken_20171013_4GB \\\n    kraken_results &gt; kraken-report"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#looking-at-the-results",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#looking-at-the-results",
    "title": "Identifying contamination",
    "section": "Looking at the results",
    "text": "Looking at the results\nLet’s have a closer look at the kraken_report for the sample. If for some reason your kraken-run failed there is a prebaked kraken-report at data/kraken-report\n\nhead -n 20 kraken-report\n\nThe six columns in this file are:\n\nPercentage of reads covered by the clade rooted at this taxon\nNumber of reads covered by the clade rooted at this taxon\nNumber of reads assigned directly to this taxon\nA rank code, indicating (U)nclassified, (D)omain, (K)ingdom, (P)hylum, (C)lass, (O)rder, (F)amily, (G)enus, or (S)pecies. All other ranks are simply ‘-’.\nNCBI taxonomy ID\nScientific name"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#exercises",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#exercises",
    "title": "Identifying contamination",
    "section": "Exercises",
    "text": "Exercises\nQ1: What is the most prevalent species in this sample?\nQ2: Are there clear signs of contamination?\nQ3: What percentage of reads could not be classified?"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#heterozygous-snps",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination.html#heterozygous-snps",
    "title": "Identifying contamination",
    "section": "Heterozygous SNPs",
    "text": "Heterozygous SNPs\nFor bacteria, another thing that you can look at to detect contamination is if there are heterozygous SNPs in your samples. Simply put, if you align your reads to a reference, you would expect any snps to be homozygous, i.e. if one read differs from the reference genome, then the rest of the reads that map to that same location will also do so:\nHomozygous SNP\nRef:       CTTGAGACGAAATCACTAAAAAACGTGACGACTTG\nRead1:  CTTGAGtCG\nRead2:  CTTGAGtCGAAA\nRead3:         GAGtCGAAATCACTAAAA\nRead4:               GtCGAAATCA\nBut if there is contamination, this may not be the case. In the example below, half of the mapped reads have the T allele and half have the A.\nHeterozygous SNP\nRef:       CTTGAGACGAAATCACTAAAAAACGTGACGACTTG\nRead1:  CTTGAGtCG\nRead2:  CTTGAGaCGAAA\nRead3:         GAGaCGAAATCACTAAAA\nRead4:               GtCGAAATCA\nNote if using the Sanger cluster: Heterozygous SNPs are calculated as part of the automated QC pipeline. The result for each lane is available in the file heterozygous_snps_report.txt.\nCongratulations! You have reached the end of this tutorial. You can find the answers to all the questions of the tutorial here.\nTo revisit the previous section click here. Alternatively you can head back to the index page"
  },
  {
    "objectID": "course_modules/Module5/module5.html",
    "href": "course_modules/Module5/module5.html",
    "title": "Overview",
    "section": "",
    "text": "Module Title\nGenome Assembly\n\n\nDuration\n3 hours\n\n\nKey topics\n\n\nActivities\n\nLecture: 1 hour\nPractical exercises: 1.5 hours\nAssessment quiz: 0.5 hours\n\n\n\nManual\nModule manual\nExercises\nSolutions\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual. Instructions on how to that will be added here.)\n\n\nLecture recording, notes and scripts\n\n\nPractical exercises\n\nVirtual Machine\nCommand line walk through.\n\n\n\nDatasets\n\n\nAssessment quiz\nQuestions\nSolution"
  },
  {
    "objectID": "course_modules/Module5/module5_manual.html",
    "href": "course_modules/Module5/module5_manual.html",
    "title": "Manual",
    "section": "",
    "text": "1. Overview of Genome Assembly\nProblem: Sequencing technologies produce only short DNA reads.\nGoal: Reconstruct a complete genome from millions to billions of short fragments.\nMethods:\n- Clone-based sequencing: Select and sequence specific clones.\n- Whole-genome shotgun sequencing: Fragment and sequence the entire genome.\n- De novo assembly: Reconstruct the genome without a reference.\n\n\n2. Challenges in Assembly (The Jigsaw Puzzle Analogy)\nMany small, damaged, and repetitive pieces make assembly harder.\nCritical factors:\n- Read length (longer is better)\n- Error rates (fewer is better)\n- Repetitive regions cause ambiguity.\n\n\n3. Key Considerations\n- Sequencing coverage\n- Error rates\n- Read length\n- Genome uniqueness\n- Genome size\n- Heterozygosity and ploidy\n- Computational resources (time and memory).\n\n\n4. Basic Terminology\n- Contig: A contiguous stretch of sequence.\n- Scaffold: Ordered set of contigs with gaps represented by Ns.\n\n\n5. Sequencing Technologies\nPacBio Sequencing:\n- Very long reads, detects base modifications.\n- High error rate, expensive.\n\nNanopore Sequencing:\n- Portability, ultra-long reads.\n- High error rates, systematic errors near homopolymers.\n\nLinked-Reads and Read Clouds:\n- Bridges long repeats and improves assembly continuity.\n\nHi-C Data:\n- Captures chromatin conformation to assist chromosome-scale assembly.\n\nOptical Mapping:\n- Labels and images long DNA fragments for structural scaffolding.\n\n\n6. Long-Range Technologies Comparison\nPlatforms: PacBio, Nanopore, 10X Genomics, Hi-C, Optical mapping.\nEach technology offers distinct advantages and bioinformatic challenges.\n\n\n7. Example Project Workflow\nTechnologies are integrated sequentially for high-quality genome assembly.\nExample: Vertebrate Genomes Project (VGP).\n\n\n8. Assembly Methods\nContig Generation:\n- OLC (Overlap-Layout-Consensus): Good for long reads.\n- de Bruijn Graphs (DBG): Good for short reads.\n\n\n9. Common Hurdles\n- Heterozygosity\n- Sequencing errors\n- Repeats\n- Low-complexity regions\n- Base composition biases.\n\n\n10. Quality Metrics\n- Total assembly length\n- Number of contigs and scaffolds\n- N50, NG50\n- Gene content assessments.\n\n\n11. Scaffolding and Chromosome Assignment\nSources: Mate-pair libraries, Hi-C, Optical maps.\nTools like RACA help build chromosome-scale scaffolds.\n\n\n12. Diploid Assembly\nProblem: Standard assemblers assume haploidy.\nSolutions:\n- Falcon-Unzip: Haplotyping assembly.\n- Purge Haplotigs: Removes haplotype duplications.\n- Falcon-Phase: Phases haplotypes using Hi-C data.\n- Trio Binning: Uses parental reads for haplotype separation.\n\n\n13. Assembly Quality Control\nGene Content: BUSCO analysis.\nBase Accuracy: Realign reads to detect errors.\nStructural Accuracy: Using known structural data and tools like QUAST, REAPR.\n\n\n14. Specialized Assemblies\nMetagenomic Assembly:\n- For mixed-organism samples (e.g., environmental DNA).\n\nTranscriptome Assembly:\n- For RNA sequencing data.\n\n\n15. Large Genome Projects\nTree of Life Project and Darwin Tree of Life: Massive efforts to sequence all eukaryotic life in Britain.\nAchievements: New assemblies of Lepidoptera.\nHighlight: Complete chromosome assemblies achieved, e.g., human X chromosome."
  },
  {
    "objectID": "course_modules/Module2/module2_solutions.html",
    "href": "course_modules/Module2/module2_solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "1 Read Alignment\nThere are no questions in this section.\n\n\n2 Performing Read Alignment\n1. 145441459\n2. The sam is ~ 157M and the bam is ~ 25M\n3. 67,461 (17.2%) or 359 + (33551 * 2)\n4. 392,820\n5. 391603/392820 (99.7%)\n6. 389410\n7. 0\n8. 419 (mean) 113.9 (standard deviation)\n9. 7,853 (2.0%)\n10. First/Forward read\n\n\n3 Alignment Visualisation\n1. This exercise is just asking you to explore the genome and become familiar with navigating in IGV.\n2. 23X-57X\n3. There is a 1bp insertion (at “T)” at position 87,483,966. This is supported by 9 reads.\nThere is a 28bp deletion at position 87,483,966. This is supported by 3 reads.\nThe third mutation is a bit harder to spot, because it’s bigger than the read length (so no single read will span it). Look first at the coverage track, and notice a sharp coverage drop between chr7:87,483,833 and chr7:87,484,169. That suggests that one of the two alleles have been deleted across that position. Notice also the soft-clipping of some reads “entering” chr7:87,483,833 from the left, and the same softclipping of reads entering chr7:87,484,169 from the right. This soft-clipping shows up as reads being partly multi-coloured. That’s happening because the physical genome between those points has been excised for one allele, causing the mis-alignment when we attempt to align some reads to the reference genome. That mis-alignment causes the bwa aligner to “give up” and mark a part of the read as soft-clipped.\n4. This mouse was bred from a zygote which was mutagenised with Crispr-Cas9, targeted at the Tyr locus. You are watching the zygote DNA-repair machinery panicking and grabbing at straws when trying to repair double-stranded DNA breaks. In the process, it makes mistakes, and those mistakes are propagated into the mouse genome: different zygote cells received different mutations, which is why some reads reflect different mutations to others. Specifically - The CRISPR-Cas9 has acted on the zygote at this locus to create Non-Homologous-End-Join-based damage around 87,483,960: that resulted in a subclonal 1bp insertion and a 28bp deletion. Also, a related DNA repair process has resulted in the a 336bp deletion across the same area.\n\n\n3.1 Alignment workflows\n1. -M marks shorter split hits as secondary and -R adds the read group to the header of the BAM file\n2. -b means create a BAM as output and -S indicates that the input files is a SAM file. The -S option is now ignored by samtools as it can now autodetect the input file type.\n3. 397506\n4. 303036/397506 (76.2%)\n5. 282478\n6. 2239\n7. 275.9 (mean) and 47.7 (standard deviation)\n8. 23,789 (7.9%)\n9. First\n10.\nbwa mem -M -R ”@RG\\tID:lane2\\tSM:60A_Sc_DBVPG6044” ../../../../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa.gz s_7_1.fastq.gz s_7_2.fastq.gz | samtools view -bS - | samtools sort -T temp -O bam -o lane1.sorted.bam -\n~22M\nMerge: ~/course_data/read_alignment/data/Exercise2/60A_Sc_DBVPG6044/library1$ picard\nMergeSamFiles -I lane1/lane1.sorted.bam -I lane2/lane1.sorted.bam -O library1.bam\nMarkdup: ~/course_data/read_alignment/data/Exercise2/60A_Sc_DBVPG6044/library1$ picard\nMarkDuplicates -I library1.bam -O library1.markdup.bam -M library1.metrics.txt\n11. 12399 or 3115 + (4642 * 2) = unpaired read dups + (paired read dups *2)\n12. 2.5%\n\n\n3.2 Exercises\n1. No answer needed\n2. The reference base is C\n3. No (the reads call T)\n4. The reference base is G and all reads agree\n5. No answer\n6. An insertion\n7. No answer\n8. A deletion. This is unlikely to be a true variant and may be due to misalignment due the run of T’s in the flanking region.\n9. The following command procuces a cram file which should be ~29MB in size.\nsamtools view -C -T ../../../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa -o library1.markdup.cram library1.markdup.bam\n-C means create a CRAM file as output\n-T is the reference file to use for the compression\n-o is the name of CRAM file to create"
  },
  {
    "objectID": "course_modules/Module3/module3_assessment.html",
    "href": "course_modules/Module3/module3_assessment.html",
    "title": "Quiz",
    "section": "",
    "text": "Q1\nWhat is the main difference between germline mutations and somatic mutations?\nA. Germline mutations occur in non-reproductive cells, while somatic mutations occur in reproductive cells.\nB. Germline mutations are heritable as they occur in egg or sperm cells, while somatic mutations occur in non-germline tissues and are not passed to offspring.\nC. Germline mutations only occur in bacteria, while somatic mutations are found only in humans.\nD. Both types of mutations are heritable but affect different chromosomes.\n\n\nQ2\nWhich of the following is an example of a large-scale genomic variation?\nA. A point mutation in a single base.\nB. A duplication involving 2 kilobases of DNA (CNV).\nC. A small insertion or deletion of 10 base pairs.\nD. A single nucleotide polymorphism (SNP).\n\n\nQ3\nWhich statement best describes the difference between single nucleotide variants (SNVs) and indels?\nA. SNVs involve changes in multiple adjacent bases; indels involve changes in just one base.\nB. SNVs are large deletions; indels are large insertions.\nC. SNVs involve single base substitutions, while indels are small insertions or deletions typically less than 50 base pairs.\nD. SNVs occur only in coding regions; indels occur only in non-coding regions.\n\n\nQ4\nWhich of the following is NOT a practical application of variant calling?\nA. Cataloging biological diversity in population genetics.\nB. Identifying pathogenic mutations for disease diagnosis.\nC. Determining the speed of DNA replication during cell division.\nD. Tailoring drug dosages in pharmacogenomics.\n\n\nQ5\nWhat is the benefit of resolving phased haplotypes in variant calling analyses?\nA. It provides information about the total number of chromosomes in the organism.\nB. It determines which variants are inherited together on the same chromosome, thereby clarifying genotype information.\nC. It helps to identify only somatic mutations exclusively.\nD. It replaces the need for any sequencing data.\n\n\nQ6\nWhich key field is NOT typically included in a Variant Call Format (VCF) file?\nA. CHROM (chromosome)\nB. POS (position)\nC. TEMP (temperature of the sample)\nD. ALT (alternate allele)\n\n\nQ7\nWhich sequence correctly describes the process from raw sequencing data to final variant calls?\nA. Alignment, Quality Control, Generate Sequencing Data, Pileup, Variant Calling.\nB. Generate Sequencing Data, Alignment, Quality Control, Variant Calling, Pileup.\nC. Generate Sequencing Data, Quality Control, Alignment, Pileup, Variant Calling.\nD. Pileup, Generate Sequencing Data, Quality Control, Alignment, Variant Calling.\n\n\nQ8\nWhich of the following is a common source of error in variant calling?\nA. Uniform base composition across the genome.\nB. Homopolymers and repetitive regions causing sequencing and mapping errors.\nC. Excessively high mapping quality in all reads.\nD. Infrequent occurrence of strand bias in high-quality datasets.\n\n\nQ9\nThe transition/transversion (Ts/Tv) ratio is used as a quality metric in SNP datasets. What Ts/Tv ratio is typically expected in high-quality human SNP calls?\nA. Approximately 0.5–1\nB. Approximately 1–2\nC. Approximately 2–3\nD. Approximately 4–5\n\n\nQ10\nHow do population-level datasets like those from the 1000 Genomes Project and gnomAD aid researchers?\nA. They provide a comprehensive snapshot of human genetic diversity and variant frequencies across various populations.\nB. They measure the metabolic rates of individuals in diverse populations.\nC. They only include data from European populations.\nD. They offer real-time monitoring of gene expression levels."
  },
  {
    "objectID": "course_modules/Module3/module3_solutions.html",
    "href": "course_modules/Module3/module3_solutions.html",
    "title": "module3_solutions",
    "section": "",
    "text": "1 Variant Calling - Solutions\nNo exercises in this section.\n\n\n2 Performing variant calling\nQ1 66 reads\nQ2 The reference allele is A and the alternate allele is G. The upper/lower case letters indicate the forward/reverse orientation of the read.\nQ3 0 reads calling reference allelle and 66 reads calling the alternate allele\nQ4 Add the -v option to the command:\nbcftools mpileup -f GRCm38_68.19.fa A_J.bam | bcftools call -mv | less -S\n\n\n2.1 Exercises\nQ1 The reference allele is A and the alternate allele is G.\nQ2 Look up the tag DP in the INFO column: there were 69 raw reads at the position.\nQ3 There are 0 reads calling the reference and 66 high-quality reads calling the alternate.\nQ4 An indel. Five bases TGTGG were inserted after the T at position 10003649\n\n\n3 Variant Filtering\nQ1 The complete command is:\nbcftools query -f'%POS %QUAL [%GT %AD] %REF %ALT\\n' -i'QUAL&gt;=30 && type=\"snp\" && AD[*:1]&gt;=25' out.vcf | head\nQ2 The complete command is:\nbcftools stats -i'QUAL&gt;=30 && AD[*:1]&gt;=25' out.vcf | grep TSTV | cut -f5\nQ3 The complete command is:\nbcftools stats -e'QUAL&gt;=30 && AD[*:1]&gt;=25' out.vcf | grep TSTV | cut -f5\nQ4 The complete command is:\nbcftools stats -i 'GT=\"het\"' out.vcf | grep TSTV | cut -f5\nQ5 The complete command is:\nbcftools norm -f GRCm38_68.19.fa out.flt.vcf -o out.flt.norm.vcf\n\n\n4 Calling Variants Across Multiple Samples\nQ1 Use the commands:\nbcftools mpileup -a AD -f GRCm38_68.19.fa *.bam -Ou | bcftools call -mv -Ob -o multi.bcf bcftools index multi.bcf\nQ2 Use the commands\nbcftools filter -s LowQual -i'QUAL&gt;=30 && AD[*:1]&gt;=25' -g8 -G10 multi.bcf -Ob -o multi.filt.bcf bcftools index multi.filt.bcf\nQ3 Use the commands:\nbcftools stats multi.filt.bcf | grep TSTV | cut -f5 (raw calls)\nbcftools stats -i 'FILTER=\"PASS\"' multi.filt.bcf | grep TSTV | cut -f5 (only filtered set)\nQ4 Use the commands:\nbcftools stats -e 'FILTER=\"PASS\"' multi.filt.bcf | grep TSTV | cut -f5\n\n\n5 Visualising Alignments\nQ1 75 in total, 39 on the forward and 36 on the reverse strand.\nQ2 Yes. Use the command:\nbcftools view -H -r 19:10001946 multi.filt.bcf\nQ3 Yes.\nQ4 Yes. Use the command:\nbcftools view -H -r 19:10072443 multi.filt.bcf\nQ5 No. It fails due to lowQual and snpGap, this means the call was removed by filtering because the quality of the call falls below the treshold set and the call is in close proximity to an indel.\nQ6 No. It is an alignment artefact, the aligner prefered two SNPs instead of a long deletion.\n\n\n6 Variant annotation\nQ1 Use the command:\nbcftools query -f '%BCSQ' -r 19:10088937 multi.filt.annot.bcf\nto return\nmissense|Fads2|ENSMUST00000025567|protein_coding|-|163V&gt;163I|10088937C&gt;T(base)\nQ2 A missense mutation\nQ3 The C&gt;T mutation changes the amino acid at position 163 in the protein sequence, from valine to isoleucine."
  },
  {
    "objectID": "course_modules/Module3/module3_assessement_answers.html",
    "href": "course_modules/Module3/module3_assessement_answers.html",
    "title": "Quiz answers",
    "section": "",
    "text": "Q1.\nCorrect Answer: B Explanation: Germline mutations occur in egg or sperm cells, meaning they are heritable, while somatic mutations occur in non-reproductive tissues and are not passed on.\n\n\nQ2.\nCorrect Answer: B Explanation: A duplication involving 2 kb is an example of a copy number variant, which is considered a large-scale variation.\n\n\nQ3.\nCorrect Answer: C Explanation: SNVs refer to single base substitutions, and indels refer to small insertions or deletions generally less than 50 bp.\n\n\nQ4.\nCorrect Answer: C Explanation: Determining the speed of DNA replication is not a direct application of variant calling.\n\n\nQ5.\nCorrect Answer: B Explanation: Phased haplotypes allow for the determination of which variants are inherited together on the same chromosome, enhancing genotype interpretation.\n\n\nQ6.\nCorrect Answer: C Explanation: “TEMP” is not a standard field in the VCF format, while CHROM, POS, and ALT are.\n\n\nQ7.\nCorrect Answer: C Explanation: The correct order is to generate sequencing data, perform quality control, align the reads, generate a pileup, and then call variants.\n\n\nQ8.\nCorrect Answer: B Explanation: Homopolymers and repetitive regions are known to cause systematic errors in sequencing and mapping, leading to inaccuracies in variant calling.\n\n\nQ9.\nCorrect Answer: C Explanation: High-quality human SNP datasets typically have a Ts/Tv ratio of around 2 to 3.\n\n\nQ10.\nCorrect Answer: A Explanation: Population-level datasets provide valuable insights into human genetic diversity and help determine variant frequencies across various populations."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html",
    "href": "course_modules/Module4/Notebooks/index.html",
    "title": "Structural Variation Calling",
    "section": "",
    "text": "Structural variants (SVs) are large genomic alterations of at least 50 bp or larger in size. There are several types of SVs, including deletions, duplications, insertions, inversions, and translocations which describe different combinations of DNA gains, losses, or rearrangements. Copy number variations (CNVs) are a particular subtype of SV mainly represented by deletions and duplications. SVs are typically described as single events, although more complex scenarios involving combinations of SV types exist."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html#introduction",
    "href": "course_modules/Module4/Notebooks/index.html#introduction",
    "title": "Structural Variation Calling",
    "section": "",
    "text": "Structural variants (SVs) are large genomic alterations of at least 50 bp or larger in size. There are several types of SVs, including deletions, duplications, insertions, inversions, and translocations which describe different combinations of DNA gains, losses, or rearrangements. Copy number variations (CNVs) are a particular subtype of SV mainly represented by deletions and duplications. SVs are typically described as single events, although more complex scenarios involving combinations of SV types exist."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html#learning-outcomes",
    "href": "course_modules/Module4/Notebooks/index.html#learning-outcomes",
    "title": "Structural Variation Calling",
    "section": "Learning outcomes",
    "text": "Learning outcomes\nOn completion of the tutorial, you can expect to be able to:\n\nCall structural variants using standard tools\nVisualise structural variants using standard tools\nCall structural variants from long read data\nUse bedtools to do regional comparisons over genomic co-ordinates"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html#tutorial-sections",
    "href": "course_modules/Module4/Notebooks/index.html#tutorial-sections",
    "title": "Structural Variation Calling",
    "section": "Tutorial sections",
    "text": "Tutorial sections\nThis tutorial comprises the following sections: 1. Looking at structural variants in VCF 2. Calling structural variants\n3. Structural variants from long reads\n4. Bedtools"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html#authors",
    "href": "course_modules/Module4/Notebooks/index.html#authors",
    "title": "Structural Variation Calling",
    "section": "Authors",
    "text": "Authors\nThis tutorial was written by Jacqui Keane based on material from Thomas Keane."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html#running-the-commands-in-this-tutorial",
    "href": "course_modules/Module4/Notebooks/index.html#running-the-commands-in-this-tutorial",
    "title": "Structural Variation Calling",
    "section": "Running the commands in this tutorial",
    "text": "Running the commands in this tutorial\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal window and type the command below:\n\ncd /home/manager/course_data/structural_variation/data"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/index.html#lets-get-started",
    "href": "course_modules/Module4/Notebooks/index.html#lets-get-started",
    "title": "Structural Variation Calling",
    "section": "Let’s get started!",
    "text": "Let’s get started!\nThis tutorial requires that you have breakdancer, lumpy, minimap2, sniffles, bedtools and igv installed on your computer. These are already installed on the virtual machine you are using. To check that these are installed, run the following commands:\n\nbreakdancer-max -h\n\n\ndysgu --help\n\n\nminimap2 --help\n\n\nsniffles --help\n\n\nbedtools --help\n\nThis should return the help message for software breakdancer, dysgu, minimap2, sniffles and bedtools respectively.\nIf after this course you would like to download and install this software the instructions can be found at the links below, alternatively we recommend bioconda for the installation and management of your bioinformatics software.\n\nThe breakdancer website\nThe dysgu github page\nThe minimap2 website\nThe sniffles website\nThe bedtools website\n\nTo get started with the tutorial, go to the first section: Looking at structural variants in VCF"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/solutions.html",
    "href": "course_modules/Module4/Notebooks/solutions.html",
    "title": "Structural Variation Calling - Solutions",
    "section": "",
    "text": "No questions in this section."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/solutions.html#exercises",
    "href": "course_modules/Module4/Notebooks/solutions.html#exercises",
    "title": "Structural Variation Calling - Solutions",
    "section": "Exercises",
    "text": "Exercises\n\nWhat does the CIPOS format tag indicate? Confidence interval around POS for imprecise variants\nWhat does the PE tag indicate? Number of paired-end reads supporting the variant across all samples\nWhat tag is used to describe an inversion event? INV\nWhat tag is used to describe a duplication event? DUP\nHow many deletions were called in total? (Hint: DEL is the info field for a deletion. The -c option of the grep command can be used to return a count of matches.) 31, try\n\ngrep -c \"&lt;DEL&gt;\" ERR1015121.vcf\n\nWhat type of event is predicted at IV:437148? What is the length of the SV? How many paired-end reads and split-reads support this SV variant call? Deletion -370 20 PE 21 split\n\ngrep \"437148\" ERR1015121.vcf\n\nWhat is the total number of SV calls predicted on the IV chromosome? 10, try\n\ngrep -c \"^IV\" ERR1015121.vcf"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/solutions.html#breakdancer",
    "href": "course_modules/Module4/Notebooks/solutions.html#breakdancer",
    "title": "Structural Variation Calling - Solutions",
    "section": "Breakdancer",
    "text": "Breakdancer\n\nExercises\ngrep \"83065\" ERR1015121.breakdancer.out\n\nInversion\n-116,\n42\n\ngrep \"258766\" ERR1015121.breakdancer.out\n\nDeletion (7325, 99)\ngrep DEL | awk OFS= breakdancer.dels.bed | awk '{print $1\"\\t\"$2\"\\t\"$5\"\\t\"$7\"\\t\"$9}' &gt; breakdancer.dels.bed"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/solutions.html#inspecting-svs-with-igv",
    "href": "course_modules/Module4/Notebooks/solutions.html#inspecting-svs-with-igv",
    "title": "Structural Variation Calling - Solutions",
    "section": "Inspecting SVs with IGV",
    "text": "Inspecting SVs with IGV\n\nExercises\n\nYes, a deletion (view as paired, sort by insert size, squish).\nThere are very few reads mapping, the reads that are mapped are of low mapQ and it has a SV score = 99\nSize estimate? ~7.5k\n\nWas the deletion at II:258766 also called by the other structural variant software and was the predicted size?\n\nYes, SVTYPE=DEL, SVLEN=-7438\nDEL called by breakdancer (score=59). Not found by other caller Lumpy.\nYes, 2 reads support (red)."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/solutions.html#dysgu",
    "href": "course_modules/Module4/Notebooks/solutions.html#dysgu",
    "title": "Structural Variation Calling - Solutions",
    "section": "Dysgu",
    "text": "Dysgu\n\nExercises\n\nWhat was the total number of SVs identified? How many PASS SVs were identified by Dysgu? Why did the rest of the SVs fail? bcftools view -H ERR1015069.vcf | wc -l 30\n\nbcftools view -H -i ‘FILTER=“PASS”’ ERR1015069.vcf |wc -l 8\nlowProb: ##FILTER=&lt;ID=lowProb,Description=“Probability below threshold set with –thresholds”&gt;\n\nWhat type of SV event occurs at position IV:384221? What is the length of the SV event? What is the genotype quality?\n\nDEL = Deletion, SVLEN=328, GQ=62\n\nWhat type of SV event occurs at position XV:31115? What is the length of the SV event? What is the probability of the structural variant?\n\nINS = Insertion, SVLEN=63, PROB=0.816\n\nCalling Structural Variants from Long Reads\n\n\n\nAlign the reads with minimap and convert to bam\nminimap2 -t 2 -x map-pb -a ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa YPS128.filtered_subreads.10x.fastq.gz | samtools view -b -o YPS128.filtered_subreads.10x.bam -\n\n\nSort the bam\nsamtools sort -T temp -o YPS128.filtered_subreads.10x.sorted.bam YPS128.filtered_subreads.10x.bam\nsamtools calmd -b YPS128.filtered_subreads.10x.sorted.bam ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa &gt; YPS128.filtered_subreads.10x.sorted.calmd.bam\n\n\nIndex the sorted bam\nsamtools index YPS128.filtered_subreads.10x.sorted.calmd.bam\n\n\nCall SVs with sniffles\nsniffles --input YPS128.filtered_subreads.10x.sorted.calmd.bam --vcf YPS128.filtered_subreads.10x.vcf\n\n\nExercises\n\nWhat sort of SV was called at on chromosome ‘XV’ at position 854272? __Deletion_\nWhat is the length of the SV? 344\nHow many reads are supporting the SV? 14 (SUPPORT tag)\nWhat sort of SV was called at on chromosome ‘XI’ at position 74608? __Insertion_\nWhat is the length of the SV? 358\nHow many reads are supporting the SV? 15\nHow many inversions were called in the VCF? Note inversions are denoted by the type ‘INV’. 6 in total, 5 passed\nHow many duplications were called in the VCF? Note duplications are denoted by the type ‘DUP’. 2"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/solutions.html#exercises-5",
    "href": "course_modules/Module4/Notebooks/solutions.html#exercises-5",
    "title": "Structural Variation Calling - Solutions",
    "section": "Exercises",
    "text": "Exercises\n\nHow many SVs found in ERR1015069.dels.vcf overlap with a gene? (Hint: Use bedtools intersect command) 18, try (note the -u parameter is required to get the unique number of SVs)\n\nbedtools intersect -u -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3  | wc -l\n\nHow many SVs found in ERR1015069.dels.vcf do not overlap with a gene? (Hint: note the -v parameter to bedtools intersect) 9, try\n\nbedtools intersect -v -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3  | wc -l\n\nHow many SVs found in ERR1015069.dels.vcf overlap with a more strict definition of 50%? 14, try\n\nbedtools intersect -u -f 0.5 -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3  | wc -l\n\nHow many features does the deletion at VII:811446 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect. bedtools intersect -wb -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | grep 811446 4 features, all of them are protein coding genes (biotype=protein_coding)\nHow many features does the deletion at XII:650823 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect. bedtools intersect -wb -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | grep 811446 2 features, all of them are protein coding genes (biotype=protein_coding)\nWhat is the closest gene to the structural variant at IV:384220 in ERR1015069.dels.vcf? YDL037C, try\n\nbedtools closest -d -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3| grep IV | grep 384220\n\nHow many SVs overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf? 27, try\n\nbedtools intersect -u -a ERR1015069.dels.vcf -b ERR1015121.dels.vcf | wc -l\n\nHow many SVs have a 90% reciprocal overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf (Hint: first find the option for reciprocal overlap by typing: bedtools intersect -h) 24, try\n\nbedtools intersect -u -r -f 0.9 -a ERR1015069.dels.vcf -b ERR1015121.dels.vcf | wc -l"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/sv-calling.html",
    "href": "course_modules/Module4/Notebooks/sv-calling.html",
    "title": "Calling Structural Variants",
    "section": "",
    "text": "There are several software tools available for calling structural variants. We will use two callers in this part of the tutorial, breakdancer and lumpy"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/sv-calling.html#breakdancer",
    "href": "course_modules/Module4/Notebooks/sv-calling.html#breakdancer",
    "title": "Calling Structural Variants",
    "section": "Breakdancer",
    "text": "Breakdancer\nBreakDancer predicts five types of structural variants: insertions (INS), deletions (DEL), inversions (INV), inter-chromosomal translocations (CTX) and intra-chromosomal translocations (ITX) from next-generation short paired-end sequencing reads using read pairs that are mapped with unexpected separation distances or orientation.\nNavigate to the exercise2 directory:\n\ncd ../exercise2\n\n\nls\n\nWe will use the Breakdancer software package to call structural variants on a yeast sample that was paired-end sequenced on the illumina HiSeq 2000. Breakdancer first needs to examine the BAM file to get information on the fragment size distribution for each sequencing library contained in the BAM file.\nThe breakdancer.config file has information about the sequencing library fragment size distribution. Use the cat command to print the contents of the breakdancer.config file.\nQ What is the mean and standard deviation of the fragment size?\nRun the breakdancer SV caller using the command:\n\nbreakdancer-max breakdancer.config &gt; ERR1015121.breakdancer.out\n\nLook at the output of Breakdancer.\n\nhead ERR1015121.breakdancer.out\n\nNote that the output from Breakdancer is NOT VCF format, instead it is a simple text format with one line per SV event.\n\nExercises\n\nWhat type of SV event is predicted at position III:83065?\nWhat is the size of this SV?\nWhat is the score of this SV?\nWhat type of SV event is predicted at position II:258766?\nConvert the output of breakdancer into BED format\n\nThe BED format is explained here: https://genome.ucsc.edu/FAQ/FAQformat.html#format1\nTo complete this task, create a command that: 1. Extracts all the deletions from the breakdancer.out file (Hint: use grep) 2. Prints columns: 1, 2, 5, 7, and 9 to create a BED file with columns: chromosome, start, end, name, and score. (Hint: use awk to do this, e.g. awk '{print $1\"\\t\"$2}') 3. Print the resulting bed output into a file called: breakdancer.dels.bed"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/sv-calling.html#inspecting-svs-with-igv",
    "href": "course_modules/Module4/Notebooks/sv-calling.html#inspecting-svs-with-igv",
    "title": "Calling Structural Variants",
    "section": "Inspecting SVs with IGV",
    "text": "Inspecting SVs with IGV\nNow we will open the IGV genome browser and inspect some of the predicted structural variants.\nTo do this type:\n\nigv\n\nOpen the reference genome. Go to ’ Genomes -&gt; Load Genome From Server… ’ and select “S. cerevisiae (SacCer3)”.\nLoad the BAM file. Go to ’ File -&gt; Load from File… ‘. Select the “ERR1015121.bam” BAM file and click’ Open ’.\nLoad the BED file for the deletion calls that you created in the exercise 5 above. Go to ’ File -&gt; Load from File… ‘. Select the “breakdancer.dels.bed” BED file and click’ Open ’.\n\nExercises\nUsing the navigation bar, go to region II:258,500-266,700.\n\nCan you see the structural variant? What type of structural variant is it? (Hint: you may need to zoom out a little to see the full structural variant).\nCan you see any evidence to support this SV call?\nCan you estimate the size of the SV?\n\nThe VCF in the exercise1 directory was produced by another structural variant caller on the same sample as this exercise.\n\nLoad the exercise1/ERR1015121.vcf VCF into IGV also (File - Load from file, and select ERR1015121.vcf in the exercise 1 directory).\nWas the structural variant at II:258766 also called by the other structural variant software (lumpy)? If so, what was the predicted size?\n\nUsing the navigation bar, go to to region II:508,064-511,840.\n\nIs there a SV deletion called in this region by either SV caller?\nIs there any read support for a SV deletion in this region? If so, how many read pairs could support the deletion call (Hint: change the IGV view to squished and View as pairs to see any inconsistently aligned read pairs)."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/sv-calling.html#dysgu",
    "href": "course_modules/Module4/Notebooks/sv-calling.html#dysgu",
    "title": "Calling Structural Variants",
    "section": "Dysgu",
    "text": "Dysgu\nWe will use the Dysgu (pronounced duss-key) software package (https://github.com/kcleal/dysgu) to call structural variants on a yeast sample that was paired-end sequenced on the Illumina Hiseq 2000. Dysgu is designed to take BAM files that have been aligned with BWA-mem.\nNavigate to the exercise 3 directory:\n\ncd ../exercise3\n\n\nls\n\nCheck that there is a BAM file called ERR1015069.bam and an index file ERR1015069.bam.bai in the directory. The sequence data has already been mapped with bwa mem and the results are stored in ERR1015069.bam.\nTo call SVs, a sorted and indexed .bam/cram is needed plus an indexed reference genome (fasta format). Also a working directory must be provided to store temporary files.\n\ndysgu run ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa temp ERR1015069.bam &gt; ERR1015069.vcf\n\nDysgu is a multi-stage pipeline * The first stage of the “run” pipeline is to separate SV-associated reads - split/discordant reads, and reads with a soft-clip &gt;= clip_length (15 bp by default). * The next stage of the pipeline is to call and genotype SVs using the reads from the first stage. The run command above combines both of these stages together.\n\nExercises\n\nWhat was the total number of SVs identified? How many PASS SVs were identified by Dysgu? Why did the rest of the SVs fail?\nWhat type of SV event occurs at position IV:384221? What is the length of the SV event? What is the genotype quality?\nWhat type of SV event occurs at position XV:31115? What is the length of the SV event? What is the probability of the structural variant?\n\nCongratulations, you have sucessfully called structural variants from some NGS data. Now continue to the next section of the tutorial: Calling structural variants from long reads"
  },
  {
    "objectID": "course_modules/Module4/module4.html",
    "href": "course_modules/Module4/module4.html",
    "title": "Overview",
    "section": "",
    "text": "Module Title\nStructural Variation Calling\n\n\nDuration\n3 hours\n\n\nKey topics\n\n\nActivities\n\nLecture: 1 hour\nPractical exercises: 1.5 hours\nAssessment quiz: 0.5 hours\n\n\n\nManual\nModule manual\nExercises\nSolutions\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual. Instructions on how to that will be added here.)\n\n\nLecture recording, notes and scripts\n\n\nPractical exercises\n\nVirtual Machine\nCommand line walk through.\n\n\n\nDatasets\n\n\nAssessment quiz\nQuestions\nSolution"
  },
  {
    "objectID": "course_modules/Module4/module4_exercises.html",
    "href": "course_modules/Module4/module4_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Structural variants (SVs) are large genomic alterations of at least 50 bp or larger in size. There are several types of SVs, including deletions, duplications, insertions, inversions, and translocations which describe different combinations of DNA gains, losses, or rearrangements. Copy number variations (CNVs) are a particular subtype of SV mainly represented by deletions and duplications. SVs are typically described as single events, although more complex scenarios involving combinations of SV types exist.\n\n\n\nOn completion of the tutorial, you can expect to be able to:\n• Call structural variants using standard tools\n• Visualise structural variants using standard tools\n• Call structural variants from long read data\n• Use bedtools to do regional comparisons over genomic co-ordinates\n\n\n\nThis tutorial comprises the following sections:\n1. Looking at structural variants in VCF\n2. Calling structural variants\n3. Structural variants from long reads\n4. Bedtools\n\n\n\nThis tutorial was written by Jacqui Keane based on material from Thomas Keane.\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files. To get started, open a new terminal window and type the command below:\ncd /home/manager/course_data/structural_variation/data\n\n\n\nThis tutorial requires that you have breakdancer, lumpy, minimap2, sniffles, bedtools and igv installed on your computer.\nThese are already installed on the virtual machine you are using. To check that these are installed, run the following commands:\nbreakdancer-max -h\ndysgu --help\nminimap2 --help\nsniffles --help\nbedtools --help\nThis should return the help message for software breakdancer, dysgu, minimap2, sniffles and bedtools respectively.\nIf after this course you would like to download and install this software the instructions can be found at the links below, alternatively we recommend bioconda for the installation and management of your bioinformatics software.\n• The breakdancer website\n• The dysgu github page\n• The minimap2 website\n• The sniffles website\n• The bedtools website\nTo get started with the tutorial, go to the first section: Looking at structural variants in VCF"
  },
  {
    "objectID": "course_modules/Module4/module4_exercises.html#structural-variation-calling",
    "href": "course_modules/Module4/module4_exercises.html#structural-variation-calling",
    "title": "Exercises",
    "section": "",
    "text": "Structural variants (SVs) are large genomic alterations of at least 50 bp or larger in size. There are several types of SVs, including deletions, duplications, insertions, inversions, and translocations which describe different combinations of DNA gains, losses, or rearrangements. Copy number variations (CNVs) are a particular subtype of SV mainly represented by deletions and duplications. SVs are typically described as single events, although more complex scenarios involving combinations of SV types exist.\n\n\n\nOn completion of the tutorial, you can expect to be able to:\n• Call structural variants using standard tools\n• Visualise structural variants using standard tools\n• Call structural variants from long read data\n• Use bedtools to do regional comparisons over genomic co-ordinates\n\n\n\nThis tutorial comprises the following sections:\n1. Looking at structural variants in VCF\n2. Calling structural variants\n3. Structural variants from long reads\n4. Bedtools\n\n\n\nThis tutorial was written by Jacqui Keane based on material from Thomas Keane.\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files. To get started, open a new terminal window and type the command below:\ncd /home/manager/course_data/structural_variation/data\n\n\n\nThis tutorial requires that you have breakdancer, lumpy, minimap2, sniffles, bedtools and igv installed on your computer.\nThese are already installed on the virtual machine you are using. To check that these are installed, run the following commands:\nbreakdancer-max -h\ndysgu --help\nminimap2 --help\nsniffles --help\nbedtools --help\nThis should return the help message for software breakdancer, dysgu, minimap2, sniffles and bedtools respectively.\nIf after this course you would like to download and install this software the instructions can be found at the links below, alternatively we recommend bioconda for the installation and management of your bioinformatics software.\n• The breakdancer website\n• The dysgu github page\n• The minimap2 website\n• The sniffles website\n• The bedtools website\nTo get started with the tutorial, go to the first section: Looking at structural variants in VCF"
  },
  {
    "objectID": "course_modules/Module4/module4_exercises.html#looking-at-structural-variants-in-vcf",
    "href": "course_modules/Module4/module4_exercises.html#looking-at-structural-variants-in-vcf",
    "title": "Exercises",
    "section": "2 Looking at Structural Variants in VCF",
    "text": "2 Looking at Structural Variants in VCF\nStructural variants can be stored in VCF files. In this part of the tutorial, we will look at how these are represented in a VCF file.\nFirst, check you are in the correct directory:\npwd\nIt should display something like: /home/manager/course_data/structural_variation/data\nNavigate to the exercise1 directory:\ncd exercise1\nThere is a VCF file called ERR1015121.vcf that was produced using the Lumpy SV calling software. Look at the VCF file using the less command and answer the questions that follow:\nless ERR1015121.vcf\n\n2.1 Excercises\n1. What does the CIPOS format tag indicate?\n2. What does the PE tag indicate?\n3. What tag is used to describe an inversion event?\n4. What tag is used to describe a duplication event?\n5. How many deletions were called in total? (Hint: DEL is the info field for a deletion. The -c option of the grep command can be used to return a count of matches.)\n6. What type of event is predicted at IV:437148? What is the length of the SV? How many paired-end reads and split-reads support this SV variant call?\n7. What is the total number of SV calls predicted on the IV chromosome?\nNow continue to the next section of the tutorial: Calling structural variants"
  },
  {
    "objectID": "course_modules/Module4/module4_exercises.html#calling-structural-variants",
    "href": "course_modules/Module4/module4_exercises.html#calling-structural-variants",
    "title": "Exercises",
    "section": "3 Calling Structural Variants",
    "text": "3 Calling Structural Variants\nThere are several software tools available for calling structural variants. We will use two callers in this part of the tutorial, breakdancer and lumpy\n\n3.1 Breakdancer\nBreakDancer predicts five types of structural variants: insertions (INS), deletions (DEL), inversions (INV), inter-chromosomal translocations (CTX) and intra-chromosomal translocations (ITX) from next-generation short paired-end sequencing reads using read pairs that are mapped with unexpected separation distances or orientation.\nNavigate to the exercise2 directory:\ncd ../exercise2\nls\nWe will use the Breakdancer software package to call structural variants on a yeast sample that was paired-end sequenced on the illumina HiSeq 2000. Breakdancer first needs to examine the BAM file to get information on the fragment size distribution for each sequencing library contained in the BAM file.\nThe breakdancer.config file has information about the sequencing library fragment size distribution. Use the cat command to print the contents of the breakdancer.config file.\nQ What is the mean and standard deviation of the fragment size?\nRun the breakdancer SV caller using the command:\nbreakdancer-max breakdancer.config &gt; ERR1015121.breakdancer.out\nLook at the output of Breakdancer.\nhead ERR1015121.breakdancer.out\nNote that the output from Breakdancer is NOT VCF format, instead it is a simple text format with one line per SV event.\n\n\n3.1.1 Exercises\n1. What type of SV event is predicted at position III:83065?\n2. What is the size of this SV?\n3. What is the score of this SV?\n4. What type of SV event is predicted at position II:258766?\n5. Convert the output of breakdancer into BED format\n\n\n3.2 Inspecting SVs with IGV\nThe BED format is explained here: https://genome.ucsc.edu/FAQ/FAQformat.html#format1\nTo complete this task, create a command that:\n1. Extracts all the deletions from the breakdancer.out file (Hint: use grep)\n2. Prints columns: 1, 2, 5, 7, and 9 to create a BED file with columns: chromosome, start, end, name, and score. (Hint: use awk to do this, e.g. awk ’{print $1”*“$2}’)**\n3. Print the resulting bed output into a file called: breakdancer.dels.bed\nNow we will open the IGV genome browser and inspect some of the predicted structural variants. To do this type:\nigv\nOpen the reference genome: Go to ’ Genomes -&gt; Load Genome From Server… ’ and select “S. cerevisiae (SacCer3)”.\nLoad the BAM file: Go to ’ File -&gt; Load from File… ‘. Select the “ERR1015121.bam” BAM file and click’ Open ’.\nLoad the BED file for the deletion calls that you created in the exercise 5 above: Go to ’ File -&gt; Load from File… ‘. Select the “breakdancer.dels.bed” BED file and click’Open’.\n\n\n3.2.1 Exercises\nUsing the navigation bar, go to region II:258,500-266,700.\n1. Can you see the structural variant? What type of structural variant is it? (Hint: you may need to zoom out a little to see the full structural variant).\n2. Can you see any evidence to support this SV call?\n3. Can you estimate the size of the SV? The VCF in the exercise1 directory was produced by another structural variant caller on the same sample as this exercise.\n4. Load the exercise1/ERR1015121.vcf VCF into IGV also (File - Load from file, and select ERR1015121.vcf in the exercise 1 directory).\n5. Was the structural variant at II:258766 also called by the other structural variant software (lumpy)? If so, what was the predicted size? Using the navigation bar, go to to region II:508,064-511,840.\n6. Is there a SV deletion called in this region by either SV caller?\n7. Is there any read support for a SV deletion in this region? If so, how many read pairs could support the deletion call (Hint: change the IGV view to squished and View as pairs to see any inconsistently aligned read pairs).\n\n\n3.3 Dysgu\nWe will use the Dysgu (pronounced duss-key) software package (https://github.com/kcleal/dysgu) to call structural variants on a yeast sample that was paired-end sequenced on the Illumina Hiseq 2000.\nDysgu is designed to take BAM files that have been aligned with BWA-mem.\nNavigate to the exercise 3 directory:\ncd ../exercise3\nls\nCheck that there is a BAM file called ERR1015069.bam and an index file ERR1015069.bam.bai in the directory. The sequence data has already been mapped with bwa mem and the results are stored in ERR1015069.bam.\nTo call SVs, a sorted and indexed .bam/cram is needed plus an indexed reference genome (fasta format). Also a working directory must be provided to store temporary files.\ndysgu run ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa temp ERR1015069.bam &gt; ERR1015069.vcf\nDysgu is a multi-stage pipeline\n\nThe first stage of the “run” pipeline is to separate SV-associated reads - split/discordant reads, and reads with a soft-clip &gt;= clip_length (15 bp by default)\nThe next stage of the pipeline is to call and genotype SVs using the reads from the first stage. The run command above combines both of these stages together.\n\n1. What was the total number of SVs identified? How many PASS SVs were identified by Dysgu? Why did the rest of the SVs fail?\n2. What type of SV event occurs at position IV:384221? What is the length of the SV event? What is the genotype quality?\n3. What type of SV event occurs at position XV:31115? What is the length of the SV event? What is the probability of the structural variant?\nCongratulations, you have sucessfully called structural variants from some NGS data. Now continue to the next section of the tutorial: Calling structural variants from long reads"
  },
  {
    "objectID": "course_modules/Module4/module4_exercises.html#calling-structural-variants-from-long-reads",
    "href": "course_modules/Module4/module4_exercises.html#calling-structural-variants-from-long-reads",
    "title": "Exercises",
    "section": "4 Calling Structural Variants from Long Reads",
    "text": "4 Calling Structural Variants from Long Reads\nIn this part of the tutorial we will use long read data to identify structural variants using the SV caller Sniffles.\nFirst navigate to the exercise4 directory:\ncd ../exercise4\nls\n\n4.1 Introducing the dataset\nWe will use data from a Saccharomyces cerevisiae strain (YPS128) that was sequenced at the Wellcome Sanger Institute and deposited in the ENA (Project: PRJEB7245, sample: SAMEA2757770, analysis: ERZ448241).\nThe sequencing reads are contained in a fastq file: YPS128.filtered_subreads.10x.fastq.gz\nThe reference genome is in the ../ref directory in a fasta file: Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa\n\n\n4.2 Align the data\nBefore you can use Sniffles to call SVs, it is very important that the reads are aligned with an aligner suitable for long reads. The software minimap2 is a long-read aligner designed to align PacBio or Oxford Nanopore (standard and ultra-long) to a reference genome. You can find the usage of minimap2 by typing:\nminimap2\nUse minimap2 to align the reads and send the output to a SAM file called YPS128.filtered_sub- reads.10x.sam\nNote: use the -t option to use multiple threads in parallel (this will increase the speed of the alignment by using more than one CPU core, I suggest using 2).\nAlso look at the -x option. Convert the SAM file to BAM.\nSort the BAM file and produce a sorted BAM file called YPS128.filtered_subreads.10X.sorted.bam.\nHint: Use samtools sort.\n\n\n4.3 Call structural variants\nUse samtools calmd to calculates MD and NM tags. This enables variant calling without requiring access to the entire original reference.\nsamtools calmd -b YPS128.filtered_subreads.10x.sorted.bam ../ref/ Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa &gt; YPS128.filtered_subreads.10x.fastq.sorted.calmd.bam\nFinally, use samtools to index this BAM file.\nHint: Use samtools index\nSniffles is a structural variation (SV) caller that is designed for long reads (Pacbio or Oxford Nanopore). It detects all types of SVs (10bp+) using evidence from split-read alignments, high- mismatch regions, and coverage analysis. Sniffles takes the BAM file as input and outputs VCF.\nTo find the usage for Sniffles, type:\nsniffles\nUsing the default parameters, call SVs with Sniffles and output the results to a VCF file called YPS128.10x.vcf.\nHint: You don’t need to change any of the default parameters, but you will need to work out how to provide the input BAM file and specify the output VCF file. The documentation on sniffles is here : https://github.com/fritzsedlazeck/Sniffles/wiki/Parameter\n\n\n4.4 Inspecting SVs with IGV\nOpen IGV:\nigv\nOpen the reference genome ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa and load the BAM file YPS128.filtered_subreads.10x.fastq.sorted.calmd.bam and the VCF file YPS128.filtered_subreads.10x.vcf.\nNow answer the questions that follow using either the command line or IGV.\n1. What sort of SV was called at on chromosome ‘XV’ at position 854272?\n2. What is the length of the SV?\n3. How many reads are supporting the SV?\n4. What sort of SV was called at on chromosome ‘XI’ at position 74608?\n5. What is the length of the SV?\n6. How many reads are supporting the SV?\n7. How many inversions were called in the VCF? Note inversions are denoted by the type ‘INV’.\n8. How many duplications were called in the VCF? Note duplications are denoted by the type ‘DUP’.\nNow continue to the next section of the tutorial: bedtools."
  },
  {
    "objectID": "course_modules/Module4/module4_exercises.html#bedtools",
    "href": "course_modules/Module4/module4_exercises.html#bedtools",
    "title": "Exercises",
    "section": "5 Bedtools",
    "text": "5 Bedtools\nBedtools is an extremely useful tool for doing regional comparisons over genomic coordinates. It has many commands for doing region based comparisons with BAM, VCF, GFF, BED file formats.\nTo see the list of commands available, on the command line type:\nbedtools\nNavigate to the exercise5 directory:\ncd ../exercise5\nls\nIn this directory, there are two VCF files and the yeast genome annotation in GFF3 format Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3.\n\n5.1 bedtools intersect\n\n\n\n(Credit to Aaron Quinlan for original source of figure: http://quinlanlab.org/tutorials/bedtool- s/bedtools.html)\n\n\nGiven two sets of genomic features, the bedtools intersect command can be used to determine whether or not any of the features in the two sets “overlap” with one another. For the intersect command, the -a and -b parameters are used to denote the input files A and B.\nFor example, to find out the overlap between the SVs in ERR1015069.dels.vcf and the annotated region of the yeast genome try\nThis command reports the variant in the file ERR1015069.dels.vcf every time it overlaps with a feature in Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3.\nTherefore if a variant overlaps more than one feature it will be reported more than once. To report the unique set of variants use:\nbedtools intersect -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\nbedtools intersect -u -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1- 1.82.genes.gff3\nThe default is to report overlaps between features in A and B so long as at least one base pair of overlap exists. However, the -f option allows you to specify what fraction of each feature in A should be overlapped by a feature in B before it is reported.\nTo specify a more strict intersect and require at least 25% of overlap of the SV with the genes use the command:\nbedtools intersect -u -f 0.25 -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\nThe bedtools intersect command can also be used to determine how many SVs overlap between two VCF files. For more information about bedtools intersect see the help:\nbedtools intersect -h\n\n\n5.1.1 Exercises\n1. How many SVs found in ERR1015069.dels.vcf overlap with a gene? (Hint: Use bedtools intersect command)\n2. How many SVs found in ERR1015069.dels.vcf do not overlap with a gene? (Hint: note the -v parameter to bedtools intersect)\n3. How many SVs found in ERR1015069.dels.vcf overlap with a more strict definition of 50%?\n4. How many features does the deletion at VII:811446 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect.\n5. How many features does the deletion at XII:650823 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect.\n\n\n5.2 bedtools closest\nSimilar to intersect, bedtools closest searches for overlapping features in A and B. In the event that no feature in B overlaps the current feature in A, closest will report the nearest (that is, least genomic distance from the start or end of A) feature in B.\nAn example of the usage of bedtools closest is:\nbedtools closest -d -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\nThis command will list all the features in the file Saccharomyces_cerevisiae.R64-1- 1.82.genes.gff3 that are closest to each of the variants in ERR1015069.dels.vcf. The -d option means that in addition to the closest feature in Saccharomyces_cerevisiae.R64-1- 1.82.genes.gff3, the distance to the variant in ERR1015069.dels.vcf will be reported as an extra column. The reported distance for any overlapping features will be 0.\nFor example, to find the closest gene to the variant found at position 43018 on chromosome XV, try:\nbedtools closest -d -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3| grep XV | grep 43018\nFor more information about bedtools closest see the help:\nbedtools closest -h\n\n\n5.3 Exercises\n6. What is the closest gene to the structural variant at IV:384220 in ERR1015069.dels.vcf?\n7. How many SVs overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf?\n8. How many SVs have a 50% reciprocal overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf (Hint: first find the option for reciprocal overlap by typing: bedtools intersect -h)\nCongratulations, you have reached the end of the tutorial"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Introduction",
    "section": "",
    "text": "The materials provided in this repository are FREE to use. This work is licensed under a Creative Commons Attribution 4.0 International License. Reuse is encouraged with acknowledgement.\n\nAll materials to be added in markdown (with extension .qmd), except for recorded lectures.\nThere are no slides on this website. Instead, you will find instructions on how to generate slides from the markdown content."
  },
  {
    "objectID": "template_course.html",
    "href": "template_course.html",
    "title": "template_course",
    "section": "",
    "text": "Course Overview\n- Course Title\n- Course Description: Short and extended versions\n- Course format \n- Target Audience: (e.g., clinicians, researchers, students)\n- Prerequisites: Knowledge or skills expected\n \nLearning Outcomes\n- Clearly defined, measurable outcomes using Blooms action verbs \n- Mapped to competencies if relevant\nAssessment\n- Formative assessments (quizzes, exercises, peer review)\n- Summative assessments (final project, test, reflection)\n- Rubrics and grading templates for instructors \n- Submission methods/platform\n- Model answers or guides \n \nTeaching Platform & Tech Tools\n- Platforms guide: e.g., GitHub Classroom, Google classroom, Moodle, Zoom etc \n- Accounts/Access setup instructions\n- Tools used in communication and teaching: e.g., Zoom, Slack, BLAST, RStudio\n- Instructions for setting up any required environments (Docker, Conda, virtual machines)\n \nResources & References\n- Core readings (articles, textbooks)\n- Supplementary materials (websites, tools, videos)\n- Glossary of terms\n- FAQs and Troubleshooting guide (tech and teaching)\n \nInstructor & Facilitator Guide - important to include\n- Adult learning strategies [see T3 resource] \n- Teaching tips for each module\n- Common learner questions/issues \n- Discussion prompts\n- Inclusivity guidance [see T3 resource, UDL] \n \n \nEvaluation & Feedback Templates\n- Evaluation questions [what do we want to know] \n- Data collection: learner feedback forms, instructor reflection log/observation structured notes, ost course survey etc)\n- Plan for data analysis \n- Plan for results dissemination"
  },
  {
    "objectID": "course_modules/Module4/module4_solutions.html",
    "href": "course_modules/Module4/module4_solutions.html",
    "title": "Solutions",
    "section": "",
    "text": "No questions in this section."
  },
  {
    "objectID": "course_modules/Module4/module4_solutions.html#structural-variation-calling---solutions",
    "href": "course_modules/Module4/module4_solutions.html#structural-variation-calling---solutions",
    "title": "Solutions",
    "section": "",
    "text": "No questions in this section."
  },
  {
    "objectID": "course_modules/Module4/module4_solutions.html#looking-at-structural-variants-in-vcf",
    "href": "course_modules/Module4/module4_solutions.html#looking-at-structural-variants-in-vcf",
    "title": "Solutions",
    "section": "2 Looking at Structural Variants in VCF",
    "text": "2 Looking at Structural Variants in VCF\n\n2.1 Exercises\n1. What does the CIPOS format tag indicate? Confidence interval around POS for imprecise variants\n2. What does the PE tag indicate? Number of paired-end reads supporting the variant across all samples\n3. What tag is used to describe an inversion event? INV\n4. What tag is used to describe a duplication event? DUP\n5. How many deletions were called in total? (Hint: DEL is the info field for a deletion. The -c option of the grep command can be used to return a count of matches.), try\ngrep -c \"&lt;DEL&gt;\" ERR1015121.vcf\n6. What type of event is predicted at IV:437148? What is the length of the SV? How many paired-end reads and split-reads support this SV variant call? Deletion -370 20 PE 21 split\ngrep \"437148\" ERR1015121.vcf\n7. What is the total number of SV calls predicted on the IV chromosome? 10, try\ngrep -c \"^IV\" ERR1015121.vcf"
  },
  {
    "objectID": "course_modules/Module4/module4_solutions.html#calling-structural-variants",
    "href": "course_modules/Module4/module4_solutions.html#calling-structural-variants",
    "title": "Solutions",
    "section": "3 Calling Structural Variants",
    "text": "3 Calling Structural Variants\nQ: mean=454.87 std=86.29\n\n3.1 Breakdancer\n\n\n3.1.1 Exercises\ngrep \"83065\" ERR1015121.breakdancer.out\n1. Inversion\n2. -116,\n3. 42\ngrep \"258766\" ERR1015121.breakdancer.out\n4. Deletion (7325, 99)\n5. grep DEL | awk OFS= breakdancer.dels.bed | awk '{print $1\"\\t\"$2\"\\t\"$5\"\\t\"$7\"\\t\"$9}' &gt; breakdancer.dels.bed"
  },
  {
    "objectID": "course_modules/Module4/module4_solutions.html#calling-structural-variants-from-long-reads",
    "href": "course_modules/Module4/module4_solutions.html#calling-structural-variants-from-long-reads",
    "title": "Solutions",
    "section": "4 Calling Structural Variants from Long Reads",
    "text": "4 Calling Structural Variants from Long Reads\n\n3.2 Inspecting SVs with IGV\n\n\n3.2.1 Exercises\n1. Yes, a deletion (view as paired, sort by insert size, squish).\n2. There are very few reads mapping, the reads that are mapped are of low mapQ and it has a SV score = 99\n3. Size estimate? ~7.5k\nWas the deletion at II:258766 also called by the other structural variant software and was the predicted size?\n5. Yes, SVTYPE=DEL, SVLEN=-7438\n6. DEL called by breakdancer (score=59). Not found by other caller Lumpy.\n7. Yes, 2 reads support (red).\n\n\n3.3 Lumpy\n\n\n3.3.1 Exercises\n1. The -F option in samtools view excludes reads matching the specified flag\n2. reads in proper pair | read unmapped | mate unmapped | not primary alignment | PCR optical duplicate\n3. Deletion -625\n4. Deletion -369\n\n\n4 Calling Structural Variants from Long Reads\n4.0.1 Align the reads with minimap and convert to bam\nminimap2 -t 2 -x map-pb -a ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa.gz YPS128.filtered_subreads.10x.fastq.gz | samtools view -b -o YPS128.filtered_subreads.10x.bam -\n4.0.2 Sort the bam\nsamtools sort -T temp -o YPS128.filtered_subreads.10x.sorted.bam YPS128.filtered_subreads.10x.bam\nsamtools calmd -b YPS128.filtered_subreads.10x.sorted.bam ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa.gz &gt; YPS128.filtered_subreads.10x.sorted.calmd.bam\n4.0.3 Index the sorted bam\nsamtools index YPS128.filtered_subreads.10x.sorted.calmd.bam\n\n\n5 Bedtools\n\n\n4.0.4 Call SVs with sniffles\nsniffles -m YPS128.filtered_subreads.10x.sorted.calmd.bam -v YPS128.filtered_subreads.10x.vcf\n\n\n4.0.5 Exercises\n1. What sort of SV was called at on chromosome ‘XV’ at position 854271? __Deletion_\n2. What is the length of the SV? 345\n3. How many reads are supporting the SV? 17 (RE tag)\n4. What sort of SV was called at on chromosome ‘XI’ at position 74608? __Insertion_\n5. What is the length of the SV? 358\n6. How many reads are supporting the SV? 15\n7. How many inversions were called in the VCF? Note inversions are denoted by the type ‘INV’.\nNone - no inversions were called\n8. How many duplications were called in the VCF? Note duplications are denoted by the type ‘DUP’. 2\n\n\n5 Bedtools\n\n\n5.1 Exercises\n1. How many SVs found in ERR1015069.dels.vcf overlap with a gene? (Hint: Use bedtools intersect command) 18, try (note the -u parameter is required to get the unique number of SVs)\nbedtools intersect -u -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | wc -l\n2. How many SVs found in ERR1015069.dels.vcf do not overlap with a gene? (Hint: note the -v parameter to bedtools intersect) 9, try\nbedtools intersect -v -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | wc -l\n3. How many SVs found in ERR1015069.dels.vcf overlap with a more strict definition of 50%?\n14, try\nbedtools intersect -u -f 0.5 -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | wc -l\n4. How many features does the deletion at VII:811446 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect.\nbedtools intersect -wb -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | grep 811446\n4 features, all of them are protein coding genes (biotype=protein_coding)\n5. How many features does the deletion at XII:650823 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect.\nbedtools intersect -wb -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 | grep 811446\n2 features, all of them are protein coding genes (biotype=protein_coding)\n6. What is the closest gene to the structural variant at IV:384220 in ERR1015069.dels.vcf?\nYDL037C, try\nbedtools closest -d -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3| grep IV | grep 384220\n5. How many SVs overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf? 27, try\nbedtools intersect -u -a ERR1015069.dels.vcf -b ERR1015121.dels.vcf | wc -l\n6. How many SVs have a 90% reciprocal overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf (Hint: first find the option for reciprocal overlap by typing: bedtools intersect -h) 24, try\nbedtools intersect -u -r -f 0.9 -a ERR1015069.dels.vcf -b ERR1015121.dels.vcf | wc -l"
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html",
    "href": "course_modules/Module4/module4_manual.html",
    "title": "Manual",
    "section": "",
    "text": "Structural Variation Detection and Interpretation Manual\nInstructor: Thomas Keane, EMBL-EBI"
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html#introduction-to-genomic-structural-variation",
    "href": "course_modules/Module4/module4_manual.html#introduction-to-genomic-structural-variation",
    "title": "Manual",
    "section": "1 Introduction to Genomic Structural Variation",
    "text": "1 Introduction to Genomic Structural Variation\n\n1.1 What is Structural Variation?\nStructural variations (SVs) refer to large-scale alterations of the chromosome structure. These rearrangements contribute to genetic diversity and evolution, leading to new gene formation, changes in gene function, and phenotypic diversity. SVs are often implicated in Mendelian diseases, complex traits such as behavior, and other genomic disorders.\nTypes of SVs:\nInsertions\nDeletions\nCopy number variations (CNVs)\nInversions\nTranslocations\nComplex events (combinations of above)\nA breakpoint is defined as a pair of bases that are adjacent in the experimental genome but not in the reference genome.\n\n\n\nGenomic Structural Variation"
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html#section",
    "href": "course_modules/Module4/module4_manual.html#section",
    "title": "Manual",
    "section": "",
    "text": "2 Methods for Detecting Structural Variations"
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html#experimental-techniques",
    "href": "course_modules/Module4/module4_manual.html#experimental-techniques",
    "title": "Manual",
    "section": "2.1 Experimental Techniques",
    "text": "2.1 Experimental Techniques\n\nChromosome Banding: Staining of chromosomes to observe large deletions, duplications, or translocations.\nFISH (Fluorescence In Situ Hybridization): Uses fluorescent probes to visualize chromosomal segments.\nMicroarrays:\n\nArray CGH: Detects copy number differences.\nSNP Arrays: Identify changes in allelic ratios.\n\nWhole Genome Sequencing (WGS): Detection of breakpoints via discordant paired-end reads.\nMate-pair Sequencing: Large insert libraries to detect breakpoints within repetitive regions.\nThird-generation Sequencing: Long-read sequencing technologies (e.g., PacBio, Oxford Nanopore) to detect SVs by aligning large DNA fragments.\n\n\n3: SV Types and Analysis by NGS\n\n\n3.1 Retrotransposition\nTransposons are DNA elements that move within the genome, heavily influencing genome organization and evolution.\nClasses:\n\nClass 1: RNA-mediated (e.g., L1, Alu)\nClass 2: DNA-mediated\nImpact: Disruption of gene expression and reorganization of the genome.\n\n\n\n3.2 Evidence for SV Detection\nSources of Evidence:\n\nRead Pairs: Anomalies in expected distance or orientation.\nSplit Reads: Reads that map discontinuously across a breakpoint.\nRead Depth: Changes in sequencing coverage indicating duplications or deletions.\n\n\n\n\nSources of Evidence for SV Detection"
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html#sv-detection-tools",
    "href": "course_modules/Module4/module4_manual.html#sv-detection-tools",
    "title": "Manual",
    "section": "4 SV Detection Tools",
    "text": "4 SV Detection Tools\n\n4.1 Breakdancer\nInput: BAM file\nDetects: Deletions, insertions, inversions, translocations\nMethod: Cluster anomalous read pairs\n\n\n4.2 Lumpy\nIntegrates multiple alignment signals (read pairs, split reads, depth)\nClusters and integrates evidence probabilistically\n\n\n4.3 Sniffles\nDesigned for long reads\nHandles complex SVs and noisy data\n\n\n\n5 VCF Format for SVs\nVCF Fields:\n\nALT: Alternative allele information\nINFO: Additional annotations (e.g., CIEND)\nFORMAT: Genotype information per sample\nExercise: Write a VCF entry for an Alu insertion at chromosome 7, position 125467, of 258bp length with a breakpoint confidence interval of +/-20bp."
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html#visualization-of-structural-variants",
    "href": "course_modules/Module4/module4_manual.html#visualization-of-structural-variants",
    "title": "Manual",
    "section": "6: Visualization of Structural Variants",
    "text": "6: Visualization of Structural Variants\nVisualization is crucial due to the complexity of SVs. IGV (Integrative Genomics Viewer) is a popular tool.\nTips for Viewing in IGV:\n\nDeletions: Squished view\nInsertions: Group alignments by read strand\nSoft clipped bases: Turn on for breakpoint visualization\n\n\n\n\nIGV Example - Deletion"
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html#long-read-sequencing-and-svs",
    "href": "course_modules/Module4/module4_manual.html#long-read-sequencing-and-svs",
    "title": "Manual",
    "section": "7: Long-read Sequencing and SVs",
    "text": "7: Long-read Sequencing and SVs"
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html#platforms",
    "href": "course_modules/Module4/module4_manual.html#platforms",
    "title": "Manual",
    "section": "7.1 Platforms",
    "text": "7.1 Platforms\n\nOxford Nanopore\nPacific Biosciences\n\nAdvantages: Ability to span breakpoints with single reads.\nChallenges:\n\nHigh error rate (5-20%)\nComplex alignments\nSolution:\n\nUse aligners like NGMLR designed for long reads.\n\n7.2 SV Detection\nSniffles: Detects simple and complex SVs using long-read data."
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html#complex-structural-variants",
    "href": "course_modules/Module4/module4_manual.html#complex-structural-variants",
    "title": "Manual",
    "section": "8: Complex Structural Variants",
    "text": "8: Complex Structural Variants\nExamples:\n\nDel+Ins (deletion with insertion)\nInv+Del (inversion with deletion)\nInversions within gains\nLong reads greatly aid in identifying complex SV structures.\n\n\n\n\n\nComplex SV Example"
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html#evaluating-sv-call-quality",
    "href": "course_modules/Module4/module4_manual.html#evaluating-sv-call-quality",
    "title": "Manual",
    "section": "9: Evaluating SV Call Quality",
    "text": "9: Evaluating SV Call Quality"
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html#sensitivity-and-specificity",
    "href": "course_modules/Module4/module4_manual.html#sensitivity-and-specificity",
    "title": "Manual",
    "section": "9.1 Sensitivity and Specificity",
    "text": "9.1 Sensitivity and Specificity\nDefinitions:\nSensitivity: Ability to detect true positives\nSpecificity: Ability to avoid false positives\nValidation Strategies:\nCompare against known SV datasets\nPCR validation\nUse of ROC curves\n10: Practical Exercises"
  },
  {
    "objectID": "course_modules/Module4/module4_manual.html#hands-on-exercises",
    "href": "course_modules/Module4/module4_manual.html#hands-on-exercises",
    "title": "Manual",
    "section": "10.1 Hands-on Exercises",
    "text": "10.1 Hands-on Exercises\nAnalyze VCF files output by Lumpy\nUse Breakdancer to detect SVs on yeast data\nUse Sniffles for PacBio data\nCompare SVs using BEDtools\n\n10.2 Practice Questions\n\nWhat information does CIEND in VCF describe?\nHow do mate-pair libraries improve SV detection?\nName two visualization challenges specific to SVs."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/vcfs.html",
    "href": "course_modules/Module4/Notebooks/vcfs.html",
    "title": "Looking at Structural Variants in VCF",
    "section": "",
    "text": "Structural variants can be stored in VCF files. In this part of the tutorial, we will look at how these are represented in a VCF file.\nFirst, check you are in the correct directory:\npwd\nIt should display something like:\n/home/manager/course_data/structural_variation/data\nNavigate to the exercise1 directory:\ncd exercise1\nThere is a VCF file called ERR1015121.vcf that was produced using the Lumpy SV calling software. Look at the VCF file using the less command and answer the questions that follow:\nless ERR1015121.vcf"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/vcfs.html#excercises",
    "href": "course_modules/Module4/Notebooks/vcfs.html#excercises",
    "title": "Looking at Structural Variants in VCF",
    "section": "Excercises",
    "text": "Excercises\n\nWhat does the CIPOS format tag indicate?\nWhat does the PE tag indicate?\nWhat tag is used to describe an inversion event?\nWhat tag is used to describe a duplication event?\nHow many deletions were called in total? (Hint: DEL is the info field for a deletion. The -c option of the grep command can be used to return a count of matches.)\nWhat type of event is predicted at IV:437148? What is the length of the SV? How many paired-end reads and split-reads support this SV variant call?\nWhat is the total number of SV calls predicted on the IV chromosome?\n\nNow continue to the next section of the tutorial: Calling structural variants"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/long-reads.html",
    "href": "course_modules/Module4/Notebooks/long-reads.html",
    "title": "Calling Structural Variants from Long Reads",
    "section": "",
    "text": "In this part of the tutorial we will use long read data to identify structural variants using the SV caller Sniffles.\nFirst navigate to the exercise4 directory:\ncd ../exercise4\nls"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/long-reads.html#introducing-the-dataset",
    "href": "course_modules/Module4/Notebooks/long-reads.html#introducing-the-dataset",
    "title": "Calling Structural Variants from Long Reads",
    "section": "Introducing the dataset",
    "text": "Introducing the dataset\nWe will use data from a Saccharomyces cerevisiae strain (YPS128) that was sequenced at the Wellcome Sanger Institute and deposited in the ENA (Project: PRJEB7245, sample: SAMEA2757770, analysis: ERZ448241).\nThe sequencing reads are contained in a fastq file:\nYPS128.filtered_subreads.10x.fastq.gz\nThe reference genome is in the ../ref directory in a fasta file:\nSaccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/long-reads.html#align-the-data",
    "href": "course_modules/Module4/Notebooks/long-reads.html#align-the-data",
    "title": "Calling Structural Variants from Long Reads",
    "section": "Align the data",
    "text": "Align the data\nBefore you can use Sniffles to call SVs, it is very important that the reads are aligned with an aligner suitable for long reads.\nThe software minimap2 is a long-read aligner designed to align PacBio or Oxford Nanopore (standard and ultra-long) to a reference genome.\nYou can find the usage of minimap2 by typing:\n\nminimap2\n\nUse minimap2 to align the reads and send the output to a SAM file called YPS128.filtered_subreads.10x.sam\nNote: use the -t option to use multiple threads in parallel (this will increase the speed of the alignment by using more than one CPU core, I suggest using 2). Also look at the -x option.\nConvert the SAM file to BAM.\nSort the BAM file and produce a sorted BAM file called YPS128.filtered_subreads.10X.sorted.bam. Hint: Use samtools sort.\nUse samtools calmd to calculates MD and NM tags. This enables variant calling without requiring access to the entire original reference.\n\nsamtools calmd -b YPS128.filtered_subreads.10x.sorted.bam ../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa &gt; YPS128.filtered_subreads.10x.fastq.sorted.calmd.bam\n\nFinally, use samtools to index this BAM file. Hint Use samtools index"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/long-reads.html#call-structural-variants",
    "href": "course_modules/Module4/Notebooks/long-reads.html#call-structural-variants",
    "title": "Calling Structural Variants from Long Reads",
    "section": "Call structural variants",
    "text": "Call structural variants\nSniffles is a structural variation (SV) caller that is designed for long reads (Pacbio or Oxford Nanopore). It detects all types of SVs (10bp+) using evidence from split-read alignments, high-mismatch regions, and coverage analysis. Sniffles takes the BAM file as input and outputs VCF.\nTo find the usage for Sniffles, type:\n\nsniffles\n\nUsing the default parameters, call SVs with Sniffles and output the results to a VCF file called YPS128.10x.vcf.\nHint: You don’t need to change any of the default parameters, but you will need to work out how to provide the input BAM file and specify the output VCF file. The documentation on sniffles is here : https://github.com/fritzsedlazeck/Sniffles/wiki/Parameter."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/long-reads.html#inspecting-svs-with-igv",
    "href": "course_modules/Module4/Notebooks/long-reads.html#inspecting-svs-with-igv",
    "title": "Calling Structural Variants from Long Reads",
    "section": "Inspecting SVs with IGV",
    "text": "Inspecting SVs with IGV\nOpen IGV:\n\nigv\n\nOpen the reference genome ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa and load the BAM file YPS128.filtered_subreads.10x.fastq.sorted.calmd.bam and the VCF file YPS128.filtered_subreads.10x.vcf. Now answer the questions that follow using either the command line or IGV."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/long-reads.html#exercises",
    "href": "course_modules/Module4/Notebooks/long-reads.html#exercises",
    "title": "Calling Structural Variants from Long Reads",
    "section": "Exercises",
    "text": "Exercises\n\nWhat sort of SV was called at on chromosome ‘XV’ at position 854272?\nWhat is the length of the SV?\nHow many reads are supporting the SV?\nWhat sort of SV was called at on chromosome ‘XI’ at position 74608?\nWhat is the length of the SV?\nHow many reads are supporting the SV?\nHow many inversions were called in the VCF? Note inversions are denoted by the type ‘INV’.\nHow many duplications were called in the VCF? Note duplications are denoted by the type ‘DUP’.\n\nNow continue to the next section of the tutorial: bedtools"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/bedtools.html",
    "href": "course_modules/Module4/Notebooks/bedtools.html",
    "title": "Bedtools",
    "section": "",
    "text": "Bedtools is an extremely useful tool for doing regional comparisons over genomic co-ordinates. It has many commands for doing region based comparisons with BAM, VCF, GFF, BED file formats.\nTo see the list of commands available, on the command line type:\nbedtools\nNavigate to the exercise5 directory.\ncd ../exercise5\nls\nIn this directory, there are two VCF files and the yeast genome annotation in GFF3 format Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/bedtools.html#bedtools-intersect",
    "href": "course_modules/Module4/Notebooks/bedtools.html#bedtools-intersect",
    "title": "Bedtools",
    "section": "bedtools intersect",
    "text": "bedtools intersect\nGiven two sets of genomic features, the bedtools intersect command can be used to determine whether or not any of the features in the two sets “overlap” with one another. For the intersect command, the -a and -b parameters are used to denote the input files A and B.\n\n\n\nIGV - main window\n\n\n(Credit to Aaron Quinlan for original source of figure: http://quinlanlab.org/tutorials/bedtools/bedtools.html)\nFor example, to find out the overlap between the SVs in ERR1015069.dels.vcf and the annotated region of the yeast genome try\nThis command reports the variant in the file ERR1015069.dels.vcf every time it overlaps with a feature in Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3. Therefore if a variant overlaps more than one feature it will be reported more than once. To report the unique set of variants use:\n\nbedtools intersect -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\n\n\nbedtools intersect -u -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\n\nThe default is to report overlaps between features in A and B so long as at least one base pair of overlap exists. However, the -f option allows you to specify what fraction of each feature in A should be overlapped by a feature in B before it is reported.\nTo specify a more strict intersect and require at least 25% of overlap of the SV with the genes use the command:\n\nbedtools intersect -u -f 0.25 -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\n\nThe bedtools intersect command can also be used to determine how many SVs overlap between two VCF files. For more information about bedtools intersect see the help:\n\nbedtools intersect -h\n\n\nExercises\n\nHow many SVs found in ERR1015069.dels.vcf overlap with a gene? (Hint: Use bedtools intersect command)\nHow many SVs found in ERR1015069.dels.vcf do not overlap with a gene? (Hint: note the -v parameter to bedtools intersect)\nHow many SVs found in ERR1015069.dels.vcf overlap with a more strict definition of 50%?\nHow many features does the deletion at VII:811446 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect.\nHow many features does the deletion at XII:650823 overlap with? What type of genes? Note you will need to also use the -wb option in bedtools intersect."
  },
  {
    "objectID": "course_modules/Module4/Notebooks/bedtools.html#bedtools-closest",
    "href": "course_modules/Module4/Notebooks/bedtools.html#bedtools-closest",
    "title": "Bedtools",
    "section": "bedtools closest",
    "text": "bedtools closest\nSimilar to intersect, bedtools closest searches for overlapping features in A and B. In the event that no feature in B overlaps the current feature in A, closest will report the nearest (that is, least genomic distance from the start or end of A) feature in B.\nAn example of the usage of bedtools closest is:\n\nbedtools closest -d -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3\n\nThis command will list all the features in the file Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3 that are closest to each of the variants in ERR1015069.dels.vcf.\nThe -d option means that in addition to the closest feature in Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3, the distance to the variant in ERR1015069.dels.vcf will be reported as an extra column. The reported distance for any overlapping features will be 0.\nFor example, to find the closest gene to the variant found at position 43018 on chromosome XV, try\n\nbedtools closest -d -a ERR1015069.dels.vcf -b Saccharomyces_cerevisiae.R64-1-1.82.genes.gff3| grep XV | grep 43018 \n\nFor more information about bedtools closest see the help:\n\nbedtools closest -h"
  },
  {
    "objectID": "course_modules/Module4/Notebooks/bedtools.html#exercises-1",
    "href": "course_modules/Module4/Notebooks/bedtools.html#exercises-1",
    "title": "Bedtools",
    "section": "Exercises",
    "text": "Exercises\n\nWhat is the closest gene to the structural variant at IV:384220 in ERR1015069.dels.vcf?\nHow many SVs overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf?\nHow many SVs have a 50% reciprocal overlap between the two files ERR1015069.dels.vcf and ERR1015121.dels.vcf (Hint: first find the option for reciprocal overlap by typing: bedtools intersect -h)\n\nCongratulations, you have reached the end of the tutorial."
  },
  {
    "objectID": "course_modules/Module3/module3_exercises.html",
    "href": "course_modules/Module3/module3_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "1 Variant Calling\n\n\n1.1 Introduction\nVariant calling is the process of identifying differences between the reference genome and the samples that have been sequenced. These differences can be single nucleotide polymorphisms (SNPs), multi-nucleotide polymorphisms (MNPs) or small insertions and deletions (indels) and examples of each of these are shown below.\n\nFigure 1: SNPs and small insertions and deletions\n\n\n1.2 Learning outcomes\nOn completion of the tutorial, you can expect to be able to:\n• Perform variant calling (SNPs and indels) using standard tools\n• Assess the quality/confidence of a variant call\n• Filter variant calls to remove low quality/confidence calls\n• Perform variant calling across multiple samples\n• Visualise variants using standard tools\n• Annotate variants with consequence calls\n\n\n1.3 Tutorial sections\nThis tutorial comprises the following sections:\n\nPerforming variant calling\nFiltering variants\nMulti-sample variant calling\nVisualising variants\n\n\n\n1.4 Authors\nThis tutorial was written by Jacqui Keane based on material from Thomas Keane and Petr Danecek.\nThere is also an additional (optional) section: 5. Variant annotation\n\n\n1.5 Running the commands from this tutorial\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal on your computer and type the command below:\ncd ~/course_data/variant_calling/data\nNow you can follow the instructions in the tutorial from here.\n\n\n1.6 Let’s get started!\nThis tutorial assumes that you have samtools, bcftools and IGV installed on your computer. These are already installed on the VM you are using. To check that these are installed, you can run the following commands:\nsamtools --help\nbcftools --help\nigv\nThis should return the help message for samtools and bcftools. The final command should launch the genome viewer IGV. You can close the IGV software, we will use it later in this tutorial to visualise variants.\nTo get started with the tutorial, go to to the first section: Performing variant calling\n\n\n2 Performing Variant Calling\nWhen performing variant calling we need the aligned sequences in SAM, BAM or CRAM format and the reference genome that we want to call varants against.\nFirst, check you are in the correct directory.\npwd\nIt should display something like: /home/manager/course_data/variant_calling/data\n\n\n2.1 Assessing the input data\nTo list the files in the current directory, type\nls -lh\nThe listing shows aligned data for two mouse strains A/J and NZO (A_J.bam and NZO.bam) and the chromosome 19 of the mouse reference genome (GRCm38_68.19.fa).\nBefore performing variant calling, it is important to check the quality of the data that you will be working with. We have already seen how to do this in the QC and Data Formats and Read Alignment sessions. The commands would look like:\nsamtools stats -r GRCm38_68.19.fa A_J.bam &gt; A_J.stats\nsamtools stats -r GRCm38_68.19.fa NZO.bam &gt; NZO.stats\nplot-bamstats -r GRCm38_68.19.fa.gc -p A_J.graphs/ A_J.stats\nplot-bamstats -r GRCm38_68.19.fa.gc -p NZO.graphs/ NZO.stats\nYou do not need to run these QC checks on this data and for this we will assume that QC has already been performed and the data is of good quality.\n\n\n2.2 Generating pileup\nThe command samtools mpileup prints the read bases that align to each position in the reference genome. Type the command:\nsamtools mpileup -f GRCm38_68.19.fa A_J.bam | less -S\nEach line corresponds to a position on the genome.\nThe columns are: chromosome, position, reference base, read depth, read bases (dot . and comma , indicate match on the forward and on the reverse strand; ACGTN and acgtn a mismatch on the forward and the reverse strand) and the final column is the base qualities encoded into characters.\nThe caret symbol ^ marks the start of a read, the dollar sign $ the end of a read, deleted bases are represented by asterisk *.\n\n\n2.3 Generating genotype likelihoods and calling variants\nThis output can be used for a simple consensus calling. One rarely needs this type of output. Instead, for a more sophisticated variant calling method, see the next section.\n\n\n2.2.1 Exercises\nLook at the output from the mpileup command above and answer the following questions.\nQ1: What is the read depth at position 10001994? (Rather than scrolling to the position, use the substring searching capabilities of less: press /, then type 10001994 followed by enter to find the position.)\nQ2: What is the reference allele and the alternate allele at position 10001994?\nQ3: How many reads call the reference allele at position 10001994 and how many reads call the alternate allele at position 10001994?\n\n\n2.3 Generating genotype likelihoods and calling variants\nThe bcftools mpileup command can be used to generate genotype likelihoods. (Beware: the command mpileup is present in both samtools and bcftools, but in both they do different things. While samtools mpileup produces the text pileup output seen in the previous exercise, bcftools mpileup generates a VCF file with genotype likelihoods.)\nRun the following command (when done, press q to quit the viewing mode):\nbcftools mpileup -f GRCm38_68.19.fa A_J.bam | less -S\nThis generates an intermediate output which contains genotype likelihoods and other raw information necessary for variant calling. This output is usually streamed directly to the caller like this\nbcftools mpileup -f GRCm38_68.19.fa A_J.bam | bcftools call -m | less -S\nThe output above contains both variant and non-variant positions. Check the input/output options section of the bcftools call usage page and see if there is an option to print out only variant sites.\nThen construct a command to print out variant sites only:\nThe INFO and FORMAT fields of each entry tells us something about the data at the position in the genome. It consists of a set of key-value pairs with the tags being explained in the header of the VCF file (see the ##INFO and ##FORMAT lines in the header).\nWe can tell mpileup to add additional ##INFO and ##FORMAT information to the output. For example, we can ask it to add the FORMAT/AD tag which informs about the number of high-quality reads that support alleles listed in REF and ALT columns. The list of all available tags can be printed with the command:\nbcftools mpileup -a ?\nNow let’s run the variant calling again, this time adding the -a AD option. We will also add the -Ou option so that it streams a binary uncompressed BCF into call. This is to avoid the unnecessary CPU overhead of formatting the internal binary format to plain text VCF only to be immediately formatted back to the internal binary format again.\nbcftools mpileup -a AD -f GRCm38_68.19.fa A_J.bam -Ou | bcftools call -mv -o out.vcf\n\n\n2.4 Exercises\nLook at the content of the VCF file produced above and answers the questions that follow.\nless -S out.vcf\nQ1: What is the reference allele and the alternate allele at position 10001994?\nQ2: What is the total raw read depth at position 10001994?\nNote: This number may be different from the values we obtained earlier, because some low quality reads or bases might have been filtered previously.\nQ3: What is the number of high-quality reads supporting the SNP call at position 10001994? How many reads support the reference allele and how many support the alternate allele?\nHint: Look up the AD tag in the FORMAT column: the first value gives the number of reads calling the reference allelle and the second gives the number of reads calling the alternate alleles.\nQ4: What sort of event is happening at position 10003649?\nCongratulations, you have sucessfully called variants from some NGS data. Now continue to the next section of the tutorial: filtering variants.\n\n\n3 Variant Filtering\nIn the next series of commands we will learn how to extract information from VCFs and how to filter the raw calls. We will use the bcftools commands again. Most of the commands accept the -i, – include and -e, –exclude options https://samtools.github.io/bcftools/bcftools.html# expressions which will be useful when filtering using fixed thresholds. We will estimate the quality of the callset by calculating the ratio of transitions and transversions https://en.wikipedia.org/wiki/Transversion.\nWhen drafting commands, it is best to build them gradually. This prevents errors and allows you to verify that they work as expected. Let’s start with printing a simple list of positions from the VCF using the bcftools query command https://samtools.github.io/bcftools/bcftools.html#query and pipe through the head command to limit the printed output to the first few lines:\nbcftools query --format 'POS=%POS\\n' out.vcf | head\nAs you can see, the command expanded the formatting expression POS=%POSin the following way: for each VCF record the string POS= was copied verbatim, the string %POS was replaced by the VCF coordinate stored in the POS column, and then the newline character ended each line. (Without the newline character, positions from the entire VCF would be printed on a single line.)\nNow add the reference and the alternate allele to the output. They are stored in the REF and ALT column in the VCF, and let’s separate them by a comma:\nbcftools query -f'%POS %REF,%ALT\\n' out.vcf | head\nIn the next step add the quality (%QUAL), genotype (%GT) and sequencing depth (%AD) to the output. Note that FORMAT tags must be enclosed within square brackets […] to iterate over all samples in the VCF. (Check the Extracting per-sample tags section in the manual https://samtools.github.io/bcftools/howtos/query.html for a more detailed explanation why the square brackets are needed.)\nbcftools query -f'%POS %QUAL [%GT %AD] %REF %ALT\\n' out.vcf | head\nNow we are able to quickly extract important information from the VCFs. Now let’s filter rows with QUAL smaller than 30 by adding the filtering expression –exclude ‘QUAL&lt;30’ or –include ‘QUAL&gt;=30’ like this:\nbcftools query -f'%POS %QUAL [%GT %AD] %REF %ALT\\n' -i'QUAL&gt;=30' out.vcf | head\nNow compare the result with the output from the previous command, were the low-quality lines removed?\nIn the next step limit the output to SNPs and ignore indels by adding the type=“snp” condition to the filtering expression. Because both conditions must be valid at the same time, we request the AND logic using the && operator:\nbcftools query -f'%POS %QUAL [%GT %AD] %REF %ALT\\n' -i'QUAL&gt;=30 && type=\"snp\"'out.vcf | head\n\n\n3.1 Exercises\nQ1: Can you print SNPs with QUAL bigger than 30 and require at least 25 alternate reads in the AD tag?\nRemember, the first value of the AD tag is the number of reference reads, the second is the number of alternate reads, therefore you will need to query the second value of the AD tag. The first value can be queried as AD[0] and the second as AD[1] (the allele indexes are zero-based). In case of FORMAT fields, also the queried sample must be selected as AD[sample:subfield] . Therefore add to the expression the condition AD[0:1] &gt;= 25 to select the first (and in our case the only one) sample or AD[*:1] &gt;= 25 to select any sample for which the condition is valid.\nNow we can filter our callset. In order to evaluate the quality, we will use bcftools stats to calculate the ratio of transitions vs transversions. We start by checking what is the ts/tv of the raw unfiltered callset. The stats command produces a text output, we extract the field of interest as follows:\nbcftools stats out.vcf | less\nbcftools stats out.vcf | grep TSTV\nbcftools stats out.vcf | grep TSTV | cut -f5\nQ2: Calculate ts/tv of the set filtered as above by adding -i ‘QUAL&gt;=30 && AD[*:1]&gt;=25’ to the bcftools stats command.\n(Here the asterisk followed by a colon tells the program to apply the filtering to all samples. At least one sample must pass in order for a site to pass.) After applying the filter, you should observe an increased ts/tv value.\nQ3: Can you do the reverse and find out the ts/tv of the removed sites? Use the -e option instead of -i. The ts/tv of the removed low-quality sites should be lower.\nQ4: The test data come from an inbred homozygous mouse, therefore any heterozygous genotypes are most likely mapping and alignment artefacts. Can you find out what is the ts/tv of the heterozyous SNPs? Do you expect higher or lower ts/tv? Use the filtering expression -i ‘GT=“het”’ to select sites with heterozygous genotypes.\nAnother useful command is bcftools filter which allows to “soft filter” the VCF: instead of removing sites, it can annotate the FILTER column to indicate sites which fail. Apply the above filters (‘QUAL&gt;=30 && AD[*:1]&gt;=25’) to produce a final callset, adding also the –SnpGap and the –IndelGap option to filter variants in close proximity to indels:\nbcftools filter -s LowQual -i'QUAL&gt;=30 && AD[*:1]&gt;=25' -g8 -G10 out.vcf -o out.flt.vcf\n\n\n3.2 Variant normalization\nThe same indel variant can be represented in different ways. For example, consider the following 2bp deletion. Although the resulting sequence does not change, the deletion can be placed at two different positions within the short repeat:\n12345\nTTCTC\nPOS=1 T–TC\nPOS=3 TTC–\nIn order to be able to compare indels between two datasets, we left-align such variants.\nQ5: Use the bcftools norm command to normalize the filtered callset. Note that you will need to provide the –fasta-ref option. Check in the output how many indels were realigned.\nNow continue to the next section of the tutorial: Multi-sample variant calling\n4 Calling Variants Across Multiple Samples\nIn many types of experiments we sequence multiple samples and compare their genetic variation across samples. The single-sample variant calling we have done so far has the disadvantage of not providing information about reference genotypes. Because only variant sites are stored, we are not able to distinguish between records missing due to reference genotypes versus records missing due to lack of coverage.\nIn this section we will call variants across two mouse samples.\nTo begin, check that there are two BAM files in the directory.\nls *.bam\nNow modify the variant calling command from the previous section to use both BAM files. Write the output to a BCF file called multi.bcf.\nNow index the file multi.bcf\nFilter the file multi.bcf using the same filters as the previous section and write the output to a BCF file called multi.filt.bcf.\nNow index the multi.filt.bcf file.\n\n\n4.1 Exercises\nQ1: What is the ts/tv of the raw calls and of the filtered set?\nQ2: What is the ts/tv of the removed sites?\nNow continue to the next section of the tutorial: Visualising variants\n\n\n5 Variant visualisation\nIt is often useful to visually inspect a SNP or indel of interest in order to assess the quality of the variant and interpret the genomic context of the variant. We can use the IGV tool to view some of the variant positions from the VCF file.\nStart IGV by typing:\nigv\n\n\n5.1 Load the reference genome\nOpen the mouse reference genome”\nGo to ’ Genomes -&gt; Load Genome From Server… ’ and select “Mouse mm10”. This is a synonym for GRCm38, which is the current mouse assembly (reference genome)\n\n\n5.2 Load the alignment\nLoad the alignment file for the sample A_J (A_J.bam).\nGo to ’ File -&gt; Load from File… ‘. Select the “A_J.bam” BAM file that you created in the previous section and click’ Open ’.\n\n\n5.3 Exercises\nUse the IGV navigation bar, go to the region chr19:10,001,874-10,002,017 and inspect the SNP at position 10001946.\nQ1: How many forward aligned reads support the SNP call?\nHint; Hover the mouse pointer over the coverage bar at the top (or click, depending on the IGV settings) to get this information.\nQ2: Was this SNP called by bcftools?\nHint Use bcftools view -H -r chr19:10001946 multi.filt.bcf to verify\nQ3: Did this SNP pass the filters?\nHint Look for this information in the BCF file\nUse the IGV navigation bar, go to the region chr19:10072443 and inspect the SNP at position 10072443.\nQ4: Was this SNP called by bcftools?\nQ5: Did the SNP pass the filters?\nQ6: Does this look like a real SNP? Please explain why.\nNow continue to the next section of the tutorial: Variant annotation\n\n\n6 Variant annotation\nVariant annotation is used to help researchers filter and prioritise functionally important variants for further study. There are several popular programs available for annotating variants. These include:\n• bcftools csq\n• Ensembl VEP (Variant Effect Predictor)\n• SnpEff\nThese tools can be used to to predict the functional consequence of the variants on the protein (e.g. whether a variant is mis-sense, stop-gain, frameshift inducing etc).\n\n\n6.1 bcftools csq\nHere we will use the lightweight bcftools csq command to annotate the variants. Type the command:\nbcftools view -i 'FILTER=\"PASS\"' multi.filt.bcf | bcftools csq -p m -f GRCm38_68.19.fa -g Mus_musculus.part.gff3.gz -Ob -o multi.filt.annot.bcf\nThe command takes VCF as input, the -f option specifies the reference file that the data was aligned\ntoo and the -g option specifies the GFF file that contains the gene models for the reference. Because our data is not phased, we provide the -p option (which does not actually phase the data, but tells the program to make an assumption about the phase). The -Ob option ensures the command produces compressed BCF as output.\nNow index the BCF file:\nbcftools index multi.filt.annot.bcf\n\n\n6.2 Exercises\nQ1 Use the bcftools query -f ‘%BCSQ’ command to extract the consequence at position 19:10088937.\nQ2 What is the functional annotation at this site?\nQ3 What is the amino acid change?\nCongratulations you have reached the end of the variant calling tutorial. For the answers to the exercises in this tutorial please visit answers."
  },
  {
    "objectID": "course_modules/Module3/module3_manual.html",
    "href": "course_modules/Module3/module3_manual.html",
    "title": "Manual",
    "section": "",
    "text": "Introduction and Learning Outcomes\nDNA Variations: Definitions and Types\nPractical Applications of Variant Calling\nVariant Calling Basics\nSteps in Variant Detection\nCommon Sources of Error\nAdvanced Topics in Variant Calling\nAssessing and Benchmarking Variant Calls\nFunctional Annotation and Variant Consequences\nPopulation-Level Variant Data\nReferences and Additional Resources\n\n\n\n\nBy the end of this course, you should be able to:\n\nUnderstand various types of genomic variations and how they arise.\nDescribe how variant calls are generated from raw sequencing data.\nAssess variant quality and visualize variants using tools such as IGV.\nPerform basic variant annotation and interpret biological consequences.\n\n\n\n\nMutations refer to any alteration in the DNA base sequence and can be broadly classified into germline and somatic mutations. Germline mutations occur in the egg or sperm cells and are heritable, meaning they can be passed on to subsequent generations. In contrast, somatic mutations arise in non-germline tissues and do not get inherited by offspring. When examining the types of genomic variations, it is essential to differentiate between large-scale and small-scale changes. Large-scale variations include chromosomal gains or losses (such as aneuploidies), various forms of translocations (for instance, reciprocal and Robertsonian), and copy number variants (CNVs), which are duplications or deletions typically ranging from approximately 1 kilobase to several megabases—collectively accounting for roughly 12% of the human genome. Structural variants (SVs), including insertions, inversions, translocations, and tandem duplications larger than 50 base pairs, also fall into this category. On the other hand, small-scale variations involve more subtle changes such as single nucleotide variants (SNVs), which are simple base substitutions (for example, A→C or G→T), indels that represent small insertions or deletions (less than 50 base pairs), multinucleotide polymorphisms (MNPs) which involve multiple adjacent base substitutions, and microsatellites or short tandem repeats (STRs). STRs are characterized by the repetition of small motifs of 2–6 base pairs—like the “GATA” sequence repeated 7 versus 8 times—which are inherently more variable. Notably, while single nucleotide polymorphisms (SNPs) are usually biallelic with a relatively low mutation rate (around 10^-8 per base pair per generation), STRs are multiallelic with higher mutation rates, making their ancestral states more challenging to infer.\nTo summarise:\nMutation / Variation: Any change in the DNA base sequence.\nGermline mutation: Heritable variation present in egg or sperm cells.\nSomatic mutation: Variation in non-germline tissues; not passed to offspring.\nCopy number variants (CNVs): Duplications or deletions ranging from ~1 kb to many Mb, accounting for roughly 12% of the human genome.\nStructural Variants (SVs): Insertions, inversions, translocations, tandem duplications, etc. (often &gt;50 bp).\nSingle Nucleotide Variants (SNVs): Base substitutions (A→C, G→T, etc.).\nIndels: Small insertions or deletions (&lt;50 bp).\nMultinucleotide Polymorphisms (MNPs): Multiple adjacent base substitutions.\nMicrosatellites / STRs: Short-tandem repeats of 2–6 bp repeated multiple times (e.g., “GATA” repeated 7 vs 8 times).\n\n\n\nCataloging biological diversity is fundamental for understanding population genetics and exploring evolutionary trends, while disease diagnosis leverages the identification of pathogenic mutations in clinical settings. In addition, deciphering genotype-phenotype associations plays a crucial role in locating variants that underlie specific traits or diseases, thereby facilitating pharmacogenomics where drug choice and dosage are tailored based on individual genotypes. DNA forensics employs STR profiling to aid law enforcement and personal identification, and comprehensive population genetics studies help infer demographic history. Finally, marker-assisted selection is widely used in agriculture and breeding programs to improve crop and livestock traits efficiently.\n\n\n\nIn a diploid genome, each human cell contains two copies of every chromosome, which means that each locus on the genome has two alleles. The combination of these alleles is referred to as the genotype, and examples include homozygous genotypes like “RR” and “AA” or heterozygous combinations such as “RA.” When these allele combinations are resolved into phase, meaning the specific arrangement on each individual chromosome is known, they form what is called a haplotype.\nIn the context of variant calling, the term “reference allele” (or “Ref”) refers to the allele that appears in the reference genome, whereas the “alternate allele” (or “Alt”) is the observed variant in the sequenced sample. For a diploid organism, the possible genotypes based on these alleles can be represented as RR (0/0), RA (0/1), or AA (1/1).\nThe Variant Call Format, or VCF, is a widely adopted tab-delimited text format designed for storing gene sequence variations. The format includes key fields such as CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO, FORMAT, and additional sample-specific columns. Notably, the VCF format captures details such as the depth (DP), which is the total number of sequencing reads covering a variant site, and the allelic depth (AD), which provides coverage information for each allele.\nSeveral tools are available for variant calling. BCFtools and samtools are frequently used for identifying single nucleotide variants (SNPs) and small insertions or deletions (indels) from alignment data. For more advanced analyses, tools like GATK’s HaplotypeCaller leverage local re-assembly techniques to improve the accuracy of indel calling. Additionally, other tools such as FreeBayes, Platypus, and DeepVariant contribute to variant calling efforts by offering alternative methodologies and algorithms suited to various data types and experimental designs.\n\n\n\nThe process begins by generating sequencing data, which consists of raw reads produced by sequencing platforms such as Illumina. These reads are subjected to rigorous quality control, including trimming of adapters and filtering out low-quality bases, before being aligned or mapped to a reference genome. Once aligned, a pileup is generated to summarize the number of reads supporting each base at every position, which then informs the variant calling step—determining the genotype probabilities at each locus. Subsequent filtering addresses factors such as base quality, where Phred scores indicate the confidence in each base call, mapping quality that reflects the accuracy of the read’s placement within the genome, and established depth thresholds to ensure sufficient coverage. Variant calling itself can be approached naïvely, using simple allele-frequency thresholds (for example, calling a heterozygote if roughly 50% of reads show a particular base), or through more sophisticated statistical methods that incorporate base and mapping error rates, prior allele frequencies, and other variables to compute a posterior probability for each potential genotype.\n\n\n\nSeveral common sources of error must be considered during variant calling. Homopolymers and repetitive regions, characterized by long runs of identical bases, frequently cause systematic sequencing and mapping errors. Strand bias, where a variant appears exclusively on forward or reverse sequencing reads, often signals technical artefacts rather than genuine biological variants. Incorrect alignments, particularly near insertions or deletions (indels), may lead to false-positive single nucleotide polymorphisms (SNPs). Additionally, extremely high sequencing coverage can indicate repetitive elements or duplicated genomic regions, complicating accurate variant calling. Differences between RNA-seq and DNA-seq data, such as splice junctions and RNA editing events, may also produce apparent variants that do not reflect underlying genomic DNA sequence changes. Finally, variability in experimental conditions can result in non-reproducible calls, emphasizing the importance of validating and replicating findings to ensure reliability.\n\n\n\nSomatic variant calling specifically aims to identify genetic variants present in tumor cells by comparing sequences from tumor samples against matched normal tissues. The primary objective is detecting mutations that may drive cancer progression. Because tumors are heterogeneous, subclonal variants—mutations present in only a subset of cells—often exhibit widely varying allele fractions, complicating their detection and interpretation. Specialized bioinformatics pipelines such as Mutect2 and Strelka2 have been developed to address these complexities and accurately distinguish true somatic mutations from background noise.\nIndel calling presents unique challenges, notably due to elevated sequencing error rates in regions such as microsatellites and homopolymers, where accurate base calling is particularly difficult. Additionally, indels can be represented in multiple valid ways by different alignment tools, leading to inconsistencies in their annotation and interpretation. As a result, complex indel regions often suffer from low reproducibility, with significant discrepancies observed across different variant callers. Users are encouraged to validate indel calls rigorously, especially in clinical or sensitive research contexts.\nEmerging approaches in variant calling are addressing these limitations. Local de novo assembly methods, implemented in tools like GATK’s HaplotypeCaller, Scalpel, and Octopus, reconstruct local genomic haplotypes directly from sequencing reads, helping resolve complex variants more effectively. Additionally, newer methods employ variation graphs—data structures that represent multiple known variants simultaneously—to improve alignment accuracy and variant calling performance, particularly in regions of high genetic complexity or variability. These innovative approaches promise to significantly enhance the accuracy and comprehensiveness of genomic analyses moving forward.\n\n\n\nThe transition/transversion (Ts/Tv) ratio is a critical metric used to assess the quality of variant calls. Transitions (Ts)—base substitutions within purines (A↔︎G) or pyrimidines (C↔︎T)—occur more frequently than transversions (Tv), substitutions between purines and pyrimidines. For high-quality SNP datasets in humans, a Ts/Tv ratio of approximately 2 to 3 is typically expected; deviations from this range can indicate underlying errors in variant calling.\nAnother key consideration is the concept of a callable genome, as certain regions of the genome are inherently difficult to analyze due to repetitive sequences, structural complexities, or extreme GC content. Not all genomic regions are equally accessible or “callable,” and thus it is beneficial to utilize established benchmark sets, such as the Genome in a Bottle project, which define high-confidence genomic regions suitable for accurate variant detection and performance assessment.\nWhen evaluating variant-calling performance, metrics such as precision and recall become essential. Precision represents the proportion of called variants that are correct (true positives), while recall, also known as sensitivity, measures the proportion of true variants correctly identified. To accurately compute these metrics, benchmarking tools like hap.py or vcfeval are routinely employed, allowing researchers to systematically assess and optimize variant-calling workflows for reliability and accuracy.\n\n\n\nFunctional annotation of genetic variants is an essential step in understanding their potential biological impacts. Annotation tools such as BCFtools/csq, Ensembl Variant Effect Predictor (VEP), and ANNOVAR are commonly utilized to systematically determine the genomic context of variants, categorizing them as coding, intronic, untranslated region (UTR), or intergenic. These tools further predict the functional consequences of variants, distinguishing between synonymous changes, which do not alter the amino acid sequence, and non-synonymous or missense mutations, which result in amino acid substitutions. More severe mutations include nonsense variants, which introduce premature stop codons, and frameshift variants, which disrupt the reading frame of the protein. Variant pathogenicity is often classified into categories such as pathogenic, likely pathogenic, benign, likely benign, or variants of uncertain significance (VUS). These classifications integrate population frequency data, computational prediction models, family-based segregation analyses, and results from functional assays to provide meaningful insights into the clinical or biological relevance of each variant.\n\n\n\nPopulation-level variant datasets provide critical resources for interpreting genetic variation in a global context. The 1000 Genomes Project, for instance, encompasses genomic data from over 2,500 individuals representing 26 distinct populations worldwide, with an average sequencing coverage of approximately 7–8X. This extensive collection offers researchers a comprehensive snapshot of human genetic diversity, aiding studies in population genetics, evolutionary biology, and clinical genomics.\nComplementary to the 1000 Genomes Project, resources like HapMap, HGDP-CEPH, and gnomAD further enrich our understanding of global genetic variation. The International HapMap Project, although now archived, was an early effort aimed at cataloging common single nucleotide polymorphisms (SNPs) across diverse human populations. The HGDP-CEPH panel provides valuable genomic data from a broad array of worldwide populations, offering insights into human evolutionary history and population structure. The Genome Aggregation Database (gnomAD) expands upon these earlier efforts by compiling allele frequency data from tens of thousands of sequenced individuals, greatly enhancing our ability to interpret genetic variants, understand their functional significance, and accurately determine their frequencies across different populations.\n\n\n\nOlson, N.D., et al. (2023) Nature Reviews Genetics 24:464–483.\nThe 1000 Genomes Project Consortium. (2015) Nature 526:68–74.\nCann, H.M., et al. (2002) Science 296:261–262. (HGDP-CEPH)\nEnsembl Variation documentation: https://www.ensembl.org/info/docs/variation/index.html\nBCFtools: https://github.com/samtools/bcftools\nVEP (Variant Effect Predictor): https://github.com/willmclaren/ensembl-vep\ngnomAD: https://gnomad.broadinstitute.org/about\nFor course-related inquiries, contact: qasim.ayub@monash.edu"
  },
  {
    "objectID": "course_modules/Module3/module3_manual.html#section",
    "href": "course_modules/Module3/module3_manual.html#section",
    "title": "Manual",
    "section": "",
    "text": "Introduction and Learning Outcomes\nDNA Variations: Definitions and Types\nPractical Applications of Variant Calling\nVariant Calling Basics\nSteps in Variant Detection\nCommon Sources of Error\nAdvanced Topics in Variant Calling\nAssessing and Benchmarking Variant Calls\nFunctional Annotation and Variant Consequences\nPopulation-Level Variant Data\nReferences and Additional Resources\n\n\n\n\nBy the end of this course, you should be able to:\n\nUnderstand various types of genomic variations and how they arise.\nDescribe how variant calls are generated from raw sequencing data.\nAssess variant quality and visualize variants using tools such as IGV.\nPerform basic variant annotation and interpret biological consequences.\n\n\n\n\nMutations refer to any alteration in the DNA base sequence and can be broadly classified into germline and somatic mutations. Germline mutations occur in the egg or sperm cells and are heritable, meaning they can be passed on to subsequent generations. In contrast, somatic mutations arise in non-germline tissues and do not get inherited by offspring. When examining the types of genomic variations, it is essential to differentiate between large-scale and small-scale changes. Large-scale variations include chromosomal gains or losses (such as aneuploidies), various forms of translocations (for instance, reciprocal and Robertsonian), and copy number variants (CNVs), which are duplications or deletions typically ranging from approximately 1 kilobase to several megabases—collectively accounting for roughly 12% of the human genome. Structural variants (SVs), including insertions, inversions, translocations, and tandem duplications larger than 50 base pairs, also fall into this category. On the other hand, small-scale variations involve more subtle changes such as single nucleotide variants (SNVs), which are simple base substitutions (for example, A→C or G→T), indels that represent small insertions or deletions (less than 50 base pairs), multinucleotide polymorphisms (MNPs) which involve multiple adjacent base substitutions, and microsatellites or short tandem repeats (STRs). STRs are characterized by the repetition of small motifs of 2–6 base pairs—like the “GATA” sequence repeated 7 versus 8 times—which are inherently more variable. Notably, while single nucleotide polymorphisms (SNPs) are usually biallelic with a relatively low mutation rate (around 10^-8 per base pair per generation), STRs are multiallelic with higher mutation rates, making their ancestral states more challenging to infer.\nTo summarise:\nMutation / Variation: Any change in the DNA base sequence.\nGermline mutation: Heritable variation present in egg or sperm cells.\nSomatic mutation: Variation in non-germline tissues; not passed to offspring.\nCopy number variants (CNVs): Duplications or deletions ranging from ~1 kb to many Mb, accounting for roughly 12% of the human genome.\nStructural Variants (SVs): Insertions, inversions, translocations, tandem duplications, etc. (often &gt;50 bp).\nSingle Nucleotide Variants (SNVs): Base substitutions (A→C, G→T, etc.).\nIndels: Small insertions or deletions (&lt;50 bp).\nMultinucleotide Polymorphisms (MNPs): Multiple adjacent base substitutions.\nMicrosatellites / STRs: Short-tandem repeats of 2–6 bp repeated multiple times (e.g., “GATA” repeated 7 vs 8 times).\n\n\n\nCataloging biological diversity is fundamental for understanding population genetics and exploring evolutionary trends, while disease diagnosis leverages the identification of pathogenic mutations in clinical settings. In addition, deciphering genotype-phenotype associations plays a crucial role in locating variants that underlie specific traits or diseases, thereby facilitating pharmacogenomics where drug choice and dosage are tailored based on individual genotypes. DNA forensics employs STR profiling to aid law enforcement and personal identification, and comprehensive population genetics studies help infer demographic history. Finally, marker-assisted selection is widely used in agriculture and breeding programs to improve crop and livestock traits efficiently.\n\n\n\nIn a diploid genome, each human cell contains two copies of every chromosome, which means that each locus on the genome has two alleles. The combination of these alleles is referred to as the genotype, and examples include homozygous genotypes like “RR” and “AA” or heterozygous combinations such as “RA.” When these allele combinations are resolved into phase, meaning the specific arrangement on each individual chromosome is known, they form what is called a haplotype.\nIn the context of variant calling, the term “reference allele” (or “Ref”) refers to the allele that appears in the reference genome, whereas the “alternate allele” (or “Alt”) is the observed variant in the sequenced sample. For a diploid organism, the possible genotypes based on these alleles can be represented as RR (0/0), RA (0/1), or AA (1/1).\nThe Variant Call Format, or VCF, is a widely adopted tab-delimited text format designed for storing gene sequence variations. The format includes key fields such as CHROM, POS, ID, REF, ALT, QUAL, FILTER, INFO, FORMAT, and additional sample-specific columns. Notably, the VCF format captures details such as the depth (DP), which is the total number of sequencing reads covering a variant site, and the allelic depth (AD), which provides coverage information for each allele.\nSeveral tools are available for variant calling. BCFtools and samtools are frequently used for identifying single nucleotide variants (SNPs) and small insertions or deletions (indels) from alignment data. For more advanced analyses, tools like GATK’s HaplotypeCaller leverage local re-assembly techniques to improve the accuracy of indel calling. Additionally, other tools such as FreeBayes, Platypus, and DeepVariant contribute to variant calling efforts by offering alternative methodologies and algorithms suited to various data types and experimental designs.\n\n\n\nThe process begins by generating sequencing data, which consists of raw reads produced by sequencing platforms such as Illumina. These reads are subjected to rigorous quality control, including trimming of adapters and filtering out low-quality bases, before being aligned or mapped to a reference genome. Once aligned, a pileup is generated to summarize the number of reads supporting each base at every position, which then informs the variant calling step—determining the genotype probabilities at each locus. Subsequent filtering addresses factors such as base quality, where Phred scores indicate the confidence in each base call, mapping quality that reflects the accuracy of the read’s placement within the genome, and established depth thresholds to ensure sufficient coverage. Variant calling itself can be approached naïvely, using simple allele-frequency thresholds (for example, calling a heterozygote if roughly 50% of reads show a particular base), or through more sophisticated statistical methods that incorporate base and mapping error rates, prior allele frequencies, and other variables to compute a posterior probability for each potential genotype.\n\n\n\nSeveral common sources of error must be considered during variant calling. Homopolymers and repetitive regions, characterized by long runs of identical bases, frequently cause systematic sequencing and mapping errors. Strand bias, where a variant appears exclusively on forward or reverse sequencing reads, often signals technical artefacts rather than genuine biological variants. Incorrect alignments, particularly near insertions or deletions (indels), may lead to false-positive single nucleotide polymorphisms (SNPs). Additionally, extremely high sequencing coverage can indicate repetitive elements or duplicated genomic regions, complicating accurate variant calling. Differences between RNA-seq and DNA-seq data, such as splice junctions and RNA editing events, may also produce apparent variants that do not reflect underlying genomic DNA sequence changes. Finally, variability in experimental conditions can result in non-reproducible calls, emphasizing the importance of validating and replicating findings to ensure reliability.\n\n\n\nSomatic variant calling specifically aims to identify genetic variants present in tumor cells by comparing sequences from tumor samples against matched normal tissues. The primary objective is detecting mutations that may drive cancer progression. Because tumors are heterogeneous, subclonal variants—mutations present in only a subset of cells—often exhibit widely varying allele fractions, complicating their detection and interpretation. Specialized bioinformatics pipelines such as Mutect2 and Strelka2 have been developed to address these complexities and accurately distinguish true somatic mutations from background noise.\nIndel calling presents unique challenges, notably due to elevated sequencing error rates in regions such as microsatellites and homopolymers, where accurate base calling is particularly difficult. Additionally, indels can be represented in multiple valid ways by different alignment tools, leading to inconsistencies in their annotation and interpretation. As a result, complex indel regions often suffer from low reproducibility, with significant discrepancies observed across different variant callers. Users are encouraged to validate indel calls rigorously, especially in clinical or sensitive research contexts.\nEmerging approaches in variant calling are addressing these limitations. Local de novo assembly methods, implemented in tools like GATK’s HaplotypeCaller, Scalpel, and Octopus, reconstruct local genomic haplotypes directly from sequencing reads, helping resolve complex variants more effectively. Additionally, newer methods employ variation graphs—data structures that represent multiple known variants simultaneously—to improve alignment accuracy and variant calling performance, particularly in regions of high genetic complexity or variability. These innovative approaches promise to significantly enhance the accuracy and comprehensiveness of genomic analyses moving forward.\n\n\n\nThe transition/transversion (Ts/Tv) ratio is a critical metric used to assess the quality of variant calls. Transitions (Ts)—base substitutions within purines (A↔︎G) or pyrimidines (C↔︎T)—occur more frequently than transversions (Tv), substitutions between purines and pyrimidines. For high-quality SNP datasets in humans, a Ts/Tv ratio of approximately 2 to 3 is typically expected; deviations from this range can indicate underlying errors in variant calling.\nAnother key consideration is the concept of a callable genome, as certain regions of the genome are inherently difficult to analyze due to repetitive sequences, structural complexities, or extreme GC content. Not all genomic regions are equally accessible or “callable,” and thus it is beneficial to utilize established benchmark sets, such as the Genome in a Bottle project, which define high-confidence genomic regions suitable for accurate variant detection and performance assessment.\nWhen evaluating variant-calling performance, metrics such as precision and recall become essential. Precision represents the proportion of called variants that are correct (true positives), while recall, also known as sensitivity, measures the proportion of true variants correctly identified. To accurately compute these metrics, benchmarking tools like hap.py or vcfeval are routinely employed, allowing researchers to systematically assess and optimize variant-calling workflows for reliability and accuracy.\n\n\n\nFunctional annotation of genetic variants is an essential step in understanding their potential biological impacts. Annotation tools such as BCFtools/csq, Ensembl Variant Effect Predictor (VEP), and ANNOVAR are commonly utilized to systematically determine the genomic context of variants, categorizing them as coding, intronic, untranslated region (UTR), or intergenic. These tools further predict the functional consequences of variants, distinguishing between synonymous changes, which do not alter the amino acid sequence, and non-synonymous or missense mutations, which result in amino acid substitutions. More severe mutations include nonsense variants, which introduce premature stop codons, and frameshift variants, which disrupt the reading frame of the protein. Variant pathogenicity is often classified into categories such as pathogenic, likely pathogenic, benign, likely benign, or variants of uncertain significance (VUS). These classifications integrate population frequency data, computational prediction models, family-based segregation analyses, and results from functional assays to provide meaningful insights into the clinical or biological relevance of each variant.\n\n\n\nPopulation-level variant datasets provide critical resources for interpreting genetic variation in a global context. The 1000 Genomes Project, for instance, encompasses genomic data from over 2,500 individuals representing 26 distinct populations worldwide, with an average sequencing coverage of approximately 7–8X. This extensive collection offers researchers a comprehensive snapshot of human genetic diversity, aiding studies in population genetics, evolutionary biology, and clinical genomics.\nComplementary to the 1000 Genomes Project, resources like HapMap, HGDP-CEPH, and gnomAD further enrich our understanding of global genetic variation. The International HapMap Project, although now archived, was an early effort aimed at cataloging common single nucleotide polymorphisms (SNPs) across diverse human populations. The HGDP-CEPH panel provides valuable genomic data from a broad array of worldwide populations, offering insights into human evolutionary history and population structure. The Genome Aggregation Database (gnomAD) expands upon these earlier efforts by compiling allele frequency data from tens of thousands of sequenced individuals, greatly enhancing our ability to interpret genetic variants, understand their functional significance, and accurately determine their frequencies across different populations.\n\n\n\nOlson, N.D., et al. (2023) Nature Reviews Genetics 24:464–483.\nThe 1000 Genomes Project Consortium. (2015) Nature 526:68–74.\nCann, H.M., et al. (2002) Science 296:261–262. (HGDP-CEPH)\nEnsembl Variation documentation: https://www.ensembl.org/info/docs/variation/index.html\nBCFtools: https://github.com/samtools/bcftools\nVEP (Variant Effect Predictor): https://github.com/willmclaren/ensembl-vep\ngnomAD: https://gnomad.broadinstitute.org/about\nFor course-related inquiries, contact: qasim.ayub@monash.edu"
  },
  {
    "objectID": "course_modules/Module3/module3.html",
    "href": "course_modules/Module3/module3.html",
    "title": "Overview",
    "section": "",
    "text": "Content by Qasim Ayub, qasim.ayub@monash.edu\n\nModule Title\nVariant calling\n\n\nDuration\n3h\n\n\nKey topics\nIn this module, learners will look at\n\nDNA variation\nVariant calling basics and steps in variant detection\nCommon sources of error in variant calling\nPopulation level variant data, genomic context and clinical relevance of variants.\n\n\n\nActivities\n\nLecture 0.5h\nPractical exercises 2h\nQuiz 0.5h\n\n\n\nManual and practical exercises\nModule manual\nExercises\nSolutions\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual)\n\n\nLecture notes or scripts\nPre-recorded lecture\n\n\nDatasets\n\n\nAssessment quiz\nQuestions\nAnswers"
  },
  {
    "objectID": "course_modules/Module2/module2_manual.html",
    "href": "course_modules/Module2/module2_manual.html",
    "title": "Manual",
    "section": "",
    "text": "Sequence alignment in NGS is the process of determining the most likely source of the observed DNA sequencing read within the reference genome sequence.\n\n\n\n1.2 Why align?\nThere are typical inferences you can make from an alignment of NGS data against a reference genome:\n• Variation from the reference – could have functional consequence.\n• Transcript abundance: Instead of a microarray, you could use alignment to genome to quantify expression: more sensitive\n• Ab-initio transcript discovery: you can see a pileup from RNA seq data showing evidence for an exon which was previously missed or an exon which is being skipped in a transcript.\n\n\n\n 1.2 Learning outcomes\nOn completion of the tutorial, you can expect to be able to:\n• Perform read alignment using standard tools (BWA-MEM)\n• Perform the following task and understand their effect on analysis results\n– Mark Duplicates\n• Visualise read alignments using IGV (Integrated Visualisation Tool)\nIf there is time you will learn how to:\n• Merge the results from multiple alignments and understand when it is appropriate to perform a merge\n\n\n\nThis tutorial comprises the following sections:\n1. Performing read alignment\n2. Alignment visualisation\nThere is also an additional (optional) section: 3. Alignment workflows\n\n\n\nThis tutorial was written by Jacqui Keane based on material from Thomas Keane, Vivek Iyer and Victoria Offord.\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal on your computer and type the command below:\ncd /home/manager/course_data/read_alignment Now you can follow the instructions in the tutorial from here.\n\n\n\nThis tutorial assumes that you have samtools, bwa, Picard tools and IGV installed on your computer.\nThese are already installed on the VM you are using. To check that these are installed, you can run the following commands:\nsamtools --help\nbwa\npicard -h\nigv\nThis should return the help message for samtools, bwa and Picard tools respectively. The final command should launch the genome viewer IGV. You can close the IGV software, we will use it later in this tutorial to visualise alignments.\nTo get started with the tutorial, head to the first section: Performing read alignment"
  },
  {
    "objectID": "course_modules/Module2/module2_manual.html#read-alignment",
    "href": "course_modules/Module2/module2_manual.html#read-alignment",
    "title": "Manual",
    "section": "",
    "text": "Sequence alignment in NGS is the process of determining the most likely source of the observed DNA sequencing read within the reference genome sequence.\n\n\n\n1.2 Why align?\nThere are typical inferences you can make from an alignment of NGS data against a reference genome:\n• Variation from the reference – could have functional consequence.\n• Transcript abundance: Instead of a microarray, you could use alignment to genome to quantify expression: more sensitive\n• Ab-initio transcript discovery: you can see a pileup from RNA seq data showing evidence for an exon which was previously missed or an exon which is being skipped in a transcript.\n\n\n\n 1.2 Learning outcomes\nOn completion of the tutorial, you can expect to be able to:\n• Perform read alignment using standard tools (BWA-MEM)\n• Perform the following task and understand their effect on analysis results\n– Mark Duplicates\n• Visualise read alignments using IGV (Integrated Visualisation Tool)\nIf there is time you will learn how to:\n• Merge the results from multiple alignments and understand when it is appropriate to perform a merge\n\n\n\nThis tutorial comprises the following sections:\n1. Performing read alignment\n2. Alignment visualisation\nThere is also an additional (optional) section: 3. Alignment workflows\n\n\n\nThis tutorial was written by Jacqui Keane based on material from Thomas Keane, Vivek Iyer and Victoria Offord.\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal on your computer and type the command below:\ncd /home/manager/course_data/read_alignment Now you can follow the instructions in the tutorial from here.\n\n\n\nThis tutorial assumes that you have samtools, bwa, Picard tools and IGV installed on your computer.\nThese are already installed on the VM you are using. To check that these are installed, you can run the following commands:\nsamtools --help\nbwa\npicard -h\nigv\nThis should return the help message for samtools, bwa and Picard tools respectively. The final command should launch the genome viewer IGV. You can close the IGV software, we will use it later in this tutorial to visualise alignments.\nTo get started with the tutorial, head to the first section: Performing read alignment"
  },
  {
    "objectID": "course_modules/Module2/module2_manual.html#performing-read-alignment",
    "href": "course_modules/Module2/module2_manual.html#performing-read-alignment",
    "title": "Manual",
    "section": "2. Performing Read Alignment",
    "text": "2. Performing Read Alignment\nHere we will use the BWA aligner to align a smll set of Illumina sequencing data to the Mus Musculus reference genome. We will align genomic sequence (from Whole-Genome Sequencing) from a mouse embryo which has been mutagenised while the one-cell stage using CRISPR-Cas9 and a gRNA targeting an exon of the Tyr gene. The successful mutation of the gene will delete one or both alleles. A bi-allelic null Tyr mouse will be albino, but otherwise healthy.\nFirst, check you are in the correct directory.\npwd\nIt should display something like:\n/home/manager/course_data/read_alignment\n\n2.1 Viewing the reference genome\nGo to the ref directory that contains the fasta files of the reference genomes: cd ~/course_data/read_alignment/data/ref\nFasta files (.fa) are used to store raw sequencing information before aligning data. A single chromosome from the mouse genome is contained in the file GRCm38.68.dna.toplevel.chr7.fa.gz\nView the file with zless (we use zless instead of less because the file is compressed):\nzless GRCm38.68.dna.toplevel.chr7.fa.gz\nQ1: What is the length of chromosome 7 of the mouse genome? (Hint: Look at the fasta header for chromosome 7)\n................................................................................................\nSimilar to a BAM file, to allow fast retrieval of data, an index file is often required. You should check for the presence of fasta indexes for the genome in the ‘ref’ directory:\nGRCm38.68.dna.toplevel.chr7.fa.gz.amb … GRCm38.68.dna.toplevel.chr7.fa.gz.sa\nThese are created by BWA: suffixtrees, bwt transform etc.\nIf these index files don’t exist, then you can run the indexing with the command\nbwa index GRCm38.68.dna.toplevel.chr7.fa.gz\nBeware – this indexing process can take 3-5 minutes so please only run it if the index files do not exist!\n\n\n2.2 Align the data with bwa\nGo to the ~/course_data/read_alignment/data/Exercise1/fastq/ directory - you can use this command:\ncd ../Exercise1/fastq\nUse the bwa mem command to align the fastq files to the mouse reference genome. By default bwa outputs SAM format directly to the standard output (in this case your terminal window), therefore you will have to redirect the result into a SAM file.\nbwa mem ../../ref/GRCm38.68.dna.toplevel.chr7.fa.gz md5638a_7_87000000_R1.fastq.gz md5638a_7_87000000_R2.fastq.gz &gt; md5638.sam\nThis may take a few minutes, please be patient.\n\n\n2.3 Convert a SAM file to a BAM file\nNow use samtools to convert the SAM file md5638.sam created in the previous step into a BAM file called md5638.bam.\nsamtools view -O BAM -o md5638.bam md5638.sam\nQ2: How much space is saved by using a BAM file instead of a SAM file?\n................................................................................................\n\n\n2.4 Sort and index the BAM file\nThe BAM files produced by BWA are sorted by read name (same order as the original fastq files). However, most viewing and variant calling software require the BAM files to be sorted by reference coordinate position and indexed for rapid retrieval. Therefore, use ‘samtools sort’ to produce a new BAM file called md5638.sorted.bam that is sorted by position.\nsamtools sort -T temp -O bam -o md5638.sorted.bam md5638.bam\nFinally index the sorted BAM file using ‘samtools index’ command.\nNote: indexing a BAM file is also a good way to check that the BAM file has not been truncated (e.g. your disk becomes full when writing the BAM file). At the end of every BAM file, a special end of file (EOF) marker is written. The Samtools index command will first check for this and produce an error message if it is not found.\nsamtools index md5638.sorted.bam\n\n\n2.5 Unix pipes to combine the commands together\nTo produce the sorted BAM file above we had to carry out several separate commands and produce intermediate files. The Unix pipe command allows you to feed the output of one command into the next command.\nYou can combine all of these commands together using unix pipes, and do all of this data processing together and avoid writing intermediate files. To do this type:\nbwa mem ../../ref/GRCm38.68.dna.toplevel.chr7.fa.gz md5638a_7_87000000_R1.fastq..gz md5638a_7_87000000_R2.fastq.gz | samtools view -O BAM - | samtools sort -T temp -O bam -o md5638_2.sorted.bam -\nNow index the BAM file:\nsamtools index md5638_2.sorted.bam\nNote: When the symbol - is used above, Unix will automatically replace - with the output produced by the preceding command (i.e. the command before the | symbol).\n\n\n\n2.6 Mark PCR Duplicates\nWe will use a program called ‘MarkDuplicates’ that is part of Picard tools (http://picard.source-forge.net) to remove PCR duplicates that may have been introduced during the library construction stage. To find the options for ‘MarkDuplicates’ – type:\npicard MarkDuplicates\nNow run MarkDuplicates using the ‘I=’ option to specify the input BAM file and the ‘O=’ option to specify the output file (e.g. md5638.markdup.bam). You will also need to specify the duplication metrics output file using ‘M=’ (e.g. md5638.markdup.metrics).\npicard MarkDuplicates I=md5638.sorted.bam O=md5638.markdup.bam M=md5638.metrics.txt\nQ3: From looking at the output metrics file - how many reads were marked as duplicates? What was the percent duplication?\n................................................................................................\nDon’t forget to generate an index for the new bam file using samtools.\nsamtools index md5638.markdup.bam\n\n\n\n2.7 Generate QC Stats\nUse samtools to collect some statistics and generate QC plots from the alignment in the BAM file from the previous step. Make sure you save the output of the stats command to a file (e.g. md5638.markdup.stats).\nsamtools stats md5638.markdup.bam &gt; md5638.markdup.stats\nplot-bamstats -p md5638_plot/ md5638.markdup.stats\n\n\n2.8 Exercises\nNow look at the output and answer the following questions:\nQ4: What is the total number of reads?\nQ5: What proportion of the reads were mapped?\nQ6: How many reads were paired correctly/properly?\nQ7: How many read pairs mapped to a different chromosome?\nQ8: What is the insert size mean and standard deviation?\nIn your web browser open the file called md5638_plot.html to view the QC information and answer the following questions:\nQ9: How many reads have zero mapping quality?\nQ10: Which of the first fragments or second fragments are higher base quality on average?\nCongratulations you have succesfully aligned some NGS data to a reference genome! Now continue to the next section of the tutorial: Alignment visualisation."
  },
  {
    "objectID": "course_modules/Module2/module2_manual.html#alignment-visualisation",
    "href": "course_modules/Module2/module2_manual.html#alignment-visualisation",
    "title": "Manual",
    "section": "3. Alignment Visualisation",
    "text": "3. Alignment Visualisation\nYou have now made it to the interesting part!\nIntegrative Genome Viewer (IGV) http://www.broadinstitute.org/igv/ allows you to visualise genomic datasets and is a very useful tool for looking at the alignment of reads onto a reference genome from BAM files.\nStart IGV by typing:\nigv &\n\n3.1 IGV main window\nWhen you start IGV, it will open a main window. At the top of this window you have a toolbar and genome ruler for navigation. The largest area in the main window is the data viewer where your alignments, annotations and other data will be displayed. To do this, IGV uses horizontal rows called tracks. Finally, at the bottom, there is a sequence viewer which contains the base level information for your reference genome.\n\n\n\n3.2 Load the reference genome\nIGV provides several genomes which can be selected with the “Genome drop-down box” on the toolbar.\nGo to ’ Genomes -&gt; Load Genome From Server… ’ and select “Mouse mm10”. This is a synonym for GRCm38, which is the current mouse assembly (reference genome)\n\n\n\n\n3.2.1 IGV toolbar and genome ruler\nOnce the genome has loaded, the chromosomes will be shown on the genome ruler with their names/numbers above. When a region is selected, a red box will appear. This represents the visible region of the genome.\nAbove the genome ruler is the toolbar which has a variety controls for navigating the genome:\n\nGenome drop-down - load a genome\nChromosome drop-down - zoom to a chromosome\nSearch - zoom to a chromosome, locus or gene\n\nThere are several other buttons which can be used to control the visible portion of the genome.\n\nWhole genome - zoom back out to whole genome view\nPrevious/next view - move backward/forward through views (like the back/forward buttons in a web browser)\nRefresh - refresh the display\nZoom - zooms in (+) / out (-) on a chromosome\n\n\n\n\n3.2.2 Sequence viewer\nThe sequence viewer shows the genome at the single nucleotide level. You won’t be able to see the sequence until you are zomed in. Let’s try it, select the zooom in (+) option in the top right of the screeen. As you start to zoom in (+), you will see that each nucleotide is represented by a coloured bar (red=T, yellow=G, blue=C and green=A). This makes it easier to spot repetitive regions in the genome. Carry on zooming in (+) and you will see the individual nucleotides.\n\n\n\n3.3 Navigation in IGV\nThere are several views in IGV\n\nGenome view\nChromosome view\nRegion view\n\nThere are several ways to to zoom in and out to these views to look at specific regions or base level information.\n\n\n3.3.1 Whole genome view\nTo get a view of the entire genome select the zoom to whole genome icon (house icon) found in the toolbar at the top of the IGV window.\n\n\n\n3.3.2 Chromosome view\nTo get a view of a specific chromosome select the chromosome from the chromosome drop down list in the toolbar of the IGV window.\n\n\n\n3.3.3 Region view\nJump to region If you know the co-ordinates of the region you want to view, you can enter them into the “Search” and click “Go”. The format is chromosome:start-stop. For example, to view from 100,000 to 100,100 on chr7, you would enter chr7:100,000-100,100 in the search box. We will practice this later in an exercise.\nSelect region If you don’t know the specific co-ordinates of the region you want to look at, you can click and drag to select a region on the genome toolbar.\n\nNote: the visible region of the chromosome is indicated by the red box on the genome ruler.\n\n\n3.3.4 Zooming in and out\nYou can zoom in and out from each view by using the “+” and “-” buttons on the zoom control at the right-hand side of the toolbar. This will also work with the “+” and “-” keys on your keyboard.\n\n\n\n3.4 Load the alignment\nIGV can be used to visualise many different types of data, including read alignments. Each time you load an alignment file it will be added to the data viewer as a new major track.\nGo to ’ File -&gt; Load from File… ‘. Select the “md5638.markdup.bam” BAM file that you created in the previous section and click’ Open ’.\nNote: BAM files and their corresponding index files must be in the same directory for IGV to load them properly.\n\n\n\n\n3.5 Visualising alignments\nFor each read alignment, a major track will appear containing two minor tracks for that sample:\n\ncoverage information\nread alignments\n\nFor the total number of visible tracks, see the bottom left of main window.\nAt the genome level view, there will be no coverage plot or read alignments visible. At the chromosome level view, there are two messages displayed: Zoom in to see coverage/alignments. Finally, once you have zoomed in (+) you will see a density plot in the coverage track and your read alignments.\n\n\n\n3.5.1 Coverage information\nWhen zoomed in to view a region, you can get alignment information for each position in the genome by hovering over the coverage track. This will open a yellow box which tells you the total number of reads mapped at that position, a breakdown of the mapped nucleotide frequencies and the number of reads mapping in a forward/reverse orientation. In the example shown below, 95 reads mapped, 50 forward and 45 reverse, all of which called A at position 202,768 on chromosome PccAS_05_v3.\nThis is just an example for illustrative purposes, please do not try to look at this position in IGV here.\n\n\n\n3.5.2 Viewing individual read alignment information\nRead are represented by grey or transparent/white bars which are stacked together where they align to the reference genome. Reads are pointed to indicate the orientation in which they mapped i.e. on the forward or reverse strand. Hovering over an individual read will display information about its alignment.\n(images/igv-read-information.png “IGV - read information”)\nMismatches occur where the nucleotide in the aligned read is not the same as the nucleotide in that position on the reference genome. A mismatch is indicated by a coloured bar at the relevant position on the read. The colour of the bar represents the mismatched base in the read (red=T, yellow=G, blue=C and green=A).\n\n\n\n3.6 IGV configuration\nFollow the instructions that follow to set up your IGV view:\nSelect the little yellow “speech bubble” icon in the toolbar and set the option to “Show Details on Click” (or you will go mad, I promise!).\n\nZoom in so you can see sequence reads and go to region chr7:87480000-87485000 using the navigation bar at the top.\n\nControl-click or right-click in the data view window. Choose sort alignments by insert size, then choose colour alignments by insert size and finally choose “View as pairs”.\nGo to ’ View -&gt; Preferences… ’ select the ’ Alignments ’ tab and ensure the “Show soft-clipped bases” option is ticked. This colour highlighting emphasises soft-clips on the read itself.\n\n\nYour IGV session should look similar to:\n\n\n\n3.7 Exercises\nGo to chromosome chr7, positions 87,483,625-87,484,330 using the navigation bar across the top. Take in the glorious view of a genome pileup. Stop and smell the roses! Click on stuff!\nScroll around, zoom in and out a bit!\n2. Go back to chromosome 7:87,483,625-87,484,330. What is the (rough) coverage across this region? (Hint: Look at the coverage track)\nCan you spot the three mutant variants (two small and one larger) in this region? State what the evidence is for them?\nHints\n• Hint1: Look around 87,483,960 for an insertion. How large is it? How many reads does it occur in?\n• Hint2: Look around 87,483,960 for a deletion. How large is it? How many reads does it occur in?\n• Hint3: Zoom out slightly and watch the coverage track between 87,483,700 - 87,484,200.\nOnce you’ve spotted the large change look at reference sequence the edges of the mutation to hazard a guess as to its mechanism.\n4. Can you venture a guess as to what happened here? Why are these mutations present?\nCongratulations you have completed the Read Alignment tutorial. If you have time left then continue to the next (optional) section of the tutorial: Alignment workflows.\nHere is an additional IGV tutorial and refresher: https://github.com/sanger-pathogens/pathogen-informatics-training/blob/master/Notebooks/IGV/IGV.pdf. You can find a copy of this tutorial in your manual. Unfortunately, there is not enough time to complete this tutorial now but you may find it useful to look at it after the course."
  },
  {
    "objectID": "course_modules/Module2/module2_manual.html#ngs-workflows",
    "href": "course_modules/Module2/module2_manual.html#ngs-workflows",
    "title": "Manual",
    "section": "4. NGS Workflows",
    "text": "4. NGS Workflows\nA typical NGS experiment involves more than one sample, potential 10’s or 100’s of samples. During the experiment, a sample may be split across multiple libraries and and a library may be split across multiple sequencing runs (lanes). For example, you may have to increase the number of runs for a specific sample to increase the read-depth (sequencing volume), so you have to prepare multiple libraries.\nTherefore you need a coordinated workflow, driven by standard software to bring it reliably together.\nRead alignment is just the first part of that. Once you have a BAM file for each sequencing run you need to merge them together to produce a BAM file for the library. At this stage it is important to perform de-duplication on the merged data. The main purpose of removing duplicates is to mitigate the effects of PCR amplification bias introduced during library construction. PCR duplicates erroneously inflate the coverage and, if not removed, can give the illusion of high confidence when it is not really there which can have an effect on downstream analysis such as variant calling.\nThe figure below outlines a typical NGS workflow:\n In this part of the tutotial, we have two lanes of illumina sequencing data produced from a single library of yeast. We will use the BWA aligner to align the data to the Saccromyces cerevisiae genome (ftp://ftp.ensembl.org/pub/current_fasta/saccharomyces_cerevisiae/dna/) and produce a merged BAM file for the library.\nTo begin go to the following directory:\ncd /home/manager/course_data/read_alignment/data/Exercise2/60A_Sc_DBVPG6044/library1\n\n4.1 Index the reference\nbwa index ../../../../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa.gz\n\n\n4.2 Align the first sequencing run\nRecall that to align a lane of data to a reference genome we must perform the following steps:\n• Align the data\n• Convert from SAM to BAM\n• Sort the BAM file\n• Index the sorted BAM file\n\n\n4.2.1 Find the data\nGo to the directory that contains the data for the first sequencing run:\ncd lane1\n\n\n4.2.2 Run the alignment\nRemember from earlier in the tutorial that the Unix pipe command allows you to feed the output of one command into the next command. So using Unix pipes, we can combine all of the alignment steps together into one command and do all of this data processing together and avoid writingintermediate files. To do this type the command:\nbwa mem -M -R '@RG\\tID:lane1\\tSM:60A_Sc_DBVPG6044' ../../../../ref/Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa.gz s_7_1.fastq.gz s_7_2. fastq.gz | samtools view -bS - | samtools sort -T temp -O bam -o lane1.sorted.bam -\nQ1: What do the -M and -R options do?\nQ2: What does the -bS option do?\nNow index the BAM file:\nsamtools index lane1.sorted.bam\n\n\n4.2.3 Generate QC stats\nNow use samtools to collect some statistics and generate QC plots from the alignment in the BAM file. Type the commands:\nsamtools stats lane1.sorted.bam &gt; lane1.stats.txt\nplot-bamstats -p plot/ lane1.stats.txt\nNow look at the output and answer the following questions:\nQ3: What is the total number of reads?\nQ4: What proportion of the reads were mapped?\nQ5: How many reads were paired correctly/properly?\nQ6: How many reads mapped to a different chromosome?\nQ7: What is the insert size mean and standard deviation?\nIn a web browser open the file called plots.html to view the QC information.\nQ8: How many reads have zero mapping quality?\nQ9: Which of the first fragments or second fragments are higher base quality on average?\n\n\n4.3 Align the second sequencing run\nThere is a second lane of sequencing data in the library1 directory contained in the directory lane2. We want to also align this sequncing data and produce a BAM file.\nGo to the directory that contains the data for the second sequencing run:\ncd ../lane2\nNow align the data in this directory to the yeast reference genome and produce a sorted BAM file.\nNote: This time when you use the bwa mem command use the following header option to specify lane2 as the read group ID:\n@RG\\tID:lane2\\tSM:60A_Sc_DBVPG6044\nQ10: What is the size of the BAM file that is produced?\n\n\n4.4 Merge the BAM files\nGo to the directory that contains the data for the library 60A_Sc_DBVPG6044/library1 . Use ls to get a listing of the files and directories contained in this directory.\ncd ..\npwd\nls\nYou will notice that there are two directories called lane1 and lane2. There were two sequencing lanes produced from this sequencing library. In order to mark library PCR duplicates, we need to merge the two lane BAM files together to produce a single BAM file. We will use the picard tool called ‘MergeSamFiles’ (http://picard.sourceforge.net) to merge the lane BAM files.\nTo find the options for ‘MergeSamFiles’ command, type:\npicard MergeSamFiles\nNow use the I= option to specify both the input BAM files and the O= option to specify the outputfile (e.g. library1.bam). Note: Multiple input files can be specified using multiple I= options\n\n\n4.5 Mark PCR duplicates\nWe will use a program called ‘MarkDuplicates’ that is part of Picard tools (http://picard.source-forge.net) to remove PCR duplicates that may have been introduced during the library construction stage. To find the options for ‘MarkDuplicates’ type:\npicard MarkDuplicates\nNow use the I= option to specify the input BAM file and the O= option to specify the output file (e.g. library1.markdup.bam). You will also need to specify the duplication metrics output file using\nM= (e.g. library1.markdup.metrics).\n**Don’t forget to index your final bam file using samtools index.\nQ11: From looking at the output metrics file - how many reads were marked as duplicates?\nQ12: What was the percent duplication?\n\n\n4.6 Visualise the alignment\nGo to the directory containing the reference genome and uncompress the file as IGV cannot read a compressed file.\ncd /home/manager/course_data/read_alignment/data/ref\ngunzip Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa.gz\nStart IGV by typing:\nigv &\n\n\n4.6.1 Load the reference genome\nOn the top menu bar go to ’ Genomes –&gt; Load Genome From File… ‘, go to the “ref” directory and select the file “Saccharomyces_cerevisiae.R64-1-1.dna.toplevel.fa” and click’ Open ’\n\n\n\n\n4.6.2 Load the alignment\nTo load the merged BAM file, on the top menu bar go to ’File –&gt; Load from File…’ and select the library BAM file that you created in the previous step.\n\n\n\n\n4.7 Exercises\n1. Go to Chromosome IV and position 764,292. (Hint: use the navigation bar across the top)\n2. What is the reference base at this position?\n3. Do the reads agree with the reference base?\n4. What about the adjacent position (IV:764,293)? What is the reference base at this position? Is it supported by the reads?\n5. Go to Chromosome IV and position 766,589.\n6. What sort of mutation are the alignments indicating might be present?\n7. Go to Chromosome IV and position 770,137 using the navigation bar across the top.\n8. What sort of mutation are the alignments indicating might be present? Is there anything in the flanking sequence of the reference genome that might make you suspicious about this mutation?\n9. Convert the BAM file to a CRAM file\nYou have reached the end of the Read Alignment tutorial."
  },
  {
    "objectID": "course_modules/Module2/module2.html",
    "href": "course_modules/Module2/module2.html",
    "title": "Overview",
    "section": "",
    "text": "Module Title\nRead alignment\n\n\nDuration\n4 hours\n\n\nKey topics\nIn this module, learners will look at:\n\nRead alignment theory and tools\nChallenges in alignment and quality assessment\nData processing and scalability\n\n\n\nActivities\n\nLecture: 1h\nPractical exercises: 2.5h\nQuiz: 0.5h\n\n\n\nManual and practical execises\nModule manual\nSolutions\n\n\nPresentation slides\n(PPT/PDF, to be generated from the module manual)\n\n\nLecture notes or scripts\nPre-recorded lecture\nVirtual machine\n\n\nDatasets"
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html",
    "href": "course_modules/Module5/module5_exercises.html",
    "title": "Exercises",
    "section": "",
    "text": "Genome assembly is the process of taking a large number of fragments of DNA and putting them back together to create a representation of the original DNA sequence from which they originated.\n\n\n\nMany genomes contain large numbers of repeat sequences. Often these repeats are thousands of nucleotides long, and some occur in many different locations in the genome. This makes genome assembly a very difficult computational problem to solve. However, there are many genome assembly tools that exist that can produce long contiguous sequences (contigs) from sequencing reads. The assembly tool that you use will be determined by different factors, largely this will be the length of the sequencing reads and the sequencing technology used to produce the reads.\nIn this practical we will assemble one chromosome of a malaria parasite: Plasmodium falciparum, the IT clone. We have sequenced the genome with both PacBio and Illumina and pre-filtered the reads to select only those reads from a single chromosome.\nOn completion of the tutorial, you can expect to be able to:\n• Describe the different approaches to genome assembly\n• Generate a genome assembly from Illumina data\n• Generate a genome assembly from PacBio data\n• Generate statistics to evaluate the quality of a genome assembly\n• Estimate the size of a genome assembly\n\n\n\nThis tutorial comprises the following sections:\n1. PacBio genome assembly\n2. Assembly algorithms\n3. Illumina genome assembly\n4. Genome assembly estimation\n5. PacBio genome assembly again\n\n\n\nThis tutorial was written by Jacqui Keane based on material from Shane McCarthy and Thomas Otto.\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal window and type the command below:\ncd ~/course_data/assembly/data\n\n\n\nThis tutorial requires that you have canu, jellyfish, velvet, assembly-stats and wtdbg installed on your computer. These are already installed on the virtual machine you are using. To check that these are installed, run the following commands:\ncanu\njellyfish\nvelvetg\nvelveth\nassembly-stats\nwtdbg2\nThis should return the help message for software canu, jellyfish, velvet, assembly-stats and wtdbg2 respectively.\nIf after this course you would like to download and install this software the instructions can be found at the links below, alternatively we recommend bioconda for the installation and management of your bioinformatics software.\n• The canu website\n•The jellyfish github page\n• The velvet website\n• The wtdbg2 github page\nTo get started with the tutorial, go to the first section: Pacbio genome assembly."
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html#genome-assembly",
    "href": "course_modules/Module5/module5_exercises.html#genome-assembly",
    "title": "Exercises",
    "section": "",
    "text": "Genome assembly is the process of taking a large number of fragments of DNA and putting them back together to create a representation of the original DNA sequence from which they originated.\n\n\n\nMany genomes contain large numbers of repeat sequences. Often these repeats are thousands of nucleotides long, and some occur in many different locations in the genome. This makes genome assembly a very difficult computational problem to solve. However, there are many genome assembly tools that exist that can produce long contiguous sequences (contigs) from sequencing reads. The assembly tool that you use will be determined by different factors, largely this will be the length of the sequencing reads and the sequencing technology used to produce the reads.\nIn this practical we will assemble one chromosome of a malaria parasite: Plasmodium falciparum, the IT clone. We have sequenced the genome with both PacBio and Illumina and pre-filtered the reads to select only those reads from a single chromosome.\nOn completion of the tutorial, you can expect to be able to:\n• Describe the different approaches to genome assembly\n• Generate a genome assembly from Illumina data\n• Generate a genome assembly from PacBio data\n• Generate statistics to evaluate the quality of a genome assembly\n• Estimate the size of a genome assembly\n\n\n\nThis tutorial comprises the following sections:\n1. PacBio genome assembly\n2. Assembly algorithms\n3. Illumina genome assembly\n4. Genome assembly estimation\n5. PacBio genome assembly again\n\n\n\nThis tutorial was written by Jacqui Keane based on material from Shane McCarthy and Thomas Otto.\n\n\n\nYou can follow this tutorial by typing all the commands you see into a terminal window. This is similar to the “Command Prompt” window on MS Windows systems, which allows the user to type DOS commands to manage files.\nTo get started, open a new terminal window and type the command below:\ncd ~/course_data/assembly/data\n\n\n\nThis tutorial requires that you have canu, jellyfish, velvet, assembly-stats and wtdbg installed on your computer. These are already installed on the virtual machine you are using. To check that these are installed, run the following commands:\ncanu\njellyfish\nvelvetg\nvelveth\nassembly-stats\nwtdbg2\nThis should return the help message for software canu, jellyfish, velvet, assembly-stats and wtdbg2 respectively.\nIf after this course you would like to download and install this software the instructions can be found at the links below, alternatively we recommend bioconda for the installation and management of your bioinformatics software.\n• The canu website\n•The jellyfish github page\n• The velvet website\n• The wtdbg2 github page\nTo get started with the tutorial, go to the first section: Pacbio genome assembly."
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html#pacbio-genome-assembly",
    "href": "course_modules/Module5/module5_exercises.html#pacbio-genome-assembly",
    "title": "Exercises",
    "section": "2 PacBio Genome Assembly",
    "text": "2 PacBio Genome Assembly\nFirst, check you are in the correct directory.\npwd\nIt should display something like:\n/home/manager/course_data/assembly/data\nNow we are going to start the PacBio assembly using the canu program. It first corrects the reads and then uses the Celera assembler to merge the long reads into contigs. To see the contents of the directory, type:\nls\nThe pre-filtered PacBio reads are called PBReads.fastq.gz - take a look at the contents of this file.\nzless -S PBReads.fastq.gz\nWhat do you notice compared to the Illumina fastq files you have seen previously?\nNow we will start the assembly with canu (https://canu.readthedocs.io/).\nNOTE: This will take some time, so we will start it running now in the background and hopefully it will complete while we work on the other exercises.\ncanu -p PB -d canu-assembly -s file.specs -pacbio-raw PBReads.fastq.gz &&gt;canu_log.txt &\nThe -p option sets the prefix of output files to PB, while the -d option sets the output directory to canu-assembly/. The & at the end will set this command running in the background while you work through the next sections of this practical.\nBefore we move on, let’s just make sure the program is running. Use the top or better htop command to show you all processes running on your machine (type q to exit top). You should hopefully see processes associated with canu running (or maybe something called meryl). We can also check the canu_log.txt file where the canu logs will be written. If we see error messages in the file, then something has gone wrong.\nhtop\nless canu_log.txt\nWhile the canu assembly is running, move on to the next section: Assembly algorithms."
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html#assembly-algorithms",
    "href": "course_modules/Module5/module5_exercises.html#assembly-algorithms",
    "title": "Exercises",
    "section": "3 Assembly algorithms",
    "text": "3 Assembly algorithms\nThere are several approaches (algorithms) that can be used to generate a set of contiguous sequences (contigs) from a set of DNA fragments (reads). Two of the main approaches are:\n• Overlap Layout Consensus (OLC): This approach looks at all pairs x,y of all reads and determines if there is a sufficient overlap of the two reads. If there is, then it bundles the stretches of overlap graphs into contigs. Examples of assembly software that use this approach are Falcon (PacBio), Canu(Pacbio, ONT), mnimap/miniasm.\n• de Brujin graph (DBG): This approach builds a graph of all subsequences of lenght k (k-mers).\nExamples of assembly software that use this approach are velvet, ABySS, SPAdes, wtdbg2.\n\n3.1 Exercise\nHere we are going to build a de Brujin graph by hand from a set of short reads! Using the 6bp reads listed below, manually create the de Brujin graph and find the contig(s).\nUsing a k-mer value of k=5 produces the following k-mers of length 5 from the reads above. To finish the graph, join k-mers that overlap by 4 bases.\nQuestions:\n1. What is the contig sequence?\n2. What was difficult here?\nNow move on to the next section: Illumina genome assembly\n\n\n4 Illumina Genome Assembly\nWe are now going to use the assembler velvet https://www.ebi.ac.uk/~zerbino/velvet/ to assemble the Illumina reads. Our Illumina reads are from the same sample we used to generate the PacBio data.\n\n\n4.1 Generating an Illumina assembly with velvet\nCreating a genome assembly using velvet is a two stage process:\n• First, the command velveth is used to generate the k-mers from the input data\n• Second, the command velvetg is used to build the de Bruijn graph and find the optimal path through the graph\nTo assemble the data, start with the command:\nvelveth k.assembly.49 49 -shortPaired -fastq IT.Chr5_1.fastq IT.Chr5_2.fastq\nThe input option k.assembly.49 is the name of the directory where the results are to be written.\nThe vallue 49 is the k-mer size. The other options specify the type of the input data (-shortPaired) and -fastq is used to specify the fastq files that contain the sequencing reads.\nTo see all possible options for velveth use:\nvelveth\nNow use velvetg to build the graph and find the path through the graph (similar to what we did manually in the previous section):\nvelvetg k.assembly.49 -exp_cov auto -ins_length 350\nThe first parameter k.assembly.49 specifies the working directory as created with the velveth command. The second -exp_cov auto instructs velvet to find the median read coverage automatically rather than specifying it yourself. Finally, -ins_length specifies the insert size of the sequencing library used. There is a lot of output printed to the screen, but the most important information is the last line:\nFinal graph has 1455 nodes and n50 of 7527, max 38045, total 1364551, using 700301/770774 reads. (Your exact result might differ depending on the velvet version used - don’t worry).\nThis gives you a quick idea of the result.\n• 1455 nodes are in the final graph.\n• An n50 of 7527 means that 50% of the assembly is in contigs of at least 7527 bases, it is the median contig size. This number is most commonly used as an indicator of assembly quality.\nThe higher, the better! (but not always!)\n• Max is the length of the longest contig.\n• Total is the size of the assembly, here 1346kb.\n• The last two numbers tell us how many reads were used from the 7.7 million pairs input data.\nTo see all possible options for velvetg use: velvetg\nNow let’s try to improve the quality of the assembly by varying some of the input parameters to velvet. Two parameters that can play a role in improving the assembly are -cov_cutoff and -min_contig_lgth.\nUsing the -cov_cutoff parameter means that nodes with less than a specific k-mer count are removed from the graph.\nUsing the -min_contig_lgth parameter means that contigs with less than a specific size are removed from the assembly.\nTry re-running the assembly with a kmer of 49 and using a -cov_cutoff of 5 and -min_contig_lgth of 200.\nvelvetg k.assembly.49 -exp_cov auto -ins_length 350 -min_contig_lgth 200 -cov_cutoff 5\nNote as we are not changing the k-mer size, we do not need run the velveth command again.\nGenerally, the k-mer size has the biggest impact on assembly results. Let us make a few other assemblies for different k-mer sizes i.e. 55, 41. Here is the example for k-mer length of 55.\nvelveth k.assembly.55 55 -shortPaired -fastq IT.Chr5_1.fastq IT.Chr5_2.fastq\nvelvetg k.assembly.55 -exp_cov auto -ins_length 350 -min_contig_lgth 200 -cov_cutoff 5\nNote: If you find that you are having trouble running the velvet assemblies or if it is running for longer than 10-15 mins then quit the command (Ctrc-C). A pre-generated set of Illumina assemblies can be found at:\nls ~/course_data/assembly/data/backup/illumina_assemblies\n\n\n4.2 Assembly metrics\nAll the assembly results are written into the directory you specified with the velvet commands, e.g. k.assembly.41, k.assembly.49, k.assembly.55. The final contigs are written to a file called contigs.fa. The stats.txt file holds some information about each contig, its length, the coverage, etc. The other files contain information for the assembler.\nAnother way to get more assembly statistics is to use a program called assembly-stats. It displays the number of contigs, the mean size and a lot of other useful statistics about the assembly. These numbers can be used to assess the quality of your assemblies and help you pick the “best” one.\nType:\nassembly-stats k.assembly*/*.fa\nWrite down the results for each assembly made using different k-mer sizes. Which one looks the best?\nQuestion: What is the best choice for k?\nWe want to choose the set of parameters that produce the assembly where the n50, average contig size and the largest contigs have the highest values, while contig number is the lowest.\nYou will notice another statistic produced by assembly-stats is N_count, what does the N_count mean?\nAs we know, DNA templates can be sequenced from both ends, resulting in mate pairs. Their outer distance is the insert size. Imagine mapping the reads back onto the assembled contigs. In some cases the two mates don’t map onto the same contig. We can use those mates to scaffold the two contigs e.g. orientate them to each other and put N’s between them, so that the insert size is correct, if enough mate pairs suggest that join. Velvet does this automatically (although you can turn it off).\nThe number of mates you need to join two contigs is defined by the parameter -min_pair_count.\nHere is the description:\n-min_pair_count &lt;integer&gt;: minimum number of paired end connections to justify the scaffolding of two long contigs (default: 5)\nHere is a schema:\nIt might be worth mentioning, that incorrect scaffolding is the most common source of error in assembly (so called mis-assemblies). If you lower the min_pair_count too much, the likelihood of generating errors increases.\nOther errors are due to repeats. In a normal assembly one would expect that the repeats are all collapsed, if they are smaller than the read length. If the repeat unit is smaller than the insert size, than it is possible to scaffold over it, leaving the space for the repeats with N’s.\nTo get the statistic for the contigs, rather than scaffolds (supercontigs), you can use seqtk to break the scaffold at any stretch of N’s with the following commands:\nseqtk cutN -n1 k.assembly.41/contigs.fa &gt; assembly.41.contigs.fasta\nassembly-stats assembly.41.contigs.fasta\nseqtk cutN -n1 k.assembly.49/contigs.fa &gt; assembly.49.contigs.fasta\nassembly-stats assembly.49.contigs.fasta\nseqtk cutN -n1 k.assembly.55/contigs.fa &gt; assembly.55.contigs.fasta\nassembly-stats assembly.55.contigs.fasta\nQuestion: How does the contig N50 compare to the scaffold N50 for each of your assemblies?\nCongratulations you have sucessfully created a genome assembly using Illumina sequence data. Now move on to the next section: Assembly estimation"
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html#assembly-estimation",
    "href": "course_modules/Module5/module5_exercises.html#assembly-estimation",
    "title": "Exercises",
    "section": "5 Assembly estimation",
    "text": "5 Assembly estimation\nFortunately with this dataset, we have a known reference genome and therefore some expectations about the size and composition of the Plasmodium falciparum genome.\nBut what if we are working with a new genome, one which has not been sequenced before? One approach is to look at k-mer distributions from the reads. This can be done using two pieces of software, jellyfish (https://github.com/gmarcais/Jellyfish) and Genomesope (http://qb.cshl.edu/genomescope/). Jellyfish is used to determine the distribution of k-mers in the dataset and then Genomescope is used to model the single copy k-mers as heterozygotes, while double copy k-mers will be the homozygous portions of the genome. It will also estimate the haploid genome size.\nLet’s check with our Plasmodium falciparum Illumina data that the k-mer distribution gives us what we expect. To get a distribution of 21-mers:\njellyfish count -C -m 21 -s 1G -t 2 -o IT.jf &lt;(cat IT.Chr5_1.fastq IT.Chr5_2.fastq)\nThis command will count canonical (-C) 21-mers (-m 21), using a hash with 1G elements (-s 1G) and 2 threads (-t 2). The output is written to IT.jf.\nTo compute the histogram of the k-mer occurences, use\njellyfish histo IT.jf &gt; IT.histo\nLook at the output:\nless IT.histo\nNow analyse the output with genomescope:\nRscript genomescope.R IT.histo 21 76 IT.jf21\nWhere 21 is the k-mer size, 76 is the read length of the input Illumina data and IT.jf21 is the directory to write the output to. To look at the output use:\nless IT.jf21/summary.txt\nfirefox IT.jf21/plot.png &\nYou should see an image similar to the ones shown below. Notice the bump to right of the main peak. These are the repeated sequences.\n\n5.1 Exercises\n1. What is the predicted heterozygosity?\n2. What is the predicted genome size?\n3. Does this seem reasonable?\nWe have used jellyfish to pre-generate a set of k-mer histograms for a handful of other species. These histo files can be found in the data directory.\nls *.histo\nTry running genomscope on these. The read length for all of the datasets is 150bp.\nfAnaTes1.jf21.histo: What is the bulge to the left of the main peak here?\nRscript genomescope.R fAnaTes1.jf21.histo 21 150 fAnaTes1.jf21\nf DreSAT1.jf21.histo: What is the striking feature of this genome?\nfMasArm1.jf21.histo: You should see a nice tight diploid peak for this sample. It has very low heterozygosity - similar to human data.\nfSalTru1.jf21.histo: This genome was actually haploid. How do we interpret the features in the genomescope profile?\nNow let us go back to our PacBio assembly: PacBio assembly"
  },
  {
    "objectID": "course_modules/Module5/module5_exercises.html#pacbio-genome-assembly-contd.",
    "href": "course_modules/Module5/module5_exercises.html#pacbio-genome-assembly-contd.",
    "title": "Exercises",
    "section": "6 PacBio Genome Assembly contd.",
    "text": "6 PacBio Genome Assembly contd.\nWe have generated an assembly for the Plasmodium falciparum chromosome from our Illumina data.\nNow, let’s have a look at the PacBio assembly.\n\n6.1 Generating pacbio assemblies\nThe canu pacbio assembly should have hopefully finished by now (check the log less canu_log.txt).\nIf it has not finished, you can find a pre-generated canu assembly at:\nls ~/course_data/assembly/data/backup/pacbio_assemblies\nNow use the assembly-stats script to look at the assembly statistics.\nassembly-stats canu-assembly/PB.contigs.fasta\nHow does it compare to the Illumina assembly?\nAnother long read assembler based on de Bruijn graphs is wtdbg2 https://github.com/ruanjue/wtdbg2. Let’s try to build a second assembly using this assembler and compare it to the assembly produced with canu.\nwtdbg2 -t2 -i PBReads.fastq.gz -o wtdbg\nwtpoa-cns -t2 -i wtdbg.ctg.lay.gz -fo wtdbg.ctg.lay.fasta\nassembly-stats wtdbg.ctg.lay.fasta\n\n\n6.2 Comparing pacbio assemblies\nHow does the wtdbg2 assembly compare to the canu assembly? Both assemblies may be similar in contig number and N50, but are they really similar? Let’s map the Illumina reads to each assembly, call variants and compare.\nbwa index canu-assembly/PB.contigs.fasta\nsamtools faidx canu-assembly/PB.contigs.fasta\nbwa mem -t2 canu-assembly/PB.contigs.fasta IT.Chr5_1.fastq IT.Chr5_2.fastq |samtools sort -@2 - | samtools mpileup -f canu-assembly/PB.contigs.fasta -ug- | bcftools call -mv &gt; canu.vcf\nDo the same for wtdbg.ctg.lay.fasta and then compare some basic statistics.\nbwa index wtdbg.ctg.lay.fasta\nsamtools faidx wtdbg.ctg.lay.fasta\n\n\n6.3 Polishing pacbio assembly\nbwa mem -t2 wtdbg.ctg.lay.fasta IT.Chr5_1.fastq IT.Chr5_2.fastq | samtools sort-@2 - | samtools mpileup -f wtdbg.ctg.lay.fasta -ug - | bcftools call -mv &gt;wtdbg.vcf\nbcftools stats canu.vcf | grep ^SN\nbcftools stats wtdbg.vcf | grep ^SN\nQuestion: What do you notice in terms of the number of SNP and indel calls?\nThe wtdbg2 assembly has more variants due to having more errors. This is mainly due to a lack of polishing or error correction - something that the canu assembler performs, but the wtdbg2 assembler does not.\nCorrecting errors is an important step in making an assembly, especially from noisy long ready data. Not polishing an assembly can lead to genes not being identified due to insertion and deletion errors in the assembly sequence. To polish a genome assembly with Illumina data we use bcftools consensus to change homozygous differences between the assembly and the Illumina data to match the Illumina data\nRun the following steps to polish the canu assembly\nbgzip -c canu.vcf &gt; canu.vcf.gz\ntabix canu.vcf.gz\nbcftools consensus -i'QUAL&gt;1 && (GT=\"AA\" || GT=\"Aa\")' -Hla -f canu-assembly/PB.contigs.fasta canu.vcf.gz &gt; canu-assembly/PB.contigs.polished.fasta\nRun the following steps to polish the `wtdbg2` assembly\nbgzip -c wtdbg.vcf &gt; wtdbg.vcf.gz\ntabix wtdbg.vcf.gz\nbcftools consensus -i'QUAL&gt;1 && (GT=\"AA\" || GT=\"Aa\")' -Hla -f wtdbg.ctg.lay.fasta wtdbg.vcf.gz &gt; wtdbg.contigs.polished.fasta\nFinally, align and call variants like before (bwa index/bwa mem/samtools-sort/mpileup/bcftools call) using the polished assemblies as the reference this time.\nWhen running this analysis on these polished genomes, do we still get variants? More or less than with the raw canu and wtdbg2 assemblies? Why?\nCongratulations, you have reached the end of the Genome Assembly tutorial."
  },
  {
    "objectID": "course_modules/template_module.html",
    "href": "course_modules/template_module.html",
    "title": "template_module",
    "section": "",
    "text": "Title of session/module\n  - Duration \n  - Key topics\n  - Activities (lectures, hands-on, discussions) - Use GHRU template here \n- Presentation slides (PPT/PDF)\n- Lecture notes or scripts\n- Videos (prerecorded lectures, screencasts, youtube etc, with transcripts)\n- Practical/lab exercises (e.g., Jupyter notebooks, Google Colab walkthrough, command line walkthroughs)\n- Datasets (example files or open data repositories)\n- Case studies or examples"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats-answers.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/formats-answers.html",
    "title": "Data formats for NGS data - Answers",
    "section": "",
    "text": "1. There are 10 sequences in this file. To count all the header lines, we can use grep -c \"&gt;\" data/example.fasta\n2. There are 8 reads in this file. We can use grep to search for /1 or /2:\ngrep -c \"/1\" data/example.fastq\nAlternatively, we can use wc -l to count the lines in the file and then divide this by 4.\n3. RG = Read Group\n4. Illumina. See the __PL__field.\n5. SC. See the CN field.\n6. ERR003612. See the ID field.\n7. 2kbp. See the PI field.\n8. The quality is 48. We can use grep to find the id, followed by awk to print the fifth column:\ngrep \"ERR003762.5016205\" data/example.sam | awk '{print $5}'\n9. The CIGAR is 37M. We can use grep and awk to find it:\ngrep ERR003814.6979522 data/example.sam | awk '{print $6}'\n10. 213. The ninth column holds the insert size, so we can use awk to get this:\ngrep ERR003814.1408899 data/example.sam | awk '{print $9}'\n11. The CIGAR in Q9 was 37M, meaning all 37 bases in the read are either matches or mismatches to the reference.\n12. CIGAR: 4M 4I 8M. The first four bases in the read are the same as in the reference, so we can represent these as 4M in the CIGAR string. Next comes 4 insertions, represented by 4I, followed by 8 alignment matches, represented by 8M.\n13. NCBI build v37\n14. There are 15 lanes in the file. We can count the @RG lines manually, or use standard UNIX commands such as:\nsamtools view -H data/NA20538.bam | grep ^@RG | wc -l\nor\nsamtools view -H data/NA20538.bam | awk '{if($1==\"@RG\")n++}END{print n}'\n15. Looking at the @PG records ID tags, we see that three programs were used: GATK IndelRealigner, GATK TableRecalibration and bwa.\n16. The @PG records contain a the tag VN. From this we see that bwa version 0.5.5 was used.\n17. The first collumn holds the name of the read: ERR003814.1408899\n18. Chromosome 1, position 19999970. Column three contains the name of the reference sequenceand the fourth column holds the leftmost position of the clipped alignment.\n19. 320 reads are mapped to this region. We have already sorted and indexed the BAM file, so now we can search for the reagion using samtools view. Then we can pipe the output to wc to count the number of reads in this region:\nsamtools view data/NA20538_sorted.bam 1:20025000-20030000 | wc -l\n20. The reference version is 37. In the same way that we can use -h in samtools to include the header in the output, we can also use this with bcftools:\nbcftools view -h data/1kg.bcf | grep \"##reference\"\n21. There are 50 samples in the file. The -l option will list all samples in the file:\nbcftools query -l data/1kg.bcf | wc -l\n22. The genotype is A/T. With -f we specify the format of the output, -r is used to specify the region we are looking for, and with -s we select the sample.\nbcftools query -f'%POS [ %TGT]\\n' -r 20:24019472 -s HG00107 data/1kg.bcf\n23. There are 4778 positions with more than 10 alternate alleles. We can use -i to specify that we are looking for instances where the value of the INFO:AC tag (Allele Count) is greater than 10:\nbcftools query -f'%POS\\n' -i 'AC[0]&gt;10' data/1kg.bcf | wc -l\n24. There are 451 such positions. The first command picks out sample HG00107. We can then pipe the output to the second command to filter by depth and non-reference genotype. Then use wc to count the lines:\nbcftools view -s HG00107 data/1kg.bcf | bcftools query -i'FMT/DP&gt;10 & FMT/GT!=\"0/0\"' -f'%POS[ %GT %DP]\\n' | wc -l\n25. 26. The first base is at position 9923 and the last is at 9948.\n26. G. To reduce file size, only the first base is provided in the REF field.\n27. 10. See the MinDP tag in the INFO field."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment-answers.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/assessment-answers.html",
    "title": "QC assessment of NGS data",
    "section": "",
    "text": "1. The peak is at 140 bp, and the read length is 100 bp. This means that the forward and reverse reads overlap with 60 bp.\n2. There are 400252 reads in total.\nLook inside the file and locate the field “raw total sequences”. To extract the information quickly from multiple files, commands similar to the following can be used:\ngrep ^SN lane*.sorted.bam.bchk | awk -F'\\t' '$2==\"raw total sequences:\"'\n3. 76% of the reads were mapped. Divide “reads mapped” (303036) by “raw total sequences” (400252).\n4. 2235 pairs mapped to a different chromosome. Look for “pairs on different chromosomes”\n5. The mean insert size is 275.9 and the standard deviation is 47.7. Look for “insert size mean” and “insert size standard deviation”.\n6. 282478 reads were properly paired. Look for “reads properly paired”.\n7. 23,803 (7.9%) of the reads have zero mapping quality. Look for “zero MQ” in the “Reads” section.\n8. The forward reads. Look at the “Quality per cycle” graphs."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion-answers.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/conversion-answers.html",
    "title": "File conversion - Answers",
    "section": "",
    "text": "1. The CRAM file is ~18 MB. We can check this using:\nls -lh data/yeast.cram\n2. Yes, the BAM file is ~16 MB bigger than the CRAM file. We can check this using:\nls -lh data/yeast*"
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination-answers.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/practical/Notebooks/contamination-answers.html",
    "title": "Identifying contamination - Answers",
    "section": "",
    "text": "1. Streptococcus pneumoniae\n2. No\n3. ~7% of the reads. Look for “unclassified” at the top of the file."
  },
  {
    "objectID": "course_modules/NGS_Data_Formats_and_QC/lecture/src/hts-qc.html",
    "href": "course_modules/NGS_Data_Formats_and_QC/lecture/src/hts-qc.html",
    "title": "only for printing",
    "section": "",
    "text": "% subset-slides\n\\vskip2em\n\\vskip10em\n\nData Formats\nFASTQ\n\nUnaligned read sequences with base qualities\n\nSAM/BAM\n\nUnaligned or aligned reads\nText and binary formats\n\nCRAM\n\nBetter compression than BAM\n\nVCF/BCF\n\nFlexible variant call format\nArbitrary types of sequence variation\nSNPs, indels, structural variations\n\n\\vskip2em\nSpecifications maintained by the Global Alliance for Genomics and Health\n\n\nFASTQ\n\nSimple format for raw unaligned sequencing reads\nExtension to the FASTA file format\nSequence and an associated per base quality score\n\n\nQuality encoded in ASCII characters with decimal codes 33-126\n\nASCII code of “A” is 65, the corresponding quality is Q\\(= 65 - 33 = 32\\)\n\n\\vskip0em\n\nPhred quality score: \\(P = 10^{-Q/10}\\) \n\nBeware: multiple quality scores were in use!\n\nSanger, Solexa, Illumina 1.3+\n\nPaired-end sequencing produces two FASTQ files\n\n{A: Q=30, one error in 1000 bases}\n\n\nSAM / BAM\nSAM (Sequence Alignment/Map) format\n\nUnified format for storing read alignments to a reference genome\nDeveloped by the 1000 Genomes Project group (2009)\n\n\nOne record (a single DNA fragment alignment) per line describing alignment between fragment and reference\n11 fixed columns + optional key:type:value tuples\n\n\\vskip0.5em\n\\vskip0.5em\n\\vskip1em\nNote that BAM can contain\n\nunmapped reads\nmultiple alignments of the same read\nsupplementary (chimeric) reads\n\n\n\nSAM\nSAM fields\n\\vskip1em \nCCCTAACCCTAACCATAGCCCTAACCCTAACCCTAACCCTAACCCT[…]CAAACCCACCCCCAAACCCAAAACCTCACCAC\nFFFFFJJJJJJJJFJJJJFJAJJJJJ-JJAAAJFJJFFJJF&lt;FJJFFJJJJFJJJJFF[…]&lt;—F—–A7-J-&lt;J-A–77AF—J7–\nMD:Z:1G24C2A76 PG:Z:MarkDuplicates RG:Z:1 NM:i:3 MQ:i:0 AS:i:94 XS:i:94 \\end{verbatim}}}\n\n\nCIGAR string\nCompact representation of sequence alignment\n\\vskip1em \nExamples:\n\n\nFlags\n\\vskip0.5em\n\\vskip0.5em\nBit operations made easy\n\npython \nsamtools flags \n\n\\vskip5em\n\n\nOptional tags\nEach lane has a unique RG tag that contains meta-data for the lane\nRG tags\n\nID: SRR/ERR number\nPL: Sequencing platform\nPU: Run name\nLB: Library name\nPI: Insert fragment size\nSM: Individual\nCN: Sequencing center\n\n\\vskip10em\n\n\nBAM\nBAM (Binary Alignment/Map) format\n\nBinary version of SAM\nDeveloped for fast processing and random access\n\nBGZF (Block GZIP) compression for indexing\n\n\nKey features\n\nCan store alignments from most mappers\nSupports multiple sequencing technologies\nSupports indexing for quick retrieval/viewing\nCompact size (e.g. 112Gbp Illumina = 116GB disk space)\nReads can be grouped into logical groups e.g. lanes, libraries, samples\nWidely supported by variant calling packages and viewers\n\n\n\nReference based Compression\nBAM files are too large\n\n~1.5-2 bytes per base pair\n\nIncreases in disk capacity are being far outstripped by sequencing technologies\nBAM stores all of the data\n\nEvery read base\nEvery base quality\nUsing a single conventional compression technique for all types of data\n\n\\vskip14em\n\n\nCRAM\nThree important concepts\n\nReference based compression\nControlled loss of quality information\nDifferent compression methods to suit the type of data, e.g. base qualities vs. metadata vs. extra tags\n\nIn lossless mode: 60% of BAM size\nArchives and sequencing centers moving from BAM to CRAM\n\nSupport for CRAM added to Samtools/HTSlib in 2014\nSoon to be available in Picard/GATK\n\n\\vskip1em\n\n\nVCF: Variant Call Format\nFile format for storing variation data\n\nTab-delimited text, parsable by standard UNIX commands\nFlexible and user-extensible\nCompressed with BGZF (bgzip), indexed with TBI or CSI (tabix)\n\n\\vskip1em\n\\vskip3em\n\n\nVCF / BCF\nVCFs can be very big\n\ncompressed VCF with 3781 samples, human data:\n\n54 GB for chromosome 1\n680 GB whole genome\n\n\nVCFs can be slow to parse\n\ntext conversion is slow\nmain bottleneck: FORMAT fields\n\n\\vskip0.5em\n\\vskip1em\nBCF\n\nbinary representation of VCF\nfields rearranged for fast access\n\n\\vskip0.5em\n\n\ngVCF\nOften it is not enough not know variant sites only\n\nwas a site dropped because of a reference call or because of missing data?\nwe need evidence for both variant and non-variant positions in the genome\n\n\\vskip0.5em\ngVCF\n\nblocks of reference-only sites can be represented in a single record using the INFO/END tag\nsymbolic alleles &lt;*&gt; for incremental calling\n\nraw, “callable” gVCF\ncalculate genotype likelihoods only once (an expensive step)\nthen call incrementally as more samples come in\n\n\n\\vskip0.5em\n\n\nOptimizing variant calls for speed\n\\vskip2em\n\\vskip3em\nNew TWK format by Marcus Klarqvist (under development)\n\nBCF still too slow for querying hundreds of thousands and millions of samples\nbigger but 100x faster for certain operations on GTs\n\n\n\nCustom formats for custom tasks\n\\centerline{ \\hskip1em } \\vskip1em \\centerline{ \\hskip2em }\n\n\nGlobal Alliance for Genomics and Health\nInternational coalition dedicated to improving human health\nMission\n\nestablish a common framework to enable sharing of genomic and clinical data\n\nWorking groups\n\nclinical\nregulatory and ethics\nsecurity\ndata\n\nData working group\n\nbeacon project .. test the willingness of international sites to share genetic data \nBRCA challenge .. advance understanding of the genetic basis of breast and other cancers \nmatchmaker exchange .. locate data on rare phenotypes or genotypes \nreference variation .. describe how genomes differ so researchers can assemble and interpret them \nbenchmarking .. develop variant calling benchmark toolkits for germline, cancer, and transcripts \nfile formats .. CRAM, SAM/BAM, VCF/BCF \n\nFile formats\n\nhttp://samtools.github.io/hts-specs/\n\n% # Coffee break and questions %\n% \\vskip7em %\n\n\nQuality Control\nBiases in sequencing\n\nBase calling accuracy\nRead cycle vs. base content\nGC vs. depth\nIndel ratio\n\n\\vskip0.5em\nBiases in mapping\n\\vskip1em\nGenotype checking\n\nSample swaps\nContaminations\n\n\n\nBase quality\nSequencing by synthesis: dephasing\n\ngrowing sequences in a cluster gradually desynchronize\nerror rate increases with read length\n\n\\vskip1em\nCalculate the average quality at each position across all reads\n\\vskip0.5em\n\\vskip0.5em\n\n\nBase calling errors\n\n\n\nBase quality\n\\centerline{ \\hskip5em } \n\n\nMismatches per cycle\nMismatches in aligned reads (requires reference sequence)\n\ndetect cycle-specific errors\nbase qualities are informative!\n\n\\vskip1em\n\n\nGC bias\nGC- and AT-rich regions are more difficult to amplify\n\ncompare the GC content against the expected distribution (reference sequence)\n\n\\vskip2em\n\\vskip3em\n\n\nGC content by cycle\nWas the adapter sequence trimmed?\n\\vskip1em\n\\vskip3em\n\n\nFragment size\nPaired-end sequencing: the size of DNA fragments matters\n\\vskip1em \\vskip3em\n\n\nQuiz\n\\vskip1em \\centerline{\\hskip2em} \\vskip1em\n\\vskip3em\n\n\nInsertions / Deletions per cycle\nFalse indels\n\nair bubbles in the flow cell can manifest as false indels\n\n\\vskip1em\n\\vskip3em\n\n\nAuto QC tests\nA suggestion for human data: \\vskip1em\n\\vskip2em\n\n\n\nDetecting sample swaps\nCheck the identity against a known set of variants\n\\vskip1em \\centerline{ \\hskip1em } \\vskip3em\n\n\nHow many markers are necessary?\n\\vskip1em Number of sites required to identify non-related human samples\n\\vskip1em \\centerline{ \\hskip1em }\n\n\nSoftware\nSoftware used to produce graphs in these slides\n\nsamtools stats and plot-bamstats\nbcftools gtcheck\nmatplotlib\n\n% # xxx %\n% Pipelining %\n% http://seqanswers.com % http://www.cbs.dtu.dk/courses/27626/Exercises/BAM-postprocessing.php %\n% Schwartz: Detection and Removal of Biases in the Analysis of Next- Generation Sequencing Reads % % # Biases in Next Generation Sequencing data %\n% * nucleotide per cycle bias % * mostly in RNA-seq, sometimes in ChIP-seq % * cannot be attributed to biased PCR-amplification % * partial explanation: random hexamer priming during reverse transcription? % - more references in the paper above: 14,15,16,17 %\n% dephasing %\n% The illumina platform uses a so called sequencing by synthesis process. Bases are added one at a time and the consensus is determined in a cluster of identical sequences. %\n% The source of errors can be numerous, here is one review that discusses the issues in more detail: %\n% The challenges of sequencing by synthesis, Nature Biotech, 2009 %\n% In a nuthsell a short answer to the best of my understanding is this: Not % all sequences in a cluster will grow at the same rate, this will slowly % lead to a desynchronization as the errors accumulate. This is why the % quality dips towards the end."
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html",
    "href": "course_modules/Module6/module6_manual.html",
    "title": "Manual",
    "section": "",
    "text": "Differential Expression using RNA-Seq\nInstructor: Vivek Iyer\nSource Material: Based extensively on slides by Victoria Offord (Wellcome Sanger Institute)\nDate: 7 June 2024"
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#learning-objectives",
    "href": "course_modules/Module6/module6_manual.html#learning-objectives",
    "title": "Manual",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nAppreciate the important aspects of RNASeq experiment design • Understand the various technical steps in RNASeq pipelines • Align RNA-Seq reads to a reference genome and a transcriptome • Visualise transcription data using standard tools • Quantify the expression values of your transcripts using standard tools • Perform QC of NGS transcriptomic data • Interpret differential gene expression data"
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#introduction-and-context",
    "href": "course_modules/Module6/module6_manual.html#introduction-and-context",
    "title": "Manual",
    "section": "Introduction and Context",
    "text": "Introduction and Context\nThe transcriptome is defined as “the complete set of transcripts present in a cell, along with their quantities, at a specific developmental stage or under specific conditions” Wang Z, Gerstein M, Snyder M. RNA-Seq: a revolutionary tool for transcriptomics. Nat Rev Genet. 2009 Jan;10(1):57-63. doi: 10.1038/nrg2484. PMID: 19015660; PMCID: PMC2949280.\nIt represents a snapshot of gene expression at a fixed point in time and under fixed environmental or biological conditions. RNA sequencing (RNA-Seq), which relies on next-generation sequencing (NGS) technology, is commonly used to measure the transcriptome. For example, in Plasmodium berghei, the transcriptome is approximately 10 million bases, covering about 50% of the genome (20 million bases), with large exons relative to introns and intergenic regions. In contrast, in Homo sapiens, the transcriptome comprises about 10% of the genome, and exons are small compared to the much larger introns and intergenic regions."
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#experimental-design",
    "href": "course_modules/Module6/module6_manual.html#experimental-design",
    "title": "Manual",
    "section": "Experimental Design",
    "text": "Experimental Design\nA successful RNA-Seq study begins with a carefully planned experimental design tailored to the biological question of interest. Key factors to consider include the type of library preparation, the sequencing depth, and the number of replicates. These decisions significantly impact the quality and interpretability of the resulting data.\n\nLibrary Preparation\nTotal RNA includes a mixture of mRNA, rRNA, tRNA, and various regulatory RNAs. Ribosomal RNA (rRNA) can constitute over 90% of total RNA, so it is essential to either enrich for the small fraction of mRNA (1–2%) or deplete the rRNA. mRNA enrichment methods typically require high-quality RNA, often assessed using the RNA Integrity Number (RIN), and may not be suitable for certain sample types such as tissue biopsies. In bacteria, where mRNA is not polyadenylated, rRNA depletion is generally required instead of poly-A selection. It is also important to be aware of the specific library preparation protocol being used, as some may remove small RNAs that could be relevant to the study.\n\n\nLibrary Type\nThe choice between stranded and unstranded libraries is another critical consideration. Strand-specific protocols preserve the information about which DNA strand the RNA was transcribed from, which is particularly valuable for distinguishing between overlapping or antisense transcripts. Another decision involves using single-end or paired-end sequencing. Paired-end sequencing is more informative and is particularly advantageous for discovering new transcripts or analyzing isoform expression, as it helps reconstruct transcript structures. However, it’s worth noting that fewer than 55% of reads typically span more than one exon.\n\n\n\nReplicates\nReplication is vital for capturing variation and ensuring the robustness of RNA-Seq results. Biological replicates involve using distinct biological samples—such as different individuals or cultures—under the same experimental conditions. These replicates are essential for assessing biological variability and drawing meaningful conclusions. In contrast, technical replicates are repeated measurements of the same sample and are used to assess variation introduced by equipment or protocols. While technical replicates are generally not required, careful sample layout and sequencing lane assignments can help control for technical variation. Some analysis tools also allow adjustments based on these technical factors or known controls (“spike-ins”).\nCareful attention to these elements of experimental design will enhance the quality and interpretability of RNA-Seq data, ensuring that the results are both reliable and biologically meaningful.\n\n\n\nSequencing Depth\nSequencing depth is a key factor in RNA-Seq experimental design, as it determines how well transcripts—especially those with low expression—can be detected. For standard human transcriptome analysis using 100 base pair (bp) paired-end reads, a sequencing depth of approximately 30 million reads per sample is typically sufficient. However, for studies aiming to identify novel transcripts or rare isoforms, deeper sequencing of 50 to 100 million reads may be necessary. As an example, 30 million 100 bp paired-end reads provide roughly 3 × 10⁹ bases of sequence. Given that the human transcriptome is around 150 megabases (1.5 × 10⁸ bases), this corresponds to approximately 20× coverage, meaning one can reasonably distinguish between full, half, or quarter expression levels.\nWhile deeper sequencing can improve sensitivity for detecting differentially expressed (DE) genes, especially those with low expression, it is subject to diminishing returns beyond a certain point. On the other hand, increasing the number of biological replicates enhances the statistical power to detect true changes in gene expression. Replicates improve the accuracy of log fold-change (logFC) estimates and help quantify variability in expression, particularly for genes with low abundance. Therefore, when designing an RNA-Seq experiment, it’s crucial to balance sequencing depth and replication according to your study goals.\nTo optimize this balance, tools such as RNA-Seq power calculators—like the one available at https://cqs-vumc.shinyapps.io/rnaseqsamplesizeweb/—can help estimate the appropriate sample size for your experiment. For further guidance, see Liu et al. (2014), Bioinformatics, which discusses the trade-offs between sequencing depth and replication in differential expression studies."
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#rna-seq-data-analysis-pipelines",
    "href": "course_modules/Module6/module6_manual.html#rna-seq-data-analysis-pipelines",
    "title": "Manual",
    "section": "RNA-Seq Data Analysis Pipelines",
    "text": "RNA-Seq Data Analysis Pipelines\n\nKey Steps in an RNA-Seq Analysis Pipeline\nEvery RNA-Seq analysis pipeline, regardless of the tools used, typically follows three core steps. First, we determine which genes or transcripts our sequencing reads belong to—this is done through mapping to a reference genome or transcriptome, or through de novo assembly. Second, we quantify how many reads align to each gene or transcript. Third, we assess whether gene or transcript expression levels differ between sample groups through differential gene expression (DGE) analysis. While these steps are common, there is no universal pipeline that suits every analysis; the best approach depends on your organism, research goals, available resources, and data quality.\n\n\nMapping RNA-Seq Reads to the Genome with HISAT2\n\nMapping RNA-Seq reads to a reference genome is useful for assessing overall data quality and examining the structure of genes, particularly in eukaryotes where introns are spliced out of mature mRNA. Splice-aware aligners are required to handle these complexities. HISAT2 is a widely used aligner that balances speed, accuracy, and low memory usage, and is especially good at discovering novel splice junctions. Alternatives include STAR, known for its speed in high-throughput settings, and Bowtie2, which is often used for more general alignment tasks. HISAT2 is particularly efficient in terms of memory footprint and splice discovery, making it a good choice for many genome-based workflows. However, after mapping, separate quantification steps are needed to count reads per gene or transcript.\n\n\n\n\nMapping to the Transcriptome and Quantifying Reads with Kallisto\n\nKallisto offers a much faster alternative by mapping reads to a reference transcriptome rather than the genome. It uses a two-step process: first, it builds an index from the set of known spliced transcript sequences; second, it quantifies read abundance through pseudoalignment. Pseudoalignment significantly speeds up the process by determining which transcripts reads belong to without calculating exact alignment positions. This method is efficient and includes quantification as part of the process.\n\nOne limitation of Kallisto is that it cannot detect novel transcripts—it relies entirely on a predefined transcriptome. Therefore, it’s not suitable if your goal is to explore new splicing events or novel isoforms. Additionally, the presence of multiple isoforms per gene introduces mapping ambiguity. However, because Kallisto maps to spliced transcripts, it accounts for this ambiguity and can provide transcript-specific read counts.\n\n\nChoosing Between Genome and Transcriptome Mapping\nMapping to the genome using tools like HISAT2, followed by visualization in IGV (Integrative Genomics Viewer), is ideal for quality control and the discovery of novel splicing events. It does, however, require more computational resources and a separate quantification step. If resources permit and novel transcript discovery is important, this method is recommended.\nIn contrast, mapping to the transcriptome with Kallisto is extremely fast and includes quantification by default. This approach is optimal when a well-annotated transcriptome is available and the focus is on efficiency rather than discovery of new transcript variants. Ultimately, the choice between genome and transcriptome mapping should align with your analysis objectives and the biological questions you aim to answer.\n\n\nNormalisation in RNA-Seq Analysis\n\nNormalization is a crucial step in RNA-Seq analysis, helping to correct for technical biases that can distort biological interpretation. For example, sequencing runs with greater depth will naturally produce more reads for each gene, and longer genes will accumulate more reads simply due to their size. To account for these issues, most normalization methods adjust for both sequencing depth and gene length. Common normalization methods include RPKM (Reads Per Kilobase per Million), FPKM (Fragments Per Kilobase per Million), and TPM (Transcripts Per Million). While these approaches help standardize read counts, they can struggle with datasets containing a few highly expressed genes that skew the overall distribution. As a result, more advanced normalization techniques—such as DESeq2’s regularized log transformation (rlog) or Sleuth’s model-based normalization—are often preferred for differential expression analysis.\n\nFPKM is a refinement of RPKM, specifically designed for paired-end sequencing. It accounts for the fact that two reads may map to a single RNA fragment and prevents double-counting. While these methods provide a reasonable first-pass normalization, more robust statistical frameworks are generally required for high-quality differential expression analysis."
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#the-importance-of-quality-control-qc",
    "href": "course_modules/Module6/module6_manual.html#the-importance-of-quality-control-qc",
    "title": "Manual",
    "section": "The Importance of Quality Control (QC)",
    "text": "The Importance of Quality Control (QC)\nQuality control is essential in RNA-Seq workflows to ensure that the data used for downstream analysis is reliable and free from avoidable technical errors. During sample collection, library preparation, and sequencing, multiple things can go wrong. RNA may degrade if samples are mishandled, sequencing lanes might perform inconsistently, and certain genomic regions may pose technical challenges (e.g., sequencing hotspots or local capture inefficiencies). These issues can lead to false positives or false negatives in differential expression results by introducing noise or systematic bias into the dataset. Identifying and removing problematic samples or outliers helps preserve the integrity of the analysis.\n\nUsing PCA for QC\n\nPrincipal Component Analysis (PCA) is a widely used dimensionality reduction technique in RNA-Seq QC. RNA-Seq datasets often consist of tens of thousands of features (e.g., ~60,000 genes), making direct interpretation difficult. PCA transforms this high-dimensional data into a new coordinate system (principal components), where each new dimension captures maximal, independent variance in the data. By visualizing the first few principal components—typically PC1 to PC4—you can quickly spot outlying samples that behave differently from the rest. This helps flag potential issues early and improves the consistency and reliability of downstream results.\n\n\nDetermining Differential Expression\nTo identify differentially expressed genes, a variety of statistical tools are available, including DESeq2, EdgeR, Limma-Voom, and Sleuth (which is specifically designed to work with Kallisto output). These methods improve on simpler approaches like per-gene t-tests, which are usually inappropriate due to limited sample sizes and the need to model variance across genes. Modern RNA-Seq tools incorporate “size factors” to account for differences in sequencing depth and use advanced statistical modeling to estimate gene-specific variance. Most tools also include mechanisms for testing complex experimental designs and adjusting for multiple testing, often reporting both p-values and q-values to help control false discovery rates.\n\n\nQuality Control with Sleuth\n\nSleuth integrates normalization, statistical modeling, and visualization for data derived from pseudoalignment tools like Kallisto. It provides built-in quality control metrics and supports flexible experimental designs, making it a convenient and powerful option for users focused on transcript-level quantification. With Sleuth, users can perform PCA, visualize sample distributions, and explore model fits directly within the same environment, streamlining the QC and analysis workflow.\n\n\nWhat to Do with a List of Differentially Expressed Genes\nOnce you’ve generated a list of differentially expressed genes, the next challenge is interpreting what it means biologically. If you have a predefined hypothesis, now is the time to test it. Otherwise, you can begin exploring the list using tools for functional enrichment and pathway analysis, such as Gene Ontology (GO) term enrichment, pathway analysis tools like GSEA, TopGO, InnateDB, or commercial options like Ingenuity Pathway Analysis. Another approach is manual exploration—read relevant literature, investigate gene functions, and look for patterns. Visual tools like volcano plots (effect size vs. p-value) are also useful for identifying genes of interest. From there, you can formulate new hypotheses and decide whether to pursue further bioinformatic exploration or design the next set of wet lab experiments."
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#practical-exercise",
    "href": "course_modules/Module6/module6_manual.html#practical-exercise",
    "title": "Manual",
    "section": "Practical Exercise",
    "text": "Practical Exercise\n·Organism: Plasmodium chabaudi (rodent malaria parasite).\n·Compare transcriptomes between mosquito-transmitted (MT) and serial blood passage (SBP) parasites.\n·Biological Question: Is the parasite’s transcriptome different after mosquito passage?"
  },
  {
    "objectID": "course_modules/Module6/module6_manual.html#appendix-key-tools-mentioned",
    "href": "course_modules/Module6/module6_manual.html#appendix-key-tools-mentioned",
    "title": "Manual",
    "section": "Appendix: Key Tools Mentioned",
    "text": "Appendix: Key Tools Mentioned\n\n\n\nTool\nPurpose\n\n\nHISAT2\nSplice-aware genome alignment\n\n\nKallisto\nFast transcriptome pseudoalignment\n\n\nIGV\nVisualization\n\n\nSleuth\nDGE and QC analysis\n\n\nDESeq2, EdgeR\nDGE analysis"
  },
  {
    "objectID": "course_modules/Module1/module1_assessment.html",
    "href": "course_modules/Module1/module1_assessment.html",
    "title": "Assessment Quiz",
    "section": "",
    "text": "Quiz\n\nLook at the following FASTQ line:\n\n@ERR007731.740 IL16_2979:6:1:9:1419/1\nTAAAAAAAAGATGTCATCAGCACATCAGAAAAGAAGGCAACTTTAAAACTTTTC\n+\nDBABBBABABAABABABBABBBAAA&gt;@B@BBAA@4AAA&gt;.&gt;BAA@779:AAA@A\nWhat is the quality of the first base (T)?\n\nLook at the following FASTQ line:\n\n@ERR007731.740 IL16_2979:6:1:9:1419/1  TAAAAAAAAGATGTCATCAGCACATCAGAAAAGAAGGCAACTTTAAAACTTTTC\n+\nDBABBBABABAABABABBABBBAAA&gt;@B@BBAA@4AAA&gt;.&gt;BAA@779:AAA@A\nWhat is the probability that there was a base call error for the first base (T)?\n\nLook at the following line from the header of a SAM file.\n\n@RGID:ERR003612 PL:ILLUMINA LB:g1k-sc-NA20538-TOS-1 PI:2000 DS:SRP000540 SM:NA20538 CN:SC\nWhat does RG stand for?"
  }
]